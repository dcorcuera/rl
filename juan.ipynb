{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n enviroment\n",
    "env_name = 'Acrobot-v1'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Semillas de reproducibilidad\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "# Set observations and actions spaces\n",
    "n_obs = env.observation_space\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4, 6)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 24)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          6400        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           8256        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          32896       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            195         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 64)           8256        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            65          dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 3)            0           dense_4[0][0]                    \n",
      "                                                                 lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 3)            0           dense_7[0][0]                    \n",
      "                                                                 subtract[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 154,756\n",
      "Trainable params: 154,756\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 22:20:01.675375: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-12-14 22:20:01.704074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-14 22:20:01.710936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:0a:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\n",
      "coreClock: 1.807GHz coreCount: 28 deviceMemorySize: 11.66GiB deviceMemoryBandwidth: 335.32GiB/s\n",
      "2024-12-14 22:20:01.710980: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-12-14 22:20:01.734923: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-12-14 22:20:01.734977: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-12-14 22:20:01.738104: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-12-14 22:20:01.738392: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-12-14 22:20:01.776823: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-12-14 22:20:01.777729: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-12-14 22:20:01.777901: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.5/lib64:/usr/local/cuda-12.5/extras/CUPTI/lib64\n",
      "2024-12-14 22:20:01.777912: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/juan/.pyenv/versions/3.8.16/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n",
      "2024-12-14 22:20:01.856407: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-14 22:20:01.857340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-12-14 22:20:01.857352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n",
      "2024-12-14 22:20:01.871806: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3792575000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 200000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.pyenv/versions/3.8.16/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    500/200000: episode: 1, duration: 0.377s, episode steps: 500, steps per second: 1326, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   1000/200000: episode: 2, duration: 0.338s, episode steps: 500, steps per second: 1477, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.pyenv/versions/3.8.16/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1500/200000: episode: 3, duration: 1.337s, episode steps: 500, steps per second: 374, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 0.070980, mae: 0.446107, mean_q: -0.445872, mean_eps: 0.988106\n",
      "   2000/200000: episode: 4, duration: 0.775s, episode steps: 500, steps per second: 645, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.075153, mae: 1.043140, mean_q: -1.389772, mean_eps: 0.983394\n",
      "   2500/200000: episode: 5, duration: 0.790s, episode steps: 500, steps per second: 633, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.082 [0.000, 2.000],  loss: 0.062427, mae: 1.737605, mean_q: -2.437035, mean_eps: 0.978644\n",
      "   3000/200000: episode: 6, duration: 0.758s, episode steps: 500, steps per second: 660, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.062926, mae: 2.367524, mean_q: -3.387083, mean_eps: 0.973894\n",
      "   3500/200000: episode: 7, duration: 0.757s, episode steps: 500, steps per second: 660, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.073672, mae: 3.003418, mean_q: -4.332900, mean_eps: 0.969144\n",
      "   4000/200000: episode: 8, duration: 0.776s, episode steps: 500, steps per second: 644, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.952 [0.000, 2.000],  loss: 0.076248, mae: 3.620506, mean_q: -5.248497, mean_eps: 0.964394\n",
      "   4500/200000: episode: 9, duration: 0.763s, episode steps: 500, steps per second: 656, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.085927, mae: 4.241376, mean_q: -6.153058, mean_eps: 0.959644\n",
      "   5000/200000: episode: 10, duration: 0.759s, episode steps: 500, steps per second: 659, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.086347, mae: 4.812004, mean_q: -7.011234, mean_eps: 0.954894\n",
      "   5345/200000: episode: 11, duration: 0.528s, episode steps: 345, steps per second: 654, episode reward: -344.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.152379, mae: 5.392501, mean_q: -7.820120, mean_eps: 0.950866\n",
      "   5845/200000: episode: 12, duration: 0.753s, episode steps: 500, steps per second: 664, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000],  loss: 0.139946, mae: 5.786510, mean_q: -8.436285, mean_eps: 0.946838\n",
      "   6345/200000: episode: 13, duration: 0.756s, episode steps: 500, steps per second: 662, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.143400, mae: 6.285151, mean_q: -9.182654, mean_eps: 0.942088\n",
      "   6845/200000: episode: 14, duration: 0.746s, episode steps: 500, steps per second: 670, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.072 [0.000, 2.000],  loss: 0.119311, mae: 6.834894, mean_q: -10.029716, mean_eps: 0.937338\n",
      "   7345/200000: episode: 15, duration: 0.758s, episode steps: 500, steps per second: 660, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.151226, mae: 7.361208, mean_q: -10.798877, mean_eps: 0.932588\n",
      "   7845/200000: episode: 16, duration: 0.776s, episode steps: 500, steps per second: 644, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.223904, mae: 7.861474, mean_q: -11.520551, mean_eps: 0.927838\n",
      "   8345/200000: episode: 17, duration: 0.776s, episode steps: 500, steps per second: 644, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.086 [0.000, 2.000],  loss: 0.233681, mae: 8.281565, mean_q: -12.133974, mean_eps: 0.923088\n",
      "   8845/200000: episode: 18, duration: 0.752s, episode steps: 500, steps per second: 665, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.297344, mae: 8.734290, mean_q: -12.798489, mean_eps: 0.918338\n",
      "   9345/200000: episode: 19, duration: 0.804s, episode steps: 500, steps per second: 622, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.209050, mae: 9.247206, mean_q: -13.598490, mean_eps: 0.913588\n",
      "   9845/200000: episode: 20, duration: 0.748s, episode steps: 500, steps per second: 668, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.278346, mae: 9.760694, mean_q: -14.338604, mean_eps: 0.908838\n",
      "  10345/200000: episode: 21, duration: 0.758s, episode steps: 500, steps per second: 659, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.230275, mae: 10.274386, mean_q: -15.115040, mean_eps: 0.904088\n",
      "  10845/200000: episode: 22, duration: 0.751s, episode steps: 500, steps per second: 666, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.407616, mae: 10.693777, mean_q: -15.722658, mean_eps: 0.899338\n",
      "  11345/200000: episode: 23, duration: 0.763s, episode steps: 500, steps per second: 656, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.342206, mae: 11.152353, mean_q: -16.414682, mean_eps: 0.894588\n",
      "  11845/200000: episode: 24, duration: 0.783s, episode steps: 500, steps per second: 638, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.288871, mae: 11.689948, mean_q: -17.223967, mean_eps: 0.889838\n",
      "  12345/200000: episode: 25, duration: 0.892s, episode steps: 500, steps per second: 560, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.470724, mae: 12.154683, mean_q: -17.879608, mean_eps: 0.885088\n",
      "  12845/200000: episode: 26, duration: 0.873s, episode steps: 500, steps per second: 573, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.311484, mae: 12.621730, mean_q: -18.545028, mean_eps: 0.880338\n",
      "  13345/200000: episode: 27, duration: 0.838s, episode steps: 500, steps per second: 596, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.405222, mae: 13.009031, mean_q: -19.135677, mean_eps: 0.875588\n",
      "  13845/200000: episode: 28, duration: 0.767s, episode steps: 500, steps per second: 652, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.311437, mae: 13.380589, mean_q: -19.668646, mean_eps: 0.870838\n",
      "  14345/200000: episode: 29, duration: 0.814s, episode steps: 500, steps per second: 614, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.972 [0.000, 2.000],  loss: 0.389914, mae: 13.760402, mean_q: -20.252147, mean_eps: 0.866088\n",
      "  14845/200000: episode: 30, duration: 0.800s, episode steps: 500, steps per second: 625, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.363729, mae: 14.241632, mean_q: -20.994195, mean_eps: 0.861338\n",
      "  15345/200000: episode: 31, duration: 0.779s, episode steps: 500, steps per second: 642, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.566378, mae: 14.605343, mean_q: -21.492837, mean_eps: 0.856588\n",
      "  15845/200000: episode: 32, duration: 0.786s, episode steps: 500, steps per second: 636, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.932 [0.000, 2.000],  loss: 0.418425, mae: 14.887805, mean_q: -21.953989, mean_eps: 0.851838\n",
      "  16345/200000: episode: 33, duration: 0.763s, episode steps: 500, steps per second: 655, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000],  loss: 0.277974, mae: 15.175079, mean_q: -22.395710, mean_eps: 0.847088\n",
      "  16845/200000: episode: 34, duration: 0.758s, episode steps: 500, steps per second: 659, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.457193, mae: 15.577169, mean_q: -22.992833, mean_eps: 0.842338\n",
      "  17345/200000: episode: 35, duration: 0.782s, episode steps: 500, steps per second: 639, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.795194, mae: 15.893268, mean_q: -23.387458, mean_eps: 0.837588\n",
      "  17713/200000: episode: 36, duration: 0.587s, episode steps: 368, steps per second: 626, episode reward: -367.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.892945, mae: 16.334507, mean_q: -24.031815, mean_eps: 0.833446\n",
      "  18213/200000: episode: 37, duration: 0.778s, episode steps: 500, steps per second: 642, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.052 [0.000, 2.000],  loss: 0.463979, mae: 16.548926, mean_q: -24.415582, mean_eps: 0.829342\n",
      "  18713/200000: episode: 38, duration: 0.772s, episode steps: 500, steps per second: 647, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.938 [0.000, 2.000],  loss: 0.737619, mae: 16.837897, mean_q: -24.833255, mean_eps: 0.824592\n",
      "  19204/200000: episode: 39, duration: 0.744s, episode steps: 491, steps per second: 660, episode reward: -490.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.627643, mae: 17.111282, mean_q: -25.234148, mean_eps: 0.819880\n",
      "  19602/200000: episode: 40, duration: 0.630s, episode steps: 398, steps per second: 631, episode reward: -397.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.811511, mae: 17.362811, mean_q: -25.587511, mean_eps: 0.815662\n",
      "  20069/200000: episode: 41, duration: 0.711s, episode steps: 467, steps per second: 656, episode reward: -466.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.667999, mae: 17.561971, mean_q: -25.882186, mean_eps: 0.811558\n",
      "  20416/200000: episode: 42, duration: 0.541s, episode steps: 347, steps per second: 641, episode reward: -346.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.756023, mae: 17.823198, mean_q: -26.241425, mean_eps: 0.807720\n",
      "  20916/200000: episode: 43, duration: 0.786s, episode steps: 500, steps per second: 636, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.954 [0.000, 2.000],  loss: 0.598864, mae: 18.259801, mean_q: -26.860197, mean_eps: 0.803692\n",
      "  21169/200000: episode: 44, duration: 0.412s, episode steps: 253, steps per second: 614, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.388439, mae: 18.491051, mean_q: -27.229420, mean_eps: 0.800082\n",
      "  21602/200000: episode: 45, duration: 0.694s, episode steps: 433, steps per second: 624, episode reward: -432.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.664069, mae: 18.684242, mean_q: -27.522687, mean_eps: 0.796814\n",
      "  22102/200000: episode: 46, duration: 0.769s, episode steps: 500, steps per second: 650, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.734541, mae: 19.073520, mean_q: -28.064563, mean_eps: 0.792406\n",
      "  22409/200000: episode: 47, duration: 0.473s, episode steps: 307, steps per second: 649, episode reward: -306.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 1.262306, mae: 19.241501, mean_q: -28.338708, mean_eps: 0.788568\n",
      "  22889/200000: episode: 48, duration: 0.737s, episode steps: 480, steps per second: 651, episode reward: -479.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 1.268856, mae: 19.558645, mean_q: -28.757614, mean_eps: 0.784806\n",
      "  23175/200000: episode: 49, duration: 0.436s, episode steps: 286, steps per second: 656, episode reward: -285.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 1.210163, mae: 19.909182, mean_q: -29.211473, mean_eps: 0.781196\n",
      "  23497/200000: episode: 50, duration: 0.512s, episode steps: 322, steps per second: 629, episode reward: -321.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.573992, mae: 20.054469, mean_q: -29.537643, mean_eps: 0.778308\n",
      "  23759/200000: episode: 51, duration: 0.424s, episode steps: 262, steps per second: 619, episode reward: -261.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.477516, mae: 20.171740, mean_q: -29.785883, mean_eps: 0.775534\n",
      "  23954/200000: episode: 52, duration: 0.318s, episode steps: 195, steps per second: 614, episode reward: -194.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.785 [0.000, 2.000],  loss: 1.331462, mae: 20.055441, mean_q: -29.530654, mean_eps: 0.773368\n",
      "  24454/200000: episode: 53, duration: 0.772s, episode steps: 500, steps per second: 648, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.868 [0.000, 2.000],  loss: 1.431727, mae: 20.241332, mean_q: -29.775193, mean_eps: 0.770062\n",
      "  24696/200000: episode: 54, duration: 0.381s, episode steps: 242, steps per second: 636, episode reward: -241.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 1.162549, mae: 20.543980, mean_q: -30.156460, mean_eps: 0.766566\n",
      "  24963/200000: episode: 55, duration: 0.420s, episode steps: 267, steps per second: 636, episode reward: -266.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 1.231521, mae: 20.603154, mean_q: -30.349035, mean_eps: 0.764134\n",
      "  25334/200000: episode: 56, duration: 0.597s, episode steps: 371, steps per second: 621, episode reward: -370.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.854 [0.000, 2.000],  loss: 1.355856, mae: 20.601559, mean_q: -30.281899, mean_eps: 0.761094\n",
      "  25621/200000: episode: 57, duration: 0.453s, episode steps: 287, steps per second: 633, episode reward: -286.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.870794, mae: 20.649973, mean_q: -30.407490, mean_eps: 0.757978\n",
      "  25789/200000: episode: 58, duration: 0.259s, episode steps: 168, steps per second: 650, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 1.183722, mae: 20.713654, mean_q: -30.472323, mean_eps: 0.755812\n",
      "  26128/200000: episode: 59, duration: 0.547s, episode steps: 339, steps per second: 620, episode reward: -338.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 1.518145, mae: 20.740191, mean_q: -30.487339, mean_eps: 0.753418\n",
      "  26471/200000: episode: 60, duration: 0.549s, episode steps: 343, steps per second: 624, episode reward: -342.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.723427, mae: 20.668559, mean_q: -30.435688, mean_eps: 0.750188\n",
      "  26707/200000: episode: 61, duration: 0.388s, episode steps: 236, steps per second: 608, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 1.112101, mae: 20.897463, mean_q: -30.746625, mean_eps: 0.747414\n",
      "  26880/200000: episode: 62, duration: 0.285s, episode steps: 173, steps per second: 606, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.693005, mae: 20.853442, mean_q: -30.653889, mean_eps: 0.745476\n",
      "  27132/200000: episode: 63, duration: 0.415s, episode steps: 252, steps per second: 608, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 1.274481, mae: 21.069144, mean_q: -30.985826, mean_eps: 0.743462\n",
      "  27409/200000: episode: 64, duration: 0.435s, episode steps: 277, steps per second: 637, episode reward: -276.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 1.288239, mae: 21.252946, mean_q: -31.303032, mean_eps: 0.740916\n",
      "  27737/200000: episode: 65, duration: 0.514s, episode steps: 328, steps per second: 638, episode reward: -327.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 1.287803, mae: 21.321829, mean_q: -31.396350, mean_eps: 0.738028\n",
      "  27979/200000: episode: 66, duration: 0.368s, episode steps: 242, steps per second: 657, episode reward: -241.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 1.291374, mae: 21.369198, mean_q: -31.425117, mean_eps: 0.735330\n",
      "  28217/200000: episode: 67, duration: 0.378s, episode steps: 238, steps per second: 630, episode reward: -237.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.860551, mae: 21.822601, mean_q: -32.144233, mean_eps: 0.733050\n",
      "  28717/200000: episode: 68, duration: 0.768s, episode steps: 500, steps per second: 651, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 1.030522, mae: 22.095364, mean_q: -32.569506, mean_eps: 0.729554\n",
      "  28900/200000: episode: 69, duration: 0.285s, episode steps: 183, steps per second: 641, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 1.327435, mae: 22.167983, mean_q: -32.680760, mean_eps: 0.726324\n",
      "  29082/200000: episode: 70, duration: 0.290s, episode steps: 182, steps per second: 628, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 1.136018, mae: 22.444412, mean_q: -33.026149, mean_eps: 0.724576\n",
      "  29400/200000: episode: 71, duration: 0.503s, episode steps: 318, steps per second: 632, episode reward: -317.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.644766, mae: 22.576207, mean_q: -33.279827, mean_eps: 0.722220\n",
      "  29621/200000: episode: 72, duration: 0.355s, episode steps: 221, steps per second: 622, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 1.523481, mae: 22.626509, mean_q: -33.323943, mean_eps: 0.719674\n",
      "  29889/200000: episode: 73, duration: 0.421s, episode steps: 268, steps per second: 636, episode reward: -267.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 1.520687, mae: 22.903518, mean_q: -33.716523, mean_eps: 0.717318\n",
      "  30317/200000: episode: 74, duration: 0.662s, episode steps: 428, steps per second: 646, episode reward: -427.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 1.438193, mae: 22.835000, mean_q: -33.612438, mean_eps: 0.714012\n",
      "  30602/200000: episode: 75, duration: 0.449s, episode steps: 285, steps per second: 635, episode reward: -284.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 1.426551, mae: 22.905092, mean_q: -33.676266, mean_eps: 0.710630\n",
      "  30796/200000: episode: 76, duration: 0.299s, episode steps: 194, steps per second: 649, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.734818, mae: 23.255429, mean_q: -34.316796, mean_eps: 0.708350\n",
      "  31021/200000: episode: 77, duration: 0.352s, episode steps: 225, steps per second: 639, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 1.235251, mae: 23.245851, mean_q: -34.254617, mean_eps: 0.706374\n",
      "  31301/200000: episode: 78, duration: 0.437s, episode steps: 280, steps per second: 641, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.963004, mae: 23.356017, mean_q: -34.449432, mean_eps: 0.703980\n",
      "  31618/200000: episode: 79, duration: 0.501s, episode steps: 317, steps per second: 633, episode reward: -316.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.386694, mae: 23.411075, mean_q: -34.449476, mean_eps: 0.701130\n",
      "  31871/200000: episode: 80, duration: 0.389s, episode steps: 253, steps per second: 650, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 1.598779, mae: 23.607628, mean_q: -34.716413, mean_eps: 0.698432\n",
      "  32119/200000: episode: 81, duration: 0.392s, episode steps: 248, steps per second: 633, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 1.353346, mae: 23.475100, mean_q: -34.554167, mean_eps: 0.696076\n",
      "  32416/200000: episode: 82, duration: 0.468s, episode steps: 297, steps per second: 635, episode reward: -296.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 1.251353, mae: 23.707428, mean_q: -34.930732, mean_eps: 0.693492\n",
      "  32840/200000: episode: 83, duration: 0.673s, episode steps: 424, steps per second: 630, episode reward: -423.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 1.427002, mae: 23.833682, mean_q: -35.082383, mean_eps: 0.690072\n",
      "  33152/200000: episode: 84, duration: 0.502s, episode steps: 312, steps per second: 622, episode reward: -311.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 1.381204, mae: 23.786054, mean_q: -34.995732, mean_eps: 0.686576\n",
      "  33339/200000: episode: 85, duration: 0.300s, episode steps: 187, steps per second: 624, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 1.022024, mae: 24.013237, mean_q: -35.369106, mean_eps: 0.684182\n",
      "  33684/200000: episode: 86, duration: 0.552s, episode steps: 345, steps per second: 625, episode reward: -344.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 1.766641, mae: 24.040897, mean_q: -35.374986, mean_eps: 0.681636\n",
      "  33935/200000: episode: 87, duration: 0.394s, episode steps: 251, steps per second: 637, episode reward: -250.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 1.908085, mae: 24.146594, mean_q: -35.536885, mean_eps: 0.678824\n",
      "  34242/200000: episode: 88, duration: 0.518s, episode steps: 307, steps per second: 593, episode reward: -306.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.003 [0.000, 2.000],  loss: 0.452521, mae: 24.487774, mean_q: -36.111999, mean_eps: 0.676164\n",
      "  34590/200000: episode: 89, duration: 0.552s, episode steps: 348, steps per second: 631, episode reward: -347.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 1.812679, mae: 24.663006, mean_q: -36.357552, mean_eps: 0.673048\n",
      "  35004/200000: episode: 90, duration: 0.650s, episode steps: 414, steps per second: 637, episode reward: -413.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 1.352349, mae: 24.736878, mean_q: -36.507513, mean_eps: 0.669438\n",
      "  35325/200000: episode: 91, duration: 0.504s, episode steps: 321, steps per second: 637, episode reward: -320.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.003 [0.000, 2.000],  loss: 1.102313, mae: 25.103459, mean_q: -37.023816, mean_eps: 0.665942\n",
      "  35530/200000: episode: 92, duration: 0.333s, episode steps: 205, steps per second: 616, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 2.773381, mae: 25.021235, mean_q: -36.855835, mean_eps: 0.663434\n",
      "  35848/200000: episode: 93, duration: 0.504s, episode steps: 318, steps per second: 631, episode reward: -317.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 1.475467, mae: 25.170714, mean_q: -37.082499, mean_eps: 0.660964\n",
      "  36021/200000: episode: 94, duration: 0.282s, episode steps: 173, steps per second: 613, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 2.599743, mae: 25.220595, mean_q: -37.097854, mean_eps: 0.658646\n",
      "  36165/200000: episode: 95, duration: 0.225s, episode steps: 144, steps per second: 639, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 1.106459, mae: 25.797937, mean_q: -38.025159, mean_eps: 0.657126\n",
      "  36361/200000: episode: 96, duration: 0.317s, episode steps: 196, steps per second: 618, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 1.358330, mae: 25.804977, mean_q: -38.050745, mean_eps: 0.655492\n",
      "  36861/200000: episode: 97, duration: 0.787s, episode steps: 500, steps per second: 636, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 1.187199, mae: 26.047915, mean_q: -38.425503, mean_eps: 0.652186\n",
      "  37116/200000: episode: 98, duration: 0.413s, episode steps: 255, steps per second: 617, episode reward: -254.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 1.886383, mae: 26.190288, mean_q: -38.573378, mean_eps: 0.648614\n",
      "  37306/200000: episode: 99, duration: 0.306s, episode steps: 190, steps per second: 621, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 1.514760, mae: 26.460890, mean_q: -38.958669, mean_eps: 0.646486\n",
      "  37565/200000: episode: 100, duration: 0.412s, episode steps: 259, steps per second: 629, episode reward: -258.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 1.511421, mae: 26.554193, mean_q: -39.111308, mean_eps: 0.644358\n",
      "  37832/200000: episode: 101, duration: 0.425s, episode steps: 267, steps per second: 628, episode reward: -266.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.184 [0.000, 2.000],  loss: 2.016195, mae: 26.460693, mean_q: -38.957626, mean_eps: 0.641888\n",
      "  38067/200000: episode: 102, duration: 0.377s, episode steps: 235, steps per second: 624, episode reward: -234.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.272 [0.000, 2.000],  loss: 2.258496, mae: 26.440969, mean_q: -38.912760, mean_eps: 0.639494\n",
      "  38464/200000: episode: 103, duration: 0.632s, episode steps: 397, steps per second: 629, episode reward: -396.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.154 [0.000, 2.000],  loss: 1.848698, mae: 26.441007, mean_q: -38.986964, mean_eps: 0.636492\n",
      "  38634/200000: episode: 104, duration: 0.279s, episode steps: 170, steps per second: 610, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 1.700839, mae: 26.659391, mean_q: -39.289472, mean_eps: 0.633794\n",
      "  38842/200000: episode: 105, duration: 0.329s, episode steps: 208, steps per second: 633, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 1.655996, mae: 26.602234, mean_q: -39.262731, mean_eps: 0.631970\n",
      "  39100/200000: episode: 106, duration: 0.403s, episode steps: 258, steps per second: 641, episode reward: -257.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 1.880402, mae: 26.859024, mean_q: -39.603691, mean_eps: 0.629766\n",
      "  39538/200000: episode: 107, duration: 0.683s, episode steps: 438, steps per second: 641, episode reward: -437.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 1.513636, mae: 27.202076, mean_q: -40.149837, mean_eps: 0.626460\n",
      "  39799/200000: episode: 108, duration: 0.401s, episode steps: 261, steps per second: 651, episode reward: -260.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 3.019913, mae: 27.007641, mean_q: -39.738496, mean_eps: 0.623154\n",
      "  40031/200000: episode: 109, duration: 0.365s, episode steps: 232, steps per second: 636, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 2.273235, mae: 27.011559, mean_q: -39.755887, mean_eps: 0.620836\n",
      "  40178/200000: episode: 110, duration: 0.239s, episode steps: 147, steps per second: 614, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 1.078857, mae: 27.101857, mean_q: -39.915282, mean_eps: 0.619012\n",
      "  40453/200000: episode: 111, duration: 0.438s, episode steps: 275, steps per second: 627, episode reward: -274.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 0.565839, mae: 26.970360, mean_q: -39.819504, mean_eps: 0.616998\n",
      "  40751/200000: episode: 112, duration: 0.484s, episode steps: 298, steps per second: 616, episode reward: -297.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.812 [0.000, 2.000],  loss: 2.492699, mae: 26.983738, mean_q: -39.735287, mean_eps: 0.614300\n",
      "  40949/200000: episode: 113, duration: 0.324s, episode steps: 198, steps per second: 612, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 2.226870, mae: 27.062440, mean_q: -39.913060, mean_eps: 0.611944\n",
      "  41437/200000: episode: 114, duration: 0.787s, episode steps: 488, steps per second: 620, episode reward: -487.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 1.116943, mae: 27.059330, mean_q: -39.899542, mean_eps: 0.608676\n",
      "  41582/200000: episode: 115, duration: 0.237s, episode steps: 145, steps per second: 611, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 2.171373, mae: 26.628247, mean_q: -39.147764, mean_eps: 0.605674\n",
      "  41742/200000: episode: 116, duration: 0.258s, episode steps: 160, steps per second: 621, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 2.761438, mae: 26.544433, mean_q: -38.965948, mean_eps: 0.604230\n",
      "  41972/200000: episode: 117, duration: 0.367s, episode steps: 230, steps per second: 627, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 2.335542, mae: 26.656385, mean_q: -39.252172, mean_eps: 0.602368\n",
      "  42132/200000: episode: 118, duration: 0.256s, episode steps: 160, steps per second: 625, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 4.247437, mae: 26.821676, mean_q: -39.396481, mean_eps: 0.600506\n",
      "  42290/200000: episode: 119, duration: 0.257s, episode steps: 158, steps per second: 614, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 1.567200, mae: 26.776599, mean_q: -39.401239, mean_eps: 0.598986\n",
      "  42507/200000: episode: 120, duration: 0.352s, episode steps: 217, steps per second: 616, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 2.019649, mae: 26.707339, mean_q: -39.352786, mean_eps: 0.597200\n",
      "  42703/200000: episode: 121, duration: 0.314s, episode steps: 196, steps per second: 624, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.444346, mae: 26.958012, mean_q: -39.695777, mean_eps: 0.595262\n",
      "  42970/200000: episode: 122, duration: 0.425s, episode steps: 267, steps per second: 628, episode reward: -266.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 2.102855, mae: 26.876043, mean_q: -39.632139, mean_eps: 0.593058\n",
      "  43160/200000: episode: 123, duration: 0.309s, episode steps: 190, steps per second: 615, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 1.773735, mae: 27.336386, mean_q: -40.301156, mean_eps: 0.590892\n",
      "  43433/200000: episode: 124, duration: 0.448s, episode steps: 273, steps per second: 609, episode reward: -272.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 1.759731, mae: 27.213984, mean_q: -40.103719, mean_eps: 0.588688\n",
      "  43625/200000: episode: 125, duration: 0.314s, episode steps: 192, steps per second: 612, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 1.870031, mae: 27.218037, mean_q: -40.116278, mean_eps: 0.586446\n",
      "  43863/200000: episode: 126, duration: 0.378s, episode steps: 238, steps per second: 629, episode reward: -237.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 1.891442, mae: 27.356095, mean_q: -40.317375, mean_eps: 0.584432\n",
      "  44034/200000: episode: 127, duration: 0.283s, episode steps: 171, steps per second: 604, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.813 [0.000, 2.000],  loss: 1.932240, mae: 27.392861, mean_q: -40.426821, mean_eps: 0.582494\n",
      "  44224/200000: episode: 128, duration: 0.314s, episode steps: 190, steps per second: 605, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.106164, mae: 27.372462, mean_q: -40.416711, mean_eps: 0.580784\n",
      "  44433/200000: episode: 129, duration: 0.342s, episode steps: 209, steps per second: 611, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.946086, mae: 27.288090, mean_q: -40.303116, mean_eps: 0.578884\n",
      "  44603/200000: episode: 130, duration: 0.279s, episode steps: 170, steps per second: 610, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.910488, mae: 27.460277, mean_q: -40.555526, mean_eps: 0.577060\n",
      "  44848/200000: episode: 131, duration: 0.400s, episode steps: 245, steps per second: 612, episode reward: -244.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 1.722933, mae: 27.445948, mean_q: -40.484457, mean_eps: 0.575122\n",
      "  45007/200000: episode: 132, duration: 0.260s, episode steps: 159, steps per second: 612, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 2.160547, mae: 27.364103, mean_q: -40.352224, mean_eps: 0.573222\n",
      "  45278/200000: episode: 133, duration: 0.437s, episode steps: 271, steps per second: 620, episode reward: -270.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 1.226146, mae: 27.755540, mean_q: -40.987996, mean_eps: 0.571170\n",
      "  45411/200000: episode: 134, duration: 0.215s, episode steps: 133, steps per second: 619, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 1.923455, mae: 27.619578, mean_q: -40.723063, mean_eps: 0.569232\n",
      "  45656/200000: episode: 135, duration: 0.399s, episode steps: 245, steps per second: 614, episode reward: -244.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 1.434310, mae: 27.704427, mean_q: -40.861032, mean_eps: 0.567446\n",
      "  45854/200000: episode: 136, duration: 0.317s, episode steps: 198, steps per second: 625, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 2.143105, mae: 28.025915, mean_q: -41.311729, mean_eps: 0.565356\n",
      "  46044/200000: episode: 137, duration: 0.306s, episode steps: 190, steps per second: 620, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 2.399170, mae: 28.061196, mean_q: -41.356150, mean_eps: 0.563494\n",
      "  46219/200000: episode: 138, duration: 0.281s, episode steps: 175, steps per second: 622, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 1.131325, mae: 28.509257, mean_q: -42.042911, mean_eps: 0.561746\n",
      "  46458/200000: episode: 139, duration: 0.388s, episode steps: 239, steps per second: 615, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 2.243481, mae: 28.428426, mean_q: -41.919165, mean_eps: 0.559770\n",
      "  46660/200000: episode: 140, duration: 0.345s, episode steps: 202, steps per second: 585, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 3.264307, mae: 28.655349, mean_q: -42.229185, mean_eps: 0.557680\n",
      "  46841/200000: episode: 141, duration: 0.314s, episode steps: 181, steps per second: 577, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 1.036632, mae: 28.764600, mean_q: -42.493733, mean_eps: 0.555856\n",
      "  47057/200000: episode: 142, duration: 0.352s, episode steps: 216, steps per second: 613, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 2.554674, mae: 28.466401, mean_q: -41.949235, mean_eps: 0.553956\n",
      "  47241/200000: episode: 143, duration: 0.296s, episode steps: 184, steps per second: 622, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 1.667618, mae: 28.424914, mean_q: -41.858038, mean_eps: 0.552056\n",
      "  47528/200000: episode: 144, duration: 0.462s, episode steps: 287, steps per second: 622, episode reward: -286.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 1.444549, mae: 28.475619, mean_q: -42.044910, mean_eps: 0.549852\n",
      "  47697/200000: episode: 145, duration: 0.279s, episode steps: 169, steps per second: 605, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.751099, mae: 28.246741, mean_q: -41.642919, mean_eps: 0.547686\n",
      "  47900/200000: episode: 146, duration: 0.326s, episode steps: 203, steps per second: 622, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.798 [0.000, 2.000],  loss: 1.181440, mae: 28.279603, mean_q: -41.710612, mean_eps: 0.545900\n",
      "  48065/200000: episode: 147, duration: 0.273s, episode steps: 165, steps per second: 604, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 1.464634, mae: 28.432009, mean_q: -41.910694, mean_eps: 0.544152\n",
      "  48208/200000: episode: 148, duration: 0.235s, episode steps: 143, steps per second: 608, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 2.042322, mae: 28.186427, mean_q: -41.498797, mean_eps: 0.542708\n",
      "  48450/200000: episode: 149, duration: 0.403s, episode steps: 242, steps per second: 600, episode reward: -241.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 2.549738, mae: 28.734792, mean_q: -42.295363, mean_eps: 0.540884\n",
      "  48614/200000: episode: 150, duration: 0.274s, episode steps: 164, steps per second: 598, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 2.911450, mae: 28.631543, mean_q: -42.122039, mean_eps: 0.538946\n",
      "  48787/200000: episode: 151, duration: 0.295s, episode steps: 173, steps per second: 587, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 2.160178, mae: 28.375640, mean_q: -41.775257, mean_eps: 0.537350\n",
      "  48915/200000: episode: 152, duration: 0.208s, episode steps: 128, steps per second: 615, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 1.115250, mae: 28.351968, mean_q: -41.806484, mean_eps: 0.535906\n",
      "  49089/200000: episode: 153, duration: 0.292s, episode steps: 174, steps per second: 595, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.787 [0.000, 2.000],  loss: 1.396451, mae: 28.312642, mean_q: -41.660898, mean_eps: 0.534462\n",
      "  49213/200000: episode: 154, duration: 0.197s, episode steps: 124, steps per second: 629, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.839 [0.000, 2.000],  loss: 2.028923, mae: 28.255872, mean_q: -41.600437, mean_eps: 0.533056\n",
      "  49376/200000: episode: 155, duration: 0.271s, episode steps: 163, steps per second: 603, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 1.192920, mae: 28.300872, mean_q: -41.690460, mean_eps: 0.531726\n",
      "  49526/200000: episode: 156, duration: 0.249s, episode steps: 150, steps per second: 603, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 1.086259, mae: 28.117988, mean_q: -41.414379, mean_eps: 0.530244\n",
      "  49704/200000: episode: 157, duration: 0.296s, episode steps: 178, steps per second: 602, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 2.725009, mae: 28.509101, mean_q: -41.972487, mean_eps: 0.528686\n",
      "  49877/200000: episode: 158, duration: 0.286s, episode steps: 173, steps per second: 604, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 1.384078, mae: 28.390526, mean_q: -41.778518, mean_eps: 0.527014\n",
      "  49989/200000: episode: 159, duration: 0.177s, episode steps: 112, steps per second: 634, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 0.763323, mae: 28.168306, mean_q: -41.508108, mean_eps: 0.525646\n",
      "  50208/200000: episode: 160, duration: 0.362s, episode steps: 219, steps per second: 605, episode reward: -218.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 2.173882, mae: 28.585367, mean_q: -41.971340, mean_eps: 0.524088\n",
      "  50351/200000: episode: 161, duration: 0.233s, episode steps: 143, steps per second: 613, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 2.845554, mae: 28.907587, mean_q: -42.601959, mean_eps: 0.522378\n",
      "  50509/200000: episode: 162, duration: 0.258s, episode steps: 158, steps per second: 612, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 3.347986, mae: 28.723955, mean_q: -42.346016, mean_eps: 0.520934\n",
      "  50666/200000: episode: 163, duration: 0.262s, episode steps: 157, steps per second: 600, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.764 [0.000, 2.000],  loss: 1.823938, mae: 28.672414, mean_q: -42.219832, mean_eps: 0.519414\n",
      "  50806/200000: episode: 164, duration: 0.237s, episode steps: 140, steps per second: 591, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 2.722656, mae: 28.633319, mean_q: -42.104546, mean_eps: 0.518008\n",
      "  50994/200000: episode: 165, duration: 0.311s, episode steps: 188, steps per second: 604, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 1.463125, mae: 28.315458, mean_q: -41.747859, mean_eps: 0.516450\n",
      "  51193/200000: episode: 166, duration: 0.322s, episode steps: 199, steps per second: 618, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 2.169121, mae: 28.832032, mean_q: -42.435012, mean_eps: 0.514588\n",
      "  51312/200000: episode: 167, duration: 0.197s, episode steps: 119, steps per second: 603, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 1.395630, mae: 28.546623, mean_q: -42.027045, mean_eps: 0.513106\n",
      "  51499/200000: episode: 168, duration: 0.303s, episode steps: 187, steps per second: 616, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 2.738719, mae: 28.699815, mean_q: -42.183236, mean_eps: 0.511662\n",
      "  51651/200000: episode: 169, duration: 0.246s, episode steps: 152, steps per second: 617, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 1.722939, mae: 28.197368, mean_q: -41.405258, mean_eps: 0.510028\n",
      "  51782/200000: episode: 170, duration: 0.207s, episode steps: 131, steps per second: 632, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 1.268402, mae: 28.219233, mean_q: -41.530736, mean_eps: 0.508698\n",
      "  51945/200000: episode: 171, duration: 0.271s, episode steps: 163, steps per second: 602, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 1.056048, mae: 27.978621, mean_q: -41.188235, mean_eps: 0.507292\n",
      "  52050/200000: episode: 172, duration: 0.169s, episode steps: 105, steps per second: 620, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.790969, mae: 28.418097, mean_q: -41.849145, mean_eps: 0.506000\n",
      "  52375/200000: episode: 173, duration: 0.520s, episode steps: 325, steps per second: 625, episode reward: -324.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.809 [0.000, 2.000],  loss: 2.262170, mae: 28.064368, mean_q: -41.218348, mean_eps: 0.503986\n",
      "  52496/200000: episode: 174, duration: 0.200s, episode steps: 121, steps per second: 606, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 1.326432, mae: 28.218111, mean_q: -41.574952, mean_eps: 0.501896\n",
      "  52717/200000: episode: 175, duration: 0.393s, episode steps: 221, steps per second: 562, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.963196, mae: 28.148273, mean_q: -41.426578, mean_eps: 0.500262\n",
      "  52854/200000: episode: 176, duration: 0.220s, episode steps: 137, steps per second: 622, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 1.869773, mae: 28.068918, mean_q: -41.308002, mean_eps: 0.498552\n",
      "  53001/200000: episode: 177, duration: 0.247s, episode steps: 147, steps per second: 596, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 2.183388, mae: 28.075701, mean_q: -41.367507, mean_eps: 0.497184\n",
      "  53147/200000: episode: 178, duration: 0.236s, episode steps: 146, steps per second: 617, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 1.129083, mae: 28.017070, mean_q: -41.174229, mean_eps: 0.495778\n",
      "  53312/200000: episode: 179, duration: 0.270s, episode steps: 165, steps per second: 611, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 1.497217, mae: 27.972627, mean_q: -41.187435, mean_eps: 0.494334\n",
      "  53482/200000: episode: 180, duration: 0.283s, episode steps: 170, steps per second: 601, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 1.092873, mae: 28.004716, mean_q: -41.248750, mean_eps: 0.492738\n",
      "  53621/200000: episode: 181, duration: 0.226s, episode steps: 139, steps per second: 616, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 1.520508, mae: 28.150565, mean_q: -41.437750, mean_eps: 0.491256\n",
      "  53784/200000: episode: 182, duration: 0.268s, episode steps: 163, steps per second: 608, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.847 [0.000, 2.000],  loss: 1.022961, mae: 28.110463, mean_q: -41.477035, mean_eps: 0.489850\n",
      "  53960/200000: episode: 183, duration: 0.294s, episode steps: 176, steps per second: 600, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.818 [0.000, 2.000],  loss: 2.145597, mae: 27.841760, mean_q: -41.008279, mean_eps: 0.488254\n",
      "  54116/200000: episode: 184, duration: 0.264s, episode steps: 156, steps per second: 591, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 1.664478, mae: 27.924193, mean_q: -41.069946, mean_eps: 0.486658\n",
      "  54267/200000: episode: 185, duration: 0.244s, episode steps: 151, steps per second: 620, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.906841, mae: 27.966216, mean_q: -41.149852, mean_eps: 0.485176\n",
      "  54403/200000: episode: 186, duration: 0.219s, episode steps: 136, steps per second: 622, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 1.211383, mae: 28.031811, mean_q: -41.252258, mean_eps: 0.483808\n",
      "  54537/200000: episode: 187, duration: 0.218s, episode steps: 134, steps per second: 614, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.761 [0.000, 2.000],  loss: 3.019431, mae: 27.903202, mean_q: -41.041064, mean_eps: 0.482516\n",
      "  54760/200000: episode: 188, duration: 0.371s, episode steps: 223, steps per second: 602, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.961399, mae: 27.698571, mean_q: -40.814501, mean_eps: 0.480844\n",
      "  54973/200000: episode: 189, duration: 0.345s, episode steps: 213, steps per second: 618, episode reward: -212.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.765 [0.000, 2.000],  loss: 1.672020, mae: 27.942193, mean_q: -41.143387, mean_eps: 0.478792\n",
      "  55118/200000: episode: 190, duration: 0.241s, episode steps: 145, steps per second: 602, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.814 [0.000, 2.000],  loss: 2.570760, mae: 28.511515, mean_q: -41.885780, mean_eps: 0.477082\n",
      "  55293/200000: episode: 191, duration: 0.285s, episode steps: 175, steps per second: 614, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 1.750644, mae: 27.973980, mean_q: -41.095310, mean_eps: 0.475562\n",
      "  55457/200000: episode: 192, duration: 0.271s, episode steps: 164, steps per second: 605, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 2.172782, mae: 28.168638, mean_q: -41.315777, mean_eps: 0.473928\n",
      "  55690/200000: episode: 193, duration: 0.378s, episode steps: 233, steps per second: 616, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.648 [0.000, 2.000],  loss: 2.194294, mae: 28.533046, mean_q: -41.908684, mean_eps: 0.472028\n",
      "  55847/200000: episode: 194, duration: 0.257s, episode steps: 157, steps per second: 611, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.732 [0.000, 2.000],  loss: 1.021112, mae: 28.520213, mean_q: -41.997127, mean_eps: 0.470204\n",
      "  56002/200000: episode: 195, duration: 0.264s, episode steps: 155, steps per second: 587, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.684 [0.000, 2.000],  loss: 1.729102, mae: 28.380299, mean_q: -41.804808, mean_eps: 0.468722\n",
      "  56144/200000: episode: 196, duration: 0.234s, episode steps: 142, steps per second: 606, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.995189, mae: 28.390697, mean_q: -41.711926, mean_eps: 0.467316\n",
      "  56313/200000: episode: 197, duration: 0.281s, episode steps: 169, steps per second: 602, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.775 [0.000, 2.000],  loss: 0.810902, mae: 28.468572, mean_q: -41.940324, mean_eps: 0.465834\n",
      "  56442/200000: episode: 198, duration: 0.214s, episode steps: 129, steps per second: 604, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.648483, mae: 28.067244, mean_q: -41.366713, mean_eps: 0.464390\n",
      "  56567/200000: episode: 199, duration: 0.211s, episode steps: 125, steps per second: 593, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 2.474365, mae: 28.550989, mean_q: -41.974620, mean_eps: 0.463212\n",
      "  56729/200000: episode: 200, duration: 0.282s, episode steps: 162, steps per second: 574, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.809 [0.000, 2.000],  loss: 0.776611, mae: 28.226208, mean_q: -41.527305, mean_eps: 0.461844\n",
      "  57015/200000: episode: 201, duration: 0.476s, episode steps: 286, steps per second: 601, episode reward: -285.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.671 [0.000, 2.000],  loss: 1.119182, mae: 28.250070, mean_q: -41.626952, mean_eps: 0.459716\n",
      "  57204/200000: episode: 202, duration: 0.321s, episode steps: 189, steps per second: 590, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 1.314494, mae: 28.194856, mean_q: -41.446614, mean_eps: 0.457474\n",
      "  57482/200000: episode: 203, duration: 0.463s, episode steps: 278, steps per second: 600, episode reward: -277.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.669 [0.000, 2.000],  loss: 1.637256, mae: 28.247316, mean_q: -41.577585, mean_eps: 0.455232\n",
      "  57641/200000: episode: 204, duration: 0.260s, episode steps: 159, steps per second: 611, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.881 [0.000, 2.000],  loss: 0.851892, mae: 28.133909, mean_q: -41.395872, mean_eps: 0.453142\n",
      "  57813/200000: episode: 205, duration: 0.278s, episode steps: 172, steps per second: 619, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 2.173921, mae: 28.317636, mean_q: -41.678198, mean_eps: 0.451584\n",
      "  57969/200000: episode: 206, duration: 0.263s, episode steps: 156, steps per second: 593, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 1.565015, mae: 28.561869, mean_q: -42.030026, mean_eps: 0.450026\n",
      "  58099/200000: episode: 207, duration: 0.217s, episode steps: 130, steps per second: 599, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 2.685333, mae: 28.431702, mean_q: -41.735949, mean_eps: 0.448658\n",
      "  58259/200000: episode: 208, duration: 0.269s, episode steps: 160, steps per second: 594, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 2.089819, mae: 28.518852, mean_q: -41.909744, mean_eps: 0.447290\n",
      "  58411/200000: episode: 209, duration: 0.258s, episode steps: 152, steps per second: 589, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 1.461394, mae: 28.771460, mean_q: -42.390931, mean_eps: 0.445808\n",
      "  58602/200000: episode: 210, duration: 0.331s, episode steps: 191, steps per second: 576, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 2.254527, mae: 28.626863, mean_q: -42.052770, mean_eps: 0.444174\n",
      "  58771/200000: episode: 211, duration: 0.284s, episode steps: 169, steps per second: 595, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 1.109270, mae: 28.468325, mean_q: -41.906496, mean_eps: 0.442464\n",
      "  58955/200000: episode: 212, duration: 0.303s, episode steps: 184, steps per second: 607, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.944124, mae: 28.470917, mean_q: -41.937935, mean_eps: 0.440792\n",
      "  59080/200000: episode: 213, duration: 0.217s, episode steps: 125, steps per second: 577, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 0.902816, mae: 28.327323, mean_q: -41.652633, mean_eps: 0.439348\n",
      "  59230/200000: episode: 214, duration: 0.259s, episode steps: 150, steps per second: 578, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 1.764867, mae: 28.561839, mean_q: -42.002537, mean_eps: 0.438056\n",
      "  59383/200000: episode: 215, duration: 0.257s, episode steps: 153, steps per second: 596, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 1.583400, mae: 28.564261, mean_q: -42.017418, mean_eps: 0.436612\n",
      "  59544/200000: episode: 216, duration: 0.280s, episode steps: 161, steps per second: 574, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 1.745813, mae: 28.414094, mean_q: -41.799041, mean_eps: 0.435130\n",
      "  59721/200000: episode: 217, duration: 0.306s, episode steps: 177, steps per second: 579, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 2.078539, mae: 28.603318, mean_q: -42.051142, mean_eps: 0.433496\n",
      "  59824/200000: episode: 218, duration: 0.172s, episode steps: 103, steps per second: 599, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 2.049967, mae: 28.274463, mean_q: -41.543812, mean_eps: 0.432166\n",
      "  59941/200000: episode: 219, duration: 0.198s, episode steps: 117, steps per second: 591, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 2.314998, mae: 28.231414, mean_q: -41.471372, mean_eps: 0.431140\n",
      "  60105/200000: episode: 220, duration: 0.286s, episode steps: 164, steps per second: 573, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.799107, mae: 28.563221, mean_q: -42.016905, mean_eps: 0.429772\n",
      "  60282/200000: episode: 221, duration: 0.291s, episode steps: 177, steps per second: 608, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.808 [0.000, 2.000],  loss: 0.678367, mae: 28.515105, mean_q: -41.979494, mean_eps: 0.428138\n",
      "  60433/200000: episode: 222, duration: 0.250s, episode steps: 151, steps per second: 605, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.558028, mae: 28.859528, mean_q: -42.575136, mean_eps: 0.426580\n",
      "  60601/200000: episode: 223, duration: 0.288s, episode steps: 168, steps per second: 583, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.810 [0.000, 2.000],  loss: 2.621070, mae: 28.773550, mean_q: -42.344414, mean_eps: 0.425060\n",
      "  60811/200000: episode: 224, duration: 0.343s, episode steps: 210, steps per second: 612, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.719 [0.000, 2.000],  loss: 0.851060, mae: 28.537597, mean_q: -42.025199, mean_eps: 0.423274\n",
      "  60944/200000: episode: 225, duration: 0.227s, episode steps: 133, steps per second: 585, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.888245, mae: 28.564016, mean_q: -42.078307, mean_eps: 0.421678\n",
      "  61112/200000: episode: 226, duration: 0.289s, episode steps: 168, steps per second: 581, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 1.553874, mae: 28.607506, mean_q: -42.091469, mean_eps: 0.420272\n",
      "  61258/200000: episode: 227, duration: 0.250s, episode steps: 146, steps per second: 584, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 0.787906, mae: 28.946835, mean_q: -42.607978, mean_eps: 0.418752\n",
      "  61381/200000: episode: 228, duration: 0.199s, episode steps: 123, steps per second: 619, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 1.564762, mae: 29.182733, mean_q: -42.975005, mean_eps: 0.417460\n",
      "  61503/200000: episode: 229, duration: 0.198s, episode steps: 122, steps per second: 615, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 1.880404, mae: 28.773413, mean_q: -42.321635, mean_eps: 0.416320\n",
      "  61692/200000: episode: 230, duration: 0.312s, episode steps: 189, steps per second: 605, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.839437, mae: 28.920805, mean_q: -42.572203, mean_eps: 0.414838\n",
      "  61830/200000: episode: 231, duration: 0.225s, episode steps: 138, steps per second: 612, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 1.154469, mae: 29.471384, mean_q: -43.416283, mean_eps: 0.413280\n",
      "  61975/200000: episode: 232, duration: 0.239s, episode steps: 145, steps per second: 606, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.989176, mae: 28.933496, mean_q: -42.581623, mean_eps: 0.411950\n",
      "  62120/200000: episode: 233, duration: 0.252s, episode steps: 145, steps per second: 576, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 2.054240, mae: 29.501517, mean_q: -43.412379, mean_eps: 0.410582\n",
      "  62313/200000: episode: 234, duration: 0.327s, episode steps: 193, steps per second: 590, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.808 [0.000, 2.000],  loss: 1.823438, mae: 29.504386, mean_q: -43.425579, mean_eps: 0.408948\n",
      "  62434/200000: episode: 235, duration: 0.204s, episode steps: 121, steps per second: 592, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.609432, mae: 29.930757, mean_q: -44.214617, mean_eps: 0.407428\n",
      "  62558/200000: episode: 236, duration: 0.222s, episode steps: 124, steps per second: 558, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.788249, mae: 29.241549, mean_q: -43.102418, mean_eps: 0.406288\n",
      "  62719/200000: episode: 237, duration: 0.274s, episode steps: 161, steps per second: 589, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 1.567426, mae: 29.178120, mean_q: -43.007429, mean_eps: 0.404958\n",
      "  62866/200000: episode: 238, duration: 0.254s, episode steps: 147, steps per second: 578, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 1.578035, mae: 29.387905, mean_q: -43.336546, mean_eps: 0.403476\n",
      "  63033/200000: episode: 239, duration: 0.287s, episode steps: 167, steps per second: 582, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 1.390445, mae: 29.291770, mean_q: -43.158699, mean_eps: 0.401956\n",
      "  63154/200000: episode: 240, duration: 0.203s, episode steps: 121, steps per second: 595, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 1.801823, mae: 29.483320, mean_q: -43.423143, mean_eps: 0.400588\n",
      "  63242/200000: episode: 241, duration: 0.153s, episode steps:  88, steps per second: 574, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 2.294012, mae: 28.896016, mean_q: -42.448193, mean_eps: 0.399600\n",
      "  63374/200000: episode: 242, duration: 0.217s, episode steps: 132, steps per second: 610, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.707077, mae: 29.656886, mean_q: -43.638057, mean_eps: 0.398574\n",
      "  63513/200000: episode: 243, duration: 0.243s, episode steps: 139, steps per second: 573, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 1.145243, mae: 29.155371, mean_q: -42.881709, mean_eps: 0.397282\n",
      "  63647/200000: episode: 244, duration: 0.224s, episode steps: 134, steps per second: 599, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 3.112198, mae: 29.201814, mean_q: -42.892291, mean_eps: 0.395990\n",
      "  63778/200000: episode: 245, duration: 0.230s, episode steps: 131, steps per second: 570, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.912124, mae: 29.411008, mean_q: -43.264092, mean_eps: 0.394736\n",
      "  63918/200000: episode: 246, duration: 0.242s, episode steps: 140, steps per second: 579, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 2.705092, mae: 29.554310, mean_q: -43.453000, mean_eps: 0.393444\n",
      "  64103/200000: episode: 247, duration: 0.311s, episode steps: 185, steps per second: 595, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.746 [0.000, 2.000],  loss: 2.063307, mae: 29.019712, mean_q: -42.606876, mean_eps: 0.391924\n",
      "  64270/200000: episode: 248, duration: 0.281s, episode steps: 167, steps per second: 595, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.210 [0.000, 2.000],  loss: 0.778518, mae: 29.133280, mean_q: -42.869253, mean_eps: 0.390252\n",
      "  64387/200000: episode: 249, duration: 0.200s, episode steps: 117, steps per second: 585, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.145 [0.000, 2.000],  loss: 0.919971, mae: 29.240646, mean_q: -43.053909, mean_eps: 0.388884\n",
      "  64535/200000: episode: 250, duration: 0.262s, episode steps: 148, steps per second: 566, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.885932, mae: 28.753125, mean_q: -42.299739, mean_eps: 0.387630\n",
      "  64642/200000: episode: 251, duration: 0.184s, episode steps: 107, steps per second: 581, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 2.053345, mae: 29.059743, mean_q: -42.724242, mean_eps: 0.386414\n",
      "  64793/200000: episode: 252, duration: 0.257s, episode steps: 151, steps per second: 587, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 2.680317, mae: 28.918694, mean_q: -42.537853, mean_eps: 0.385160\n",
      "  64943/200000: episode: 253, duration: 0.249s, episode steps: 150, steps per second: 603, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 1.579074, mae: 28.552866, mean_q: -41.932831, mean_eps: 0.383754\n",
      "  65125/200000: episode: 254, duration: 0.308s, episode steps: 182, steps per second: 591, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 1.438413, mae: 28.923990, mean_q: -42.478368, mean_eps: 0.382196\n",
      "  65246/200000: episode: 255, duration: 0.202s, episode steps: 121, steps per second: 598, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.223 [0.000, 2.000],  loss: 2.289518, mae: 28.927736, mean_q: -42.511334, mean_eps: 0.380752\n",
      "  65343/200000: episode: 256, duration: 0.164s, episode steps:  97, steps per second: 591, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 4.020467, mae: 28.767724, mean_q: -42.186193, mean_eps: 0.379726\n",
      "  65446/200000: episode: 257, duration: 0.175s, episode steps: 103, steps per second: 589, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.152306, mae: 28.748457, mean_q: -42.263224, mean_eps: 0.378776\n",
      "  65558/200000: episode: 258, duration: 0.192s, episode steps: 112, steps per second: 585, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 2.906424, mae: 28.955814, mean_q: -42.511954, mean_eps: 0.377750\n",
      "  66045/200000: episode: 259, duration: 0.823s, episode steps: 487, steps per second: 592, episode reward: -486.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.472 [0.000, 2.000],  loss: 1.114606, mae: 29.238733, mean_q: -43.004866, mean_eps: 0.374900\n",
      "  66220/200000: episode: 260, duration: 0.331s, episode steps: 175, steps per second: 529, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.965143, mae: 29.280969, mean_q: -43.120936, mean_eps: 0.371746\n",
      "  66352/200000: episode: 261, duration: 0.228s, episode steps: 132, steps per second: 578, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 1.093628, mae: 29.415819, mean_q: -43.329057, mean_eps: 0.370302\n",
      "  66522/200000: episode: 262, duration: 0.306s, episode steps: 170, steps per second: 556, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.659124, mae: 29.172098, mean_q: -42.985322, mean_eps: 0.368858\n",
      "  66699/200000: episode: 263, duration: 0.301s, episode steps: 177, steps per second: 587, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 2.191695, mae: 29.076232, mean_q: -42.757119, mean_eps: 0.367186\n",
      "  66820/200000: episode: 264, duration: 0.207s, episode steps: 121, steps per second: 585, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 2.020215, mae: 29.591555, mean_q: -43.515699, mean_eps: 0.365780\n",
      "  66931/200000: episode: 265, duration: 0.191s, episode steps: 111, steps per second: 582, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.847 [0.000, 2.000],  loss: 1.267491, mae: 29.051245, mean_q: -42.762657, mean_eps: 0.364678\n",
      "  67067/200000: episode: 266, duration: 0.238s, episode steps: 136, steps per second: 570, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 0.797478, mae: 29.472625, mean_q: -43.419242, mean_eps: 0.363500\n",
      "  67247/200000: episode: 267, duration: 0.306s, episode steps: 180, steps per second: 588, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.744 [0.000, 2.000],  loss: 0.788707, mae: 29.836903, mean_q: -43.982834, mean_eps: 0.362018\n",
      "  67380/200000: episode: 268, duration: 0.232s, episode steps: 133, steps per second: 574, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 1.893038, mae: 29.693439, mean_q: -43.789686, mean_eps: 0.360536\n",
      "  67479/200000: episode: 269, duration: 0.166s, episode steps:  99, steps per second: 595, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.758 [0.000, 2.000],  loss: 2.654277, mae: 29.863588, mean_q: -43.992377, mean_eps: 0.359434\n",
      "  67609/200000: episode: 270, duration: 0.234s, episode steps: 130, steps per second: 556, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.864990, mae: 29.202256, mean_q: -42.928504, mean_eps: 0.358332\n",
      "  67739/200000: episode: 271, duration: 0.220s, episode steps: 130, steps per second: 590, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.669556, mae: 30.129806, mean_q: -44.350681, mean_eps: 0.357078\n",
      "  67885/200000: episode: 272, duration: 0.245s, episode steps: 146, steps per second: 596, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.607279, mae: 29.667678, mean_q: -43.665522, mean_eps: 0.355786\n",
      "  67996/200000: episode: 273, duration: 0.189s, episode steps: 111, steps per second: 587, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.632202, mae: 29.388344, mean_q: -43.208180, mean_eps: 0.354570\n",
      "  68127/200000: episode: 274, duration: 0.223s, episode steps: 131, steps per second: 586, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.832 [0.000, 2.000],  loss: 2.873016, mae: 30.142847, mean_q: -44.303384, mean_eps: 0.353430\n",
      "  68257/200000: episode: 275, duration: 0.252s, episode steps: 130, steps per second: 516, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.712417, mae: 29.774303, mean_q: -43.745154, mean_eps: 0.352176\n",
      "  68369/200000: episode: 276, duration: 0.193s, episode steps: 112, steps per second: 581, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.524397, mae: 30.305287, mean_q: -44.663601, mean_eps: 0.350998\n",
      "  68550/200000: episode: 277, duration: 0.311s, episode steps: 181, steps per second: 583, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.886097, mae: 29.483003, mean_q: -43.341746, mean_eps: 0.349630\n",
      "  68654/200000: episode: 278, duration: 0.179s, episode steps: 104, steps per second: 582, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.798 [0.000, 2.000],  loss: 2.258958, mae: 29.988635, mean_q: -44.019920, mean_eps: 0.348300\n",
      "  68831/200000: episode: 279, duration: 0.298s, episode steps: 177, steps per second: 595, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 2.830944, mae: 29.789916, mean_q: -43.689101, mean_eps: 0.346970\n",
      "  68930/200000: episode: 280, duration: 0.171s, episode steps:  99, steps per second: 578, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 2.475886, mae: 29.605109, mean_q: -43.444034, mean_eps: 0.345640\n",
      "  69044/200000: episode: 281, duration: 0.202s, episode steps: 114, steps per second: 565, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.829956, mae: 29.778375, mean_q: -43.693723, mean_eps: 0.344614\n",
      "  69152/200000: episode: 282, duration: 0.188s, episode steps: 108, steps per second: 574, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.815 [0.000, 2.000],  loss: 1.006902, mae: 30.382249, mean_q: -44.605372, mean_eps: 0.343588\n",
      "  69338/200000: episode: 283, duration: 0.317s, episode steps: 186, steps per second: 586, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.960622, mae: 30.133140, mean_q: -44.323806, mean_eps: 0.342182\n",
      "  69454/200000: episode: 284, duration: 0.194s, episode steps: 116, steps per second: 599, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.759 [0.000, 2.000],  loss: 1.002511, mae: 30.191947, mean_q: -44.417539, mean_eps: 0.340738\n",
      "  69588/200000: episode: 285, duration: 0.237s, episode steps: 134, steps per second: 564, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.701 [0.000, 2.000],  loss: 1.163474, mae: 29.554195, mean_q: -43.436728, mean_eps: 0.339560\n",
      "  69715/200000: episode: 286, duration: 0.238s, episode steps: 127, steps per second: 534, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 1.429466, mae: 29.953916, mean_q: -44.092061, mean_eps: 0.338306\n",
      "  69820/200000: episode: 287, duration: 0.177s, episode steps: 105, steps per second: 593, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.724 [0.000, 2.000],  loss: 2.309251, mae: 29.272488, mean_q: -42.919318, mean_eps: 0.337204\n",
      "  70019/200000: episode: 288, duration: 0.339s, episode steps: 199, steps per second: 586, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.523 [0.000, 2.000],  loss: 1.784209, mae: 29.969009, mean_q: -44.055570, mean_eps: 0.335760\n",
      "  70204/200000: episode: 289, duration: 0.319s, episode steps: 185, steps per second: 580, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.724 [0.000, 2.000],  loss: 1.381517, mae: 29.521970, mean_q: -43.351376, mean_eps: 0.333936\n",
      "  70310/200000: episode: 290, duration: 0.178s, episode steps: 106, steps per second: 594, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 0.880972, mae: 29.783684, mean_q: -43.848120, mean_eps: 0.332568\n",
      "  70434/200000: episode: 291, duration: 0.220s, episode steps: 124, steps per second: 564, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.726 [0.000, 2.000],  loss: 1.759949, mae: 29.380758, mean_q: -43.086214, mean_eps: 0.331466\n",
      "  70548/200000: episode: 292, duration: 0.192s, episode steps: 114, steps per second: 592, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 1.679583, mae: 30.176058, mean_q: -44.376484, mean_eps: 0.330326\n",
      "  70647/200000: episode: 293, duration: 0.164s, episode steps:  99, steps per second: 603, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.733683, mae: 29.895148, mean_q: -43.988819, mean_eps: 0.329338\n",
      "  70761/200000: episode: 294, duration: 0.196s, episode steps: 114, steps per second: 583, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 2.080534, mae: 29.748833, mean_q: -43.747535, mean_eps: 0.328312\n",
      "  70853/200000: episode: 295, duration: 0.152s, episode steps:  92, steps per second: 604, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.617210, mae: 30.123740, mean_q: -44.379853, mean_eps: 0.327324\n",
      "  70959/200000: episode: 296, duration: 0.170s, episode steps: 106, steps per second: 622, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.858 [0.000, 2.000],  loss: 1.969163, mae: 30.819555, mean_q: -45.432515, mean_eps: 0.326412\n",
      "  71062/200000: episode: 297, duration: 0.177s, episode steps: 103, steps per second: 583, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.998216, mae: 29.914211, mean_q: -44.021084, mean_eps: 0.325424\n",
      "  71184/200000: episode: 298, duration: 0.207s, episode steps: 122, steps per second: 588, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 1.143880, mae: 30.515634, mean_q: -44.896509, mean_eps: 0.324360\n",
      "  71307/200000: episode: 299, duration: 0.215s, episode steps: 123, steps per second: 572, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.797 [0.000, 2.000],  loss: 0.963257, mae: 30.346148, mean_q: -44.652212, mean_eps: 0.323182\n",
      "  71428/200000: episode: 300, duration: 0.200s, episode steps: 121, steps per second: 606, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 1.036930, mae: 29.485801, mean_q: -43.350528, mean_eps: 0.322004\n",
      "  71578/200000: episode: 301, duration: 0.256s, episode steps: 150, steps per second: 585, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.658036, mae: 30.075128, mean_q: -44.286227, mean_eps: 0.320712\n",
      "  71750/200000: episode: 302, duration: 0.284s, episode steps: 172, steps per second: 606, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.792376, mae: 30.236312, mean_q: -44.526278, mean_eps: 0.319192\n",
      "  71874/200000: episode: 303, duration: 0.210s, episode steps: 124, steps per second: 591, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.782 [0.000, 2.000],  loss: 1.699570, mae: 29.844045, mean_q: -43.966814, mean_eps: 0.317786\n",
      "  71974/200000: episode: 304, duration: 0.169s, episode steps: 100, steps per second: 591, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.731669, mae: 29.320985, mean_q: -43.170218, mean_eps: 0.316722\n",
      "  72124/200000: episode: 305, duration: 0.267s, episode steps: 150, steps per second: 561, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 1.294215, mae: 30.204282, mean_q: -44.392460, mean_eps: 0.315544\n",
      "  72262/200000: episode: 306, duration: 0.235s, episode steps: 138, steps per second: 588, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.812 [0.000, 2.000],  loss: 2.845674, mae: 29.792405, mean_q: -43.736873, mean_eps: 0.314176\n",
      "  72375/200000: episode: 307, duration: 0.193s, episode steps: 113, steps per second: 586, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.871373, mae: 30.474301, mean_q: -44.820078, mean_eps: 0.312998\n",
      "  72493/200000: episode: 308, duration: 0.207s, episode steps: 118, steps per second: 571, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 1.190951, mae: 29.920900, mean_q: -43.964305, mean_eps: 0.311896\n",
      "  72597/200000: episode: 309, duration: 0.178s, episode steps: 104, steps per second: 585, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 1.989333, mae: 30.544603, mean_q: -44.927077, mean_eps: 0.310832\n",
      "  72712/200000: episode: 310, duration: 0.205s, episode steps: 115, steps per second: 561, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 2.075073, mae: 29.618492, mean_q: -43.408190, mean_eps: 0.309806\n",
      "  72909/200000: episode: 311, duration: 0.338s, episode steps: 197, steps per second: 582, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.335 [0.000, 2.000],  loss: 0.687178, mae: 30.116585, mean_q: -44.256632, mean_eps: 0.308324\n",
      "  73029/200000: episode: 312, duration: 0.204s, episode steps: 120, steps per second: 588, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.819645, mae: 30.211462, mean_q: -44.396225, mean_eps: 0.306804\n",
      "  73153/200000: episode: 313, duration: 0.213s, episode steps: 124, steps per second: 582, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.758 [0.000, 2.000],  loss: 1.075485, mae: 29.508732, mean_q: -43.337883, mean_eps: 0.305626\n",
      "  73256/200000: episode: 314, duration: 0.175s, episode steps: 103, steps per second: 590, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 0.877075, mae: 29.936540, mean_q: -44.046323, mean_eps: 0.304562\n",
      "  73403/200000: episode: 315, duration: 0.262s, episode steps: 147, steps per second: 562, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 0.945582, mae: 29.997786, mean_q: -44.103907, mean_eps: 0.303384\n",
      "  73496/200000: episode: 316, duration: 0.163s, episode steps:  93, steps per second: 571, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.728804, mae: 29.845480, mean_q: -43.895437, mean_eps: 0.302244\n",
      "  73613/200000: episode: 317, duration: 0.202s, episode steps: 117, steps per second: 579, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.844613, mae: 29.748117, mean_q: -43.700700, mean_eps: 0.301256\n",
      "  73743/200000: episode: 318, duration: 0.218s, episode steps: 130, steps per second: 596, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.598511, mae: 30.256503, mean_q: -44.566789, mean_eps: 0.300078\n",
      "  73891/200000: episode: 319, duration: 0.261s, episode steps: 148, steps per second: 566, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.625482, mae: 29.919403, mean_q: -44.036345, mean_eps: 0.298748\n",
      "  73986/200000: episode: 320, duration: 0.181s, episode steps:  95, steps per second: 525, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.619263, mae: 30.034079, mean_q: -44.277097, mean_eps: 0.297570\n",
      "  74084/200000: episode: 321, duration: 0.170s, episode steps:  98, steps per second: 577, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 1.307983, mae: 29.512458, mean_q: -43.365967, mean_eps: 0.296658\n",
      "  74187/200000: episode: 322, duration: 0.178s, episode steps: 103, steps per second: 578, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.827393, mae: 29.878456, mean_q: -43.895915, mean_eps: 0.295708\n",
      "  74281/200000: episode: 323, duration: 0.169s, episode steps:  94, steps per second: 556, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.793864, mae: 29.678895, mean_q: -43.614293, mean_eps: 0.294758\n",
      "  74410/200000: episode: 324, duration: 0.228s, episode steps: 129, steps per second: 565, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 2.128540, mae: 29.992219, mean_q: -44.057062, mean_eps: 0.293694\n",
      "  74529/200000: episode: 325, duration: 0.212s, episode steps: 119, steps per second: 561, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.851449, mae: 30.200331, mean_q: -44.444012, mean_eps: 0.292516\n",
      "  74659/200000: episode: 326, duration: 0.223s, episode steps: 130, steps per second: 584, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.968887, mae: 29.235668, mean_q: -42.963437, mean_eps: 0.291338\n",
      "  74783/200000: episode: 327, duration: 0.211s, episode steps: 124, steps per second: 589, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.654232, mae: 29.804436, mean_q: -43.833445, mean_eps: 0.290160\n",
      "  74923/200000: episode: 328, duration: 0.248s, episode steps: 140, steps per second: 564, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.608602, mae: 29.642645, mean_q: -43.594771, mean_eps: 0.288906\n",
      "  75048/200000: episode: 329, duration: 0.222s, episode steps: 125, steps per second: 563, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.685596, mae: 29.529027, mean_q: -43.392419, mean_eps: 0.287652\n",
      "  75131/200000: episode: 330, duration: 0.149s, episode steps:  83, steps per second: 556, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.889604, mae: 29.059857, mean_q: -42.663316, mean_eps: 0.286664\n",
      "  75244/200000: episode: 331, duration: 0.196s, episode steps: 113, steps per second: 576, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.544486, mae: 29.863504, mean_q: -43.943633, mean_eps: 0.285714\n",
      "  75367/200000: episode: 332, duration: 0.205s, episode steps: 123, steps per second: 600, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.755900, mae: 29.356143, mean_q: -43.168388, mean_eps: 0.284612\n",
      "  75459/200000: episode: 333, duration: 0.163s, episode steps:  92, steps per second: 564, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.477580, mae: 29.880937, mean_q: -43.980409, mean_eps: 0.283586\n",
      "  75569/200000: episode: 334, duration: 0.196s, episode steps: 110, steps per second: 562, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 4.938180, mae: 29.045337, mean_q: -42.487145, mean_eps: 0.282598\n",
      "  75671/200000: episode: 335, duration: 0.171s, episode steps: 102, steps per second: 596, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.680603, mae: 29.230888, mean_q: -42.889436, mean_eps: 0.281610\n",
      "  75788/200000: episode: 336, duration: 0.208s, episode steps: 117, steps per second: 563, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.063151, mae: 28.792572, mean_q: -42.269080, mean_eps: 0.280584\n",
      "  75882/200000: episode: 337, duration: 0.177s, episode steps:  94, steps per second: 530, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.740041, mae: 29.277521, mean_q: -43.028544, mean_eps: 0.279558\n",
      "  75981/200000: episode: 338, duration: 0.171s, episode steps:  99, steps per second: 580, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 0.924174, mae: 29.227235, mean_q: -42.970019, mean_eps: 0.278646\n",
      "  76088/200000: episode: 339, duration: 0.194s, episode steps: 107, steps per second: 553, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.947632, mae: 29.478478, mean_q: -43.334661, mean_eps: 0.277696\n",
      "  76237/200000: episode: 340, duration: 0.262s, episode steps: 149, steps per second: 568, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 0.793341, mae: 29.175875, mean_q: -42.857182, mean_eps: 0.276480\n",
      "  76356/200000: episode: 341, duration: 0.207s, episode steps: 119, steps per second: 575, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.553109, mae: 29.442641, mean_q: -43.237016, mean_eps: 0.275188\n",
      "  76473/200000: episode: 342, duration: 0.206s, episode steps: 117, steps per second: 569, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.662061, mae: 28.792208, mean_q: -42.251265, mean_eps: 0.274048\n",
      "  76761/200000: episode: 343, duration: 0.512s, episode steps: 288, steps per second: 562, episode reward: -287.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.535 [0.000, 2.000],  loss: 1.446547, mae: 29.197442, mean_q: -42.844859, mean_eps: 0.272110\n",
      "  76856/200000: episode: 344, duration: 0.168s, episode steps:  95, steps per second: 566, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 0.592896, mae: 29.125599, mean_q: -42.741393, mean_eps: 0.270324\n",
      "  76968/200000: episode: 345, duration: 0.205s, episode steps: 112, steps per second: 547, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 2.274344, mae: 29.173709, mean_q: -42.750068, mean_eps: 0.269374\n",
      "  77127/200000: episode: 346, duration: 0.285s, episode steps: 159, steps per second: 557, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.796045, mae: 28.348485, mean_q: -41.591223, mean_eps: 0.268082\n",
      "  77232/200000: episode: 347, duration: 0.192s, episode steps: 105, steps per second: 547, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 0.948993, mae: 29.301939, mean_q: -43.002523, mean_eps: 0.266828\n",
      "  77330/200000: episode: 348, duration: 0.177s, episode steps:  98, steps per second: 555, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 0.745230, mae: 29.293039, mean_q: -43.065313, mean_eps: 0.265840\n",
      "  77432/200000: episode: 349, duration: 0.176s, episode steps: 102, steps per second: 579, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.649801, mae: 28.996589, mean_q: -42.563188, mean_eps: 0.264890\n",
      "  77559/200000: episode: 350, duration: 0.226s, episode steps: 127, steps per second: 562, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 1.983841, mae: 29.427970, mean_q: -43.183319, mean_eps: 0.263826\n",
      "  77645/200000: episode: 351, duration: 0.157s, episode steps:  86, steps per second: 548, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 2.135875, mae: 29.137892, mean_q: -42.707715, mean_eps: 0.262800\n",
      "  77801/200000: episode: 352, duration: 0.277s, episode steps: 156, steps per second: 562, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.764465, mae: 29.038965, mean_q: -42.628499, mean_eps: 0.261622\n",
      "  77902/200000: episode: 353, duration: 0.169s, episode steps: 101, steps per second: 598, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 1.319071, mae: 28.922673, mean_q: -42.490597, mean_eps: 0.260406\n",
      "  78007/200000: episode: 354, duration: 0.185s, episode steps: 105, steps per second: 569, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.469548, mae: 29.256912, mean_q: -43.013921, mean_eps: 0.259456\n",
      "  78093/200000: episode: 355, duration: 0.152s, episode steps:  86, steps per second: 567, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.849 [0.000, 2.000],  loss: 0.919212, mae: 28.847655, mean_q: -42.311604, mean_eps: 0.258544\n",
      "  78210/200000: episode: 356, duration: 0.203s, episode steps: 117, steps per second: 577, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.179 [0.000, 2.000],  loss: 0.767668, mae: 28.591069, mean_q: -41.936751, mean_eps: 0.257556\n",
      "  78344/200000: episode: 357, duration: 0.231s, episode steps: 134, steps per second: 579, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.224 [0.000, 2.000],  loss: 0.722046, mae: 28.718957, mean_q: -42.094983, mean_eps: 0.256378\n",
      "  78427/200000: episode: 358, duration: 0.146s, episode steps:  83, steps per second: 568, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 1.840021, mae: 28.757741, mean_q: -42.190648, mean_eps: 0.255352\n",
      "  78537/200000: episode: 359, duration: 0.198s, episode steps: 110, steps per second: 556, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.761684, mae: 28.912624, mean_q: -42.438913, mean_eps: 0.254402\n",
      "  78643/200000: episode: 360, duration: 0.181s, episode steps: 106, steps per second: 586, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.954233, mae: 29.483971, mean_q: -43.272415, mean_eps: 0.253376\n",
      "  78758/200000: episode: 361, duration: 0.197s, episode steps: 115, steps per second: 583, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 1.411185, mae: 28.230215, mean_q: -41.377847, mean_eps: 0.252350\n",
      "  78866/200000: episode: 362, duration: 0.192s, episode steps: 108, steps per second: 561, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.788853, mae: 28.416169, mean_q: -41.718775, mean_eps: 0.251286\n",
      "  78976/200000: episode: 363, duration: 0.195s, episode steps: 110, steps per second: 563, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.578200, mae: 29.710482, mean_q: -43.664975, mean_eps: 0.250260\n",
      "  79078/200000: episode: 364, duration: 0.181s, episode steps: 102, steps per second: 563, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 1.165584, mae: 28.375354, mean_q: -41.588158, mean_eps: 0.249272\n",
      "  79167/200000: episode: 365, duration: 0.160s, episode steps:  89, steps per second: 558, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 2.139426, mae: 28.305205, mean_q: -41.477512, mean_eps: 0.248360\n",
      "  79292/200000: episode: 366, duration: 0.222s, episode steps: 125, steps per second: 562, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 1.351089, mae: 29.258673, mean_q: -42.956950, mean_eps: 0.247334\n",
      "  79391/200000: episode: 367, duration: 0.168s, episode steps:  99, steps per second: 590, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.895671, mae: 29.307918, mean_q: -42.946926, mean_eps: 0.246270\n",
      "  79482/200000: episode: 368, duration: 0.166s, episode steps:  91, steps per second: 547, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.266886, mae: 28.353041, mean_q: -41.568210, mean_eps: 0.245358\n",
      "  79597/200000: episode: 369, duration: 0.213s, episode steps: 115, steps per second: 541, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.696 [0.000, 2.000],  loss: 0.771711, mae: 28.650826, mean_q: -42.035236, mean_eps: 0.244370\n",
      "  79690/200000: episode: 370, duration: 0.167s, episode steps:  93, steps per second: 558, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.153809, mae: 28.837945, mean_q: -42.337699, mean_eps: 0.243382\n",
      "  79779/200000: episode: 371, duration: 0.156s, episode steps:  89, steps per second: 572, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 6.862837, mae: 28.364638, mean_q: -41.364914, mean_eps: 0.242508\n",
      "  79892/200000: episode: 372, duration: 0.198s, episode steps: 113, steps per second: 571, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.186 [0.000, 2.000],  loss: 2.174351, mae: 28.314653, mean_q: -41.554571, mean_eps: 0.241558\n",
      "  80001/200000: episode: 373, duration: 0.201s, episode steps: 109, steps per second: 543, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 1.633388, mae: 28.600370, mean_q: -41.993087, mean_eps: 0.240494\n",
      "  80127/200000: episode: 374, duration: 0.217s, episode steps: 126, steps per second: 581, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 1.422624, mae: 28.691387, mean_q: -42.063748, mean_eps: 0.239392\n",
      "  80203/200000: episode: 375, duration: 0.138s, episode steps:  76, steps per second: 550, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.789 [0.000, 2.000],  loss: 0.802539, mae: 28.755671, mean_q: -42.255952, mean_eps: 0.238442\n",
      "  80295/200000: episode: 376, duration: 0.166s, episode steps:  92, steps per second: 556, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 1.130637, mae: 28.294357, mean_q: -41.338969, mean_eps: 0.237644\n",
      "  80379/200000: episode: 377, duration: 0.145s, episode steps:  84, steps per second: 578, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.881 [0.000, 2.000],  loss: 0.712602, mae: 29.019416, mean_q: -42.604769, mean_eps: 0.236808\n",
      "  80468/200000: episode: 378, duration: 0.153s, episode steps:  89, steps per second: 581, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.507468, mae: 28.516727, mean_q: -41.894788, mean_eps: 0.235972\n",
      "  80569/200000: episode: 379, duration: 0.173s, episode steps: 101, steps per second: 583, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.725023, mae: 28.266791, mean_q: -41.468026, mean_eps: 0.235060\n",
      "  80690/200000: episode: 380, duration: 0.202s, episode steps: 121, steps per second: 599, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.744 [0.000, 2.000],  loss: 0.675578, mae: 28.570878, mean_q: -41.923644, mean_eps: 0.233996\n",
      "  80773/200000: episode: 381, duration: 0.134s, episode steps:  83, steps per second: 619, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 0.741528, mae: 27.896930, mean_q: -40.851177, mean_eps: 0.233046\n",
      "  80854/200000: episode: 382, duration: 0.142s, episode steps:  81, steps per second: 569, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.462476, mae: 28.739524, mean_q: -42.245184, mean_eps: 0.232286\n",
      "  80950/200000: episode: 383, duration: 0.169s, episode steps:  96, steps per second: 568, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.535461, mae: 28.910243, mean_q: -42.455939, mean_eps: 0.231450\n",
      "  81042/200000: episode: 384, duration: 0.166s, episode steps:  92, steps per second: 554, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.163 [0.000, 2.000],  loss: 0.823730, mae: 29.101585, mean_q: -42.712001, mean_eps: 0.230538\n",
      "  81127/200000: episode: 385, duration: 0.137s, episode steps:  85, steps per second: 620, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.858325, mae: 28.369154, mean_q: -41.519529, mean_eps: 0.229702\n",
      "  81246/200000: episode: 386, duration: 0.203s, episode steps: 119, steps per second: 585, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.193 [0.000, 2.000],  loss: 0.750488, mae: 27.825539, mean_q: -40.711100, mean_eps: 0.228752\n",
      "  81384/200000: episode: 387, duration: 0.256s, episode steps: 138, steps per second: 540, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 1.991527, mae: 27.872517, mean_q: -40.803958, mean_eps: 0.227536\n",
      "  81489/200000: episode: 388, duration: 0.184s, episode steps: 105, steps per second: 569, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.819 [0.000, 2.000],  loss: 2.163958, mae: 28.296211, mean_q: -41.433761, mean_eps: 0.226358\n",
      "  81599/200000: episode: 389, duration: 0.191s, episode steps: 110, steps per second: 577, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.709 [0.000, 2.000],  loss: 0.790189, mae: 28.861884, mean_q: -42.359400, mean_eps: 0.225332\n",
      "  81687/200000: episode: 390, duration: 0.158s, episode steps:  88, steps per second: 555, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.875111, mae: 28.955005, mean_q: -42.436005, mean_eps: 0.224420\n",
      "  81799/200000: episode: 391, duration: 0.195s, episode steps: 112, steps per second: 573, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.936628, mae: 28.374182, mean_q: -41.549977, mean_eps: 0.223470\n",
      "  81919/200000: episode: 392, duration: 0.210s, episode steps: 120, steps per second: 571, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 1.892204, mae: 28.794486, mean_q: -42.163901, mean_eps: 0.222368\n",
      "  82017/200000: episode: 393, duration: 0.183s, episode steps:  98, steps per second: 535, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.720628, mae: 28.382142, mean_q: -41.574334, mean_eps: 0.221304\n",
      "  82133/200000: episode: 394, duration: 0.200s, episode steps: 116, steps per second: 579, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.708793, mae: 28.301700, mean_q: -41.442112, mean_eps: 0.220278\n",
      "  82221/200000: episode: 395, duration: 0.152s, episode steps:  88, steps per second: 578, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.922696, mae: 28.376367, mean_q: -41.611808, mean_eps: 0.219328\n",
      "  82335/200000: episode: 396, duration: 0.200s, episode steps: 114, steps per second: 569, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 1.849173, mae: 28.490852, mean_q: -41.732973, mean_eps: 0.218378\n",
      "  82434/200000: episode: 397, duration: 0.178s, episode steps:  99, steps per second: 555, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 1.982084, mae: 29.315899, mean_q: -43.008933, mean_eps: 0.217352\n",
      "  82532/200000: episode: 398, duration: 0.177s, episode steps:  98, steps per second: 554, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.633748, mae: 28.010791, mean_q: -40.944795, mean_eps: 0.216402\n",
      "  82661/200000: episode: 399, duration: 0.224s, episode steps: 129, steps per second: 575, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.256 [0.000, 2.000],  loss: 0.875854, mae: 28.100935, mean_q: -41.095759, mean_eps: 0.215338\n",
      "  82762/200000: episode: 400, duration: 0.176s, episode steps: 101, steps per second: 573, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.861056, mae: 28.978531, mean_q: -42.484063, mean_eps: 0.214236\n",
      "  82855/200000: episode: 401, duration: 0.156s, episode steps:  93, steps per second: 598, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.490634, mae: 28.307369, mean_q: -41.525947, mean_eps: 0.213324\n",
      "  82947/200000: episode: 402, duration: 0.166s, episode steps:  92, steps per second: 553, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.395457, mae: 28.742693, mean_q: -42.288498, mean_eps: 0.212450\n",
      "  83046/200000: episode: 403, duration: 0.172s, episode steps:  99, steps per second: 576, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.562358, mae: 28.350662, mean_q: -41.623310, mean_eps: 0.211538\n",
      "  83176/200000: episode: 404, duration: 0.231s, episode steps: 130, steps per second: 562, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.308 [0.000, 2.000],  loss: 1.545174, mae: 28.230182, mean_q: -41.342817, mean_eps: 0.210474\n",
      "  83315/200000: episode: 405, duration: 0.261s, episode steps: 139, steps per second: 532, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.681925, mae: 27.981902, mean_q: -41.028935, mean_eps: 0.209182\n",
      "  83436/200000: episode: 406, duration: 0.210s, episode steps: 121, steps per second: 577, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 0.624154, mae: 28.348476, mean_q: -41.663377, mean_eps: 0.207928\n",
      "  83570/200000: episode: 407, duration: 0.241s, episode steps: 134, steps per second: 556, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.657040, mae: 27.883507, mean_q: -40.911911, mean_eps: 0.206712\n",
      "  83662/200000: episode: 408, duration: 0.156s, episode steps:  92, steps per second: 588, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 2.338445, mae: 27.951700, mean_q: -40.915550, mean_eps: 0.205648\n",
      "  83803/200000: episode: 409, duration: 0.254s, episode steps: 141, steps per second: 556, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.703654, mae: 28.879388, mean_q: -42.272266, mean_eps: 0.204546\n",
      "  83901/200000: episode: 410, duration: 0.168s, episode steps:  98, steps per second: 582, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 2.122681, mae: 27.874467, mean_q: -40.733329, mean_eps: 0.203406\n",
      "  84027/200000: episode: 411, duration: 0.228s, episode steps: 126, steps per second: 552, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.947258, mae: 28.206615, mean_q: -41.275930, mean_eps: 0.202342\n",
      "  84125/200000: episode: 412, duration: 0.164s, episode steps:  98, steps per second: 596, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 0.590373, mae: 29.261661, mean_q: -42.923412, mean_eps: 0.201278\n",
      "  84239/200000: episode: 413, duration: 0.197s, episode steps: 114, steps per second: 578, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 1.776908, mae: 28.608776, mean_q: -41.928288, mean_eps: 0.200290\n",
      "  84323/200000: episode: 414, duration: 0.149s, episode steps:  84, steps per second: 565, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.810 [0.000, 2.000],  loss: 2.159668, mae: 28.431897, mean_q: -41.601237, mean_eps: 0.199340\n",
      "  84423/200000: episode: 415, duration: 0.176s, episode steps: 100, steps per second: 569, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 2.125692, mae: 28.513826, mean_q: -41.742486, mean_eps: 0.198466\n",
      "  84523/200000: episode: 416, duration: 0.174s, episode steps: 100, steps per second: 574, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.789663, mae: 28.662880, mean_q: -42.086277, mean_eps: 0.197516\n",
      "  84648/200000: episode: 417, duration: 0.215s, episode steps: 125, steps per second: 581, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.644661, mae: 28.213363, mean_q: -41.387409, mean_eps: 0.196452\n",
      "  84742/200000: episode: 418, duration: 0.164s, episode steps:  94, steps per second: 574, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.819 [0.000, 2.000],  loss: 1.288127, mae: 28.511497, mean_q: -41.833860, mean_eps: 0.195426\n",
      "  84860/200000: episode: 419, duration: 0.207s, episode steps: 118, steps per second: 571, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.763 [0.000, 2.000],  loss: 0.653654, mae: 28.888035, mean_q: -42.396824, mean_eps: 0.194400\n",
      "  84967/200000: episode: 420, duration: 0.179s, episode steps: 107, steps per second: 596, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 2.804425, mae: 28.823076, mean_q: -42.251077, mean_eps: 0.193336\n",
      "  85090/200000: episode: 421, duration: 0.227s, episode steps: 123, steps per second: 541, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 1.974091, mae: 28.240781, mean_q: -41.327871, mean_eps: 0.192234\n",
      "  85204/200000: episode: 422, duration: 0.191s, episode steps: 114, steps per second: 596, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 0.777143, mae: 28.361190, mean_q: -41.496308, mean_eps: 0.191094\n",
      "  85295/200000: episode: 423, duration: 0.156s, episode steps:  91, steps per second: 585, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.758 [0.000, 2.000],  loss: 0.847567, mae: 28.528187, mean_q: -41.683042, mean_eps: 0.190144\n",
      "  85375/200000: episode: 424, duration: 0.136s, episode steps:  80, steps per second: 587, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.663281, mae: 28.068127, mean_q: -41.047358, mean_eps: 0.189346\n",
      "  85460/200000: episode: 425, duration: 0.152s, episode steps:  85, steps per second: 559, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.796143, mae: 28.259830, mean_q: -41.360392, mean_eps: 0.188548\n",
      "  85556/200000: episode: 426, duration: 0.173s, episode steps:  96, steps per second: 554, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.828186, mae: 27.757043, mean_q: -40.525036, mean_eps: 0.187674\n",
      "  85657/200000: episode: 427, duration: 0.177s, episode steps: 101, steps per second: 569, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.881 [0.000, 2.000],  loss: 1.777306, mae: 27.717195, mean_q: -40.390220, mean_eps: 0.186724\n",
      "  85738/200000: episode: 428, duration: 0.140s, episode steps:  81, steps per second: 578, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.489014, mae: 28.156888, mean_q: -41.272663, mean_eps: 0.185850\n",
      "  85826/200000: episode: 429, duration: 0.156s, episode steps:  88, steps per second: 565, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.739 [0.000, 2.000],  loss: 0.613570, mae: 27.651799, mean_q: -40.477047, mean_eps: 0.185052\n",
      "  85914/200000: episode: 430, duration: 0.152s, episode steps:  88, steps per second: 578, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 0.633656, mae: 28.205050, mean_q: -41.339748, mean_eps: 0.184216\n",
      "  85998/200000: episode: 431, duration: 0.147s, episode steps:  84, steps per second: 572, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.658130, mae: 28.041186, mean_q: -41.085685, mean_eps: 0.183418\n",
      "  86101/200000: episode: 432, duration: 0.181s, episode steps: 103, steps per second: 570, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 1.843318, mae: 27.943589, mean_q: -40.863897, mean_eps: 0.182544\n",
      "  86226/200000: episode: 433, duration: 0.220s, episode steps: 125, steps per second: 569, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.870148, mae: 27.514119, mean_q: -40.145700, mean_eps: 0.181442\n",
      "  86308/200000: episode: 434, duration: 0.138s, episode steps:  82, steps per second: 596, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 1.138000, mae: 28.243977, mean_q: -41.271238, mean_eps: 0.180454\n",
      "  86410/200000: episode: 435, duration: 0.206s, episode steps: 102, steps per second: 496, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.534537, mae: 28.142630, mean_q: -41.256605, mean_eps: 0.179580\n",
      "  86516/200000: episode: 436, duration: 0.181s, episode steps: 106, steps per second: 585, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.849 [0.000, 2.000],  loss: 0.574613, mae: 28.654338, mean_q: -42.103155, mean_eps: 0.178592\n",
      "  86634/200000: episode: 437, duration: 0.211s, episode steps: 118, steps per second: 560, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.823918, mae: 28.781225, mean_q: -42.191493, mean_eps: 0.177528\n",
      "  86731/200000: episode: 438, duration: 0.169s, episode steps:  97, steps per second: 573, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.713440, mae: 27.825112, mean_q: -40.788403, mean_eps: 0.176502\n",
      "  86808/200000: episode: 439, duration: 0.143s, episode steps:  77, steps per second: 538, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.525404, mae: 27.772541, mean_q: -40.684330, mean_eps: 0.175704\n",
      "  86887/200000: episode: 440, duration: 0.138s, episode steps:  79, steps per second: 574, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.811548, mae: 28.233145, mean_q: -41.283360, mean_eps: 0.174982\n",
      "  86970/200000: episode: 441, duration: 0.154s, episode steps:  83, steps per second: 540, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 1.980435, mae: 28.153020, mean_q: -41.110927, mean_eps: 0.174184\n",
      "  87075/200000: episode: 442, duration: 0.182s, episode steps: 105, steps per second: 577, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 1.803636, mae: 27.342086, mean_q: -39.916893, mean_eps: 0.173272\n",
      "  87172/200000: episode: 443, duration: 0.174s, episode steps:  97, steps per second: 557, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.794 [0.000, 2.000],  loss: 0.730164, mae: 28.128321, mean_q: -41.069123, mean_eps: 0.172322\n",
      "  87277/200000: episode: 444, duration: 0.185s, episode steps: 105, steps per second: 566, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 1.821665, mae: 28.354378, mean_q: -41.360717, mean_eps: 0.171372\n",
      "  87378/200000: episode: 445, duration: 0.179s, episode steps: 101, steps per second: 563, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.842 [0.000, 2.000],  loss: 0.616925, mae: 27.666535, mean_q: -40.392358, mean_eps: 0.170384\n",
      "  87464/200000: episode: 446, duration: 0.149s, episode steps:  86, steps per second: 579, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 1.710767, mae: 26.847079, mean_q: -39.146233, mean_eps: 0.169510\n",
      "  87546/200000: episode: 447, duration: 0.154s, episode steps:  82, steps per second: 532, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 2.187811, mae: 28.386539, mean_q: -41.450196, mean_eps: 0.168712\n",
      "  87683/200000: episode: 448, duration: 0.238s, episode steps: 137, steps per second: 576, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 1.075641, mae: 28.153853, mean_q: -41.109824, mean_eps: 0.167648\n",
      "  87784/200000: episode: 449, duration: 0.184s, episode steps: 101, steps per second: 548, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.832 [0.000, 2.000],  loss: 0.489319, mae: 29.026071, mean_q: -42.546252, mean_eps: 0.166546\n",
      "  87871/200000: episode: 450, duration: 0.161s, episode steps:  87, steps per second: 541, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 2.525790, mae: 28.489842, mean_q: -41.695920, mean_eps: 0.165672\n",
      "  87954/200000: episode: 451, duration: 0.153s, episode steps:  83, steps per second: 543, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.636297, mae: 28.061756, mean_q: -41.055617, mean_eps: 0.164836\n",
      "  88042/200000: episode: 452, duration: 0.158s, episode steps:  88, steps per second: 557, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.807 [0.000, 2.000],  loss: 0.684260, mae: 28.214473, mean_q: -41.283295, mean_eps: 0.164000\n",
      "  88123/200000: episode: 453, duration: 0.144s, episode steps:  81, steps per second: 562, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.487244, mae: 28.653573, mean_q: -41.962897, mean_eps: 0.163202\n",
      "  88218/200000: episode: 454, duration: 0.171s, episode steps:  95, steps per second: 557, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.509684, mae: 28.527701, mean_q: -41.800243, mean_eps: 0.162366\n",
      "  88304/200000: episode: 455, duration: 0.152s, episode steps:  86, steps per second: 566, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.643018, mae: 28.390387, mean_q: -41.503323, mean_eps: 0.161530\n",
      "  88408/200000: episode: 456, duration: 0.193s, episode steps: 104, steps per second: 539, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.634296, mae: 28.417684, mean_q: -41.624641, mean_eps: 0.160656\n",
      "  88488/200000: episode: 457, duration: 0.150s, episode steps:  80, steps per second: 533, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 1.370947, mae: 27.603927, mean_q: -40.334109, mean_eps: 0.159782\n",
      "  88581/200000: episode: 458, duration: 0.173s, episode steps:  93, steps per second: 536, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.710815, mae: 28.159183, mean_q: -41.158703, mean_eps: 0.158946\n",
      "  88661/200000: episode: 459, duration: 0.141s, episode steps:  80, steps per second: 567, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.915894, mae: 28.526109, mean_q: -41.725196, mean_eps: 0.158110\n",
      "  88738/200000: episode: 460, duration: 0.155s, episode steps:  77, steps per second: 498, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.623657, mae: 28.204771, mean_q: -41.323695, mean_eps: 0.157350\n",
      "  88865/200000: episode: 461, duration: 0.224s, episode steps: 127, steps per second: 568, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.134 [0.000, 2.000],  loss: 0.589981, mae: 28.328857, mean_q: -41.529253, mean_eps: 0.156362\n",
      "  88948/200000: episode: 462, duration: 0.143s, episode steps:  83, steps per second: 582, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.549219, mae: 28.402828, mean_q: -41.593180, mean_eps: 0.155374\n",
      "  89032/200000: episode: 463, duration: 0.163s, episode steps:  84, steps per second: 514, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.607715, mae: 28.119720, mean_q: -41.115137, mean_eps: 0.154614\n",
      "  89113/200000: episode: 464, duration: 0.151s, episode steps:  81, steps per second: 537, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.539506, mae: 27.841229, mean_q: -40.719509, mean_eps: 0.153816\n",
      "  89220/200000: episode: 465, duration: 0.184s, episode steps: 107, steps per second: 580, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.391836, mae: 28.722096, mean_q: -42.115678, mean_eps: 0.152904\n",
      "  89321/200000: episode: 466, duration: 0.178s, episode steps: 101, steps per second: 567, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 0.459491, mae: 27.531676, mean_q: -40.214385, mean_eps: 0.151916\n",
      "  89470/200000: episode: 467, duration: 0.258s, episode steps: 149, steps per second: 577, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.148 [0.000, 2.000],  loss: 0.759535, mae: 27.335193, mean_q: -39.971133, mean_eps: 0.150738\n",
      "  89562/200000: episode: 468, duration: 0.168s, episode steps:  92, steps per second: 546, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.936707, mae: 27.731719, mean_q: -40.522784, mean_eps: 0.149598\n",
      "  89682/200000: episode: 469, duration: 0.212s, episode steps: 120, steps per second: 565, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.634945, mae: 27.719513, mean_q: -40.491168, mean_eps: 0.148572\n",
      "  89823/200000: episode: 470, duration: 0.246s, episode steps: 141, steps per second: 573, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.706766, mae: 27.672521, mean_q: -40.456547, mean_eps: 0.147356\n",
      "  89914/200000: episode: 471, duration: 0.168s, episode steps:  91, steps per second: 543, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.590444, mae: 27.760970, mean_q: -40.628783, mean_eps: 0.146254\n",
      "  90032/200000: episode: 472, duration: 0.217s, episode steps: 118, steps per second: 543, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.619 [0.000, 2.000],  loss: 0.685643, mae: 27.856242, mean_q: -40.757044, mean_eps: 0.145266\n",
      "  90135/200000: episode: 473, duration: 0.181s, episode steps: 103, steps per second: 570, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.514817, mae: 27.479179, mean_q: -40.170057, mean_eps: 0.144240\n",
      "  90236/200000: episode: 474, duration: 0.182s, episode steps: 101, steps per second: 554, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.752 [0.000, 2.000],  loss: 0.614483, mae: 27.850815, mean_q: -40.703293, mean_eps: 0.143252\n",
      "  90316/200000: episode: 475, duration: 0.142s, episode steps:  80, steps per second: 563, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.533203, mae: 26.955557, mean_q: -39.400861, mean_eps: 0.142378\n",
      "  90404/200000: episode: 476, duration: 0.157s, episode steps:  88, steps per second: 560, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.841 [0.000, 2.000],  loss: 0.746449, mae: 28.442771, mean_q: -41.632304, mean_eps: 0.141580\n",
      "  90502/200000: episode: 477, duration: 0.174s, episode steps:  98, steps per second: 562, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 1.048584, mae: 26.961868, mean_q: -39.323368, mean_eps: 0.140706\n",
      "  90589/200000: episode: 478, duration: 0.164s, episode steps:  87, steps per second: 530, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 0.653764, mae: 27.372150, mean_q: -39.961583, mean_eps: 0.139832\n",
      "  90689/200000: episode: 479, duration: 0.180s, episode steps: 100, steps per second: 557, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.773240, mae: 27.145723, mean_q: -39.587769, mean_eps: 0.138920\n",
      "  90787/200000: episode: 480, duration: 0.171s, episode steps:  98, steps per second: 572, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.456156, mae: 27.291113, mean_q: -39.876259, mean_eps: 0.137970\n",
      "  90899/200000: episode: 481, duration: 0.195s, episode steps: 112, steps per second: 575, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 1.764108, mae: 27.616153, mean_q: -40.340640, mean_eps: 0.136982\n",
      "  90983/200000: episode: 482, duration: 0.149s, episode steps:  84, steps per second: 565, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 1.488037, mae: 28.157928, mean_q: -41.090049, mean_eps: 0.136070\n",
      "  91070/200000: episode: 483, duration: 0.156s, episode steps:  87, steps per second: 558, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 2.739380, mae: 28.230607, mean_q: -41.206925, mean_eps: 0.135272\n",
      "  91153/200000: episode: 484, duration: 0.152s, episode steps:  83, steps per second: 547, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.664196, mae: 27.796372, mean_q: -40.675799, mean_eps: 0.134436\n",
      "  91252/200000: episode: 485, duration: 0.169s, episode steps:  99, steps per second: 584, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.737 [0.000, 2.000],  loss: 0.560771, mae: 27.514751, mean_q: -40.240356, mean_eps: 0.133562\n",
      "  91338/200000: episode: 486, duration: 0.156s, episode steps:  86, steps per second: 552, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 2.136608, mae: 27.466046, mean_q: -40.111642, mean_eps: 0.132688\n",
      "  91432/200000: episode: 487, duration: 0.167s, episode steps:  94, steps per second: 561, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.599532, mae: 27.095963, mean_q: -39.541282, mean_eps: 0.131852\n",
      "  91526/200000: episode: 488, duration: 0.170s, episode steps:  94, steps per second: 553, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.559753, mae: 27.868807, mean_q: -40.698263, mean_eps: 0.130978\n",
      "  91610/200000: episode: 489, duration: 0.149s, episode steps:  84, steps per second: 563, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.588645, mae: 27.675900, mean_q: -40.411649, mean_eps: 0.130104\n",
      "  91695/200000: episode: 490, duration: 0.150s, episode steps:  85, steps per second: 566, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.695874, mae: 27.140255, mean_q: -39.593105, mean_eps: 0.129306\n",
      "  91779/200000: episode: 491, duration: 0.154s, episode steps:  84, steps per second: 546, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.821 [0.000, 2.000],  loss: 1.028698, mae: 27.519137, mean_q: -40.099583, mean_eps: 0.128508\n",
      "  91850/200000: episode: 492, duration: 0.131s, episode steps:  71, steps per second: 540, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.695150, mae: 26.677409, mean_q: -38.943089, mean_eps: 0.127748\n",
      "  91931/200000: episode: 493, duration: 0.139s, episode steps:  81, steps per second: 581, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.815 [0.000, 2.000],  loss: 0.610107, mae: 26.818160, mean_q: -39.138672, mean_eps: 0.127026\n",
      "  92039/200000: episode: 494, duration: 0.193s, episode steps: 108, steps per second: 559, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 2.134306, mae: 27.570013, mean_q: -40.286429, mean_eps: 0.126152\n",
      "  92128/200000: episode: 495, duration: 0.163s, episode steps:  89, steps per second: 545, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.854 [0.000, 2.000],  loss: 0.423018, mae: 27.684085, mean_q: -40.531666, mean_eps: 0.125240\n",
      "  92204/200000: episode: 496, duration: 0.140s, episode steps:  76, steps per second: 544, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 2.101831, mae: 27.340392, mean_q: -39.911656, mean_eps: 0.124442\n",
      "  92308/200000: episode: 497, duration: 0.192s, episode steps: 104, steps per second: 543, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.572641, mae: 26.905432, mean_q: -39.256743, mean_eps: 0.123568\n",
      "  92394/200000: episode: 498, duration: 0.160s, episode steps:  86, steps per second: 536, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.163 [0.000, 2.000],  loss: 2.440352, mae: 27.492751, mean_q: -40.145861, mean_eps: 0.122656\n",
      "  92507/200000: episode: 499, duration: 0.207s, episode steps: 113, steps per second: 546, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.611 [0.000, 2.000],  loss: 0.533046, mae: 27.242514, mean_q: -39.809804, mean_eps: 0.121706\n",
      "  92597/200000: episode: 500, duration: 0.162s, episode steps:  90, steps per second: 555, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.744 [0.000, 2.000],  loss: 0.740345, mae: 27.127791, mean_q: -39.598819, mean_eps: 0.120756\n",
      "  92717/200000: episode: 501, duration: 0.213s, episode steps: 120, steps per second: 564, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.825 [0.000, 2.000],  loss: 1.993319, mae: 27.246303, mean_q: -39.784228, mean_eps: 0.119768\n",
      "  92811/200000: episode: 502, duration: 0.172s, episode steps:  94, steps per second: 546, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 1.669800, mae: 27.048450, mean_q: -39.532583, mean_eps: 0.118742\n",
      "  92921/200000: episode: 503, duration: 0.202s, episode steps: 110, steps per second: 545, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 0.839111, mae: 27.403467, mean_q: -39.989461, mean_eps: 0.117754\n",
      "  93088/200000: episode: 504, duration: 0.304s, episode steps: 167, steps per second: 550, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.653 [0.000, 2.000],  loss: 1.414703, mae: 27.247280, mean_q: -39.856443, mean_eps: 0.116462\n",
      "  93162/200000: episode: 505, duration: 0.140s, episode steps:  74, steps per second: 527, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 0.682129, mae: 27.073169, mean_q: -39.607779, mean_eps: 0.115322\n",
      "  93273/200000: episode: 506, duration: 0.206s, episode steps: 111, steps per second: 539, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.766 [0.000, 2.000],  loss: 0.560163, mae: 27.201406, mean_q: -39.835146, mean_eps: 0.114410\n",
      "  93350/200000: episode: 507, duration: 0.134s, episode steps:  77, steps per second: 574, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.572021, mae: 26.895107, mean_q: -39.386495, mean_eps: 0.113536\n",
      "  93448/200000: episode: 508, duration: 0.187s, episode steps:  98, steps per second: 523, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.714 [0.000, 2.000],  loss: 0.547201, mae: 27.550444, mean_q: -40.324817, mean_eps: 0.112738\n",
      "  93535/200000: episode: 509, duration: 0.163s, episode steps:  87, steps per second: 533, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.406938, mae: 26.795733, mean_q: -39.234198, mean_eps: 0.111864\n",
      "  93650/200000: episode: 510, duration: 0.208s, episode steps: 115, steps per second: 552, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.696 [0.000, 2.000],  loss: 0.589722, mae: 27.348059, mean_q: -39.998277, mean_eps: 0.110876\n",
      "  93743/200000: episode: 511, duration: 0.160s, episode steps:  93, steps per second: 582, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.849 [0.000, 2.000],  loss: 0.569070, mae: 26.693477, mean_q: -39.001750, mean_eps: 0.109888\n",
      "  93862/200000: episode: 512, duration: 0.213s, episode steps: 119, steps per second: 558, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.655 [0.000, 2.000],  loss: 0.444303, mae: 27.514870, mean_q: -40.266066, mean_eps: 0.108900\n",
      "  93957/200000: episode: 513, duration: 0.170s, episode steps:  95, steps per second: 559, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 1.699615, mae: 27.429721, mean_q: -40.105194, mean_eps: 0.107874\n",
      "  94074/200000: episode: 514, duration: 0.215s, episode steps: 117, steps per second: 544, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.658 [0.000, 2.000],  loss: 0.515438, mae: 26.801759, mean_q: -39.152420, mean_eps: 0.106848\n",
      "  94154/200000: episode: 515, duration: 0.150s, episode steps:  80, steps per second: 535, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.394006, mae: 27.096186, mean_q: -39.579958, mean_eps: 0.105898\n",
      "  94225/200000: episode: 516, duration: 0.129s, episode steps:  71, steps per second: 550, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.789 [0.000, 2.000],  loss: 0.643161, mae: 27.490843, mean_q: -40.218606, mean_eps: 0.105176\n",
      "  94316/200000: episode: 517, duration: 0.155s, episode steps:  91, steps per second: 587, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.433638, mae: 26.868970, mean_q: -39.323014, mean_eps: 0.104416\n",
      "  94393/200000: episode: 518, duration: 0.146s, episode steps:  77, steps per second: 526, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.593945, mae: 27.575257, mean_q: -40.315340, mean_eps: 0.103618\n",
      "  94485/200000: episode: 519, duration: 0.160s, episode steps:  92, steps per second: 574, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.076427, mae: 26.732787, mean_q: -38.998487, mean_eps: 0.102820\n",
      "  94558/200000: episode: 520, duration: 0.137s, episode steps:  73, steps per second: 531, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.479248, mae: 27.710674, mean_q: -40.443590, mean_eps: 0.102060\n",
      "  94634/200000: episode: 521, duration: 0.143s, episode steps:  76, steps per second: 532, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.496814, mae: 27.551963, mean_q: -40.285763, mean_eps: 0.101338\n",
      "  94741/200000: episode: 522, duration: 0.191s, episode steps: 107, steps per second: 561, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.708111, mae: 27.270217, mean_q: -39.811732, mean_eps: 0.100464\n",
      "  94825/200000: episode: 523, duration: 0.159s, episode steps:  84, steps per second: 527, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.451150, mae: 26.966970, mean_q: -39.407352, mean_eps: 0.099552\n",
      "  94920/200000: episode: 524, duration: 0.177s, episode steps:  95, steps per second: 537, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 0.676292, mae: 27.313715, mean_q: -39.960668, mean_eps: 0.098716\n",
      "  95022/200000: episode: 525, duration: 0.186s, episode steps: 102, steps per second: 547, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.498714, mae: 27.078124, mean_q: -39.603805, mean_eps: 0.097804\n",
      "  95138/200000: episode: 526, duration: 0.215s, episode steps: 116, steps per second: 541, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 0.656234, mae: 26.802731, mean_q: -39.122766, mean_eps: 0.096740\n",
      "  95239/200000: episode: 527, duration: 0.181s, episode steps: 101, steps per second: 559, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.449412, mae: 27.736775, mean_q: -40.560831, mean_eps: 0.095714\n",
      "  95324/200000: episode: 528, duration: 0.160s, episode steps:  85, steps per second: 530, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.573730, mae: 27.528206, mean_q: -40.243761, mean_eps: 0.094840\n",
      "  95536/200000: episode: 529, duration: 0.390s, episode steps: 212, steps per second: 543, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.491 [0.000, 2.000],  loss: 1.235229, mae: 27.169797, mean_q: -39.687029, mean_eps: 0.093434\n",
      "  95617/200000: episode: 530, duration: 0.155s, episode steps:  81, steps per second: 524, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.501265, mae: 26.708212, mean_q: -38.994159, mean_eps: 0.092028\n",
      "  95696/200000: episode: 531, duration: 0.145s, episode steps:  79, steps per second: 544, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.494575, mae: 26.310413, mean_q: -38.470175, mean_eps: 0.091268\n",
      "  95777/200000: episode: 532, duration: 0.154s, episode steps:  81, steps per second: 526, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.597745, mae: 27.209904, mean_q: -39.738306, mean_eps: 0.090508\n",
      "  95869/200000: episode: 533, duration: 0.160s, episode steps:  92, steps per second: 574, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.608665, mae: 27.068126, mean_q: -39.506204, mean_eps: 0.089672\n",
      "  95965/200000: episode: 534, duration: 0.179s, episode steps:  96, steps per second: 536, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.781 [0.000, 2.000],  loss: 0.461884, mae: 26.926175, mean_q: -39.383333, mean_eps: 0.088798\n",
      "  96047/200000: episode: 535, duration: 0.152s, episode steps:  82, steps per second: 539, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 0.833130, mae: 26.759558, mean_q: -39.009898, mean_eps: 0.087962\n",
      "  96145/200000: episode: 536, duration: 0.183s, episode steps:  98, steps per second: 535, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.204 [0.000, 2.000],  loss: 0.583440, mae: 26.922940, mean_q: -39.292051, mean_eps: 0.087088\n",
      "  96221/200000: episode: 537, duration: 0.135s, episode steps:  76, steps per second: 562, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 1.343723, mae: 26.722607, mean_q: -38.984865, mean_eps: 0.086252\n",
      "  96460/200000: episode: 538, duration: 0.433s, episode steps: 239, steps per second: 552, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.569 [0.000, 2.000],  loss: 0.620760, mae: 26.712624, mean_q: -38.998188, mean_eps: 0.084770\n",
      "  96591/200000: episode: 539, duration: 0.237s, episode steps: 131, steps per second: 553, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.702 [0.000, 2.000],  loss: 0.482277, mae: 26.919804, mean_q: -39.355509, mean_eps: 0.083022\n",
      "  96662/200000: episode: 540, duration: 0.129s, episode steps:  71, steps per second: 550, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 2.557536, mae: 26.614856, mean_q: -38.773989, mean_eps: 0.082072\n",
      "  96756/200000: episode: 541, duration: 0.176s, episode steps:  94, steps per second: 534, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 1.122762, mae: 26.757128, mean_q: -39.066135, mean_eps: 0.081274\n",
      "  96842/200000: episode: 542, duration: 0.159s, episode steps:  86, steps per second: 540, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.637607, mae: 26.742494, mean_q: -39.075499, mean_eps: 0.080400\n",
      "  96947/200000: episode: 543, duration: 0.192s, episode steps: 105, steps per second: 548, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 2.845478, mae: 26.735556, mean_q: -38.967524, mean_eps: 0.079488\n",
      "  97033/200000: episode: 544, duration: 0.161s, episode steps:  86, steps per second: 535, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.128 [0.000, 2.000],  loss: 0.744940, mae: 26.881685, mean_q: -39.233629, mean_eps: 0.078576\n",
      "  97128/200000: episode: 545, duration: 0.178s, episode steps:  95, steps per second: 532, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.221 [0.000, 2.000],  loss: 0.690407, mae: 26.756592, mean_q: -39.037883, mean_eps: 0.077740\n",
      "  97223/200000: episode: 546, duration: 0.175s, episode steps:  95, steps per second: 542, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.158 [0.000, 2.000],  loss: 0.554026, mae: 26.906919, mean_q: -39.288859, mean_eps: 0.076866\n",
      "  97285/200000: episode: 547, duration: 0.117s, episode steps:  62, steps per second: 530, episode reward: -61.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.529266, mae: 27.539810, mean_q: -40.269997, mean_eps: 0.076106\n",
      "  97363/200000: episode: 548, duration: 0.145s, episode steps:  78, steps per second: 536, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.527393, mae: 27.000474, mean_q: -39.440411, mean_eps: 0.075422\n",
      "  97442/200000: episode: 549, duration: 0.140s, episode steps:  79, steps per second: 566, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.388904, mae: 26.863512, mean_q: -39.318259, mean_eps: 0.074662\n",
      "  97518/200000: episode: 550, duration: 0.134s, episode steps:  76, steps per second: 566, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.450521, mae: 26.544486, mean_q: -38.827015, mean_eps: 0.073940\n",
      "  97598/200000: episode: 551, duration: 0.142s, episode steps:  80, steps per second: 565, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.595349, mae: 27.013284, mean_q: -39.482500, mean_eps: 0.073218\n",
      "  97677/200000: episode: 552, duration: 0.139s, episode steps:  79, steps per second: 570, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.581665, mae: 26.710909, mean_q: -39.042412, mean_eps: 0.072458\n",
      "  97780/200000: episode: 553, duration: 0.190s, episode steps: 103, steps per second: 543, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.846360, mae: 26.866704, mean_q: -39.266639, mean_eps: 0.071584\n",
      "  97876/200000: episode: 554, duration: 0.167s, episode steps:  96, steps per second: 573, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.493439, mae: 26.935727, mean_q: -39.368123, mean_eps: 0.070634\n",
      "  97948/200000: episode: 555, duration: 0.129s, episode steps:  72, steps per second: 557, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.464667, mae: 27.328539, mean_q: -39.970926, mean_eps: 0.069836\n",
      "  98072/200000: episode: 556, duration: 0.226s, episode steps: 124, steps per second: 549, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.177 [0.000, 2.000],  loss: 0.524170, mae: 26.446154, mean_q: -38.557864, mean_eps: 0.068924\n",
      "  98168/200000: episode: 557, duration: 0.184s, episode steps:  96, steps per second: 521, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.513896, mae: 27.058215, mean_q: -39.552242, mean_eps: 0.067898\n",
      "  98245/200000: episode: 558, duration: 0.146s, episode steps:  77, steps per second: 527, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.492151, mae: 27.404065, mean_q: -40.084238, mean_eps: 0.067062\n",
      "  98331/200000: episode: 559, duration: 0.157s, episode steps:  86, steps per second: 546, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.710205, mae: 26.845404, mean_q: -39.241996, mean_eps: 0.066264\n",
      "  98421/200000: episode: 560, duration: 0.168s, episode steps:  90, steps per second: 537, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.415405, mae: 26.982547, mean_q: -39.426009, mean_eps: 0.065428\n",
      "  98508/200000: episode: 561, duration: 0.162s, episode steps:  87, steps per second: 538, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 2.146052, mae: 27.142331, mean_q: -39.565996, mean_eps: 0.064592\n",
      "  98585/200000: episode: 562, duration: 0.145s, episode steps:  77, steps per second: 531, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.554468, mae: 27.059122, mean_q: -39.563885, mean_eps: 0.063794\n",
      "  98682/200000: episode: 563, duration: 0.169s, episode steps:  97, steps per second: 574, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 0.404338, mae: 27.074332, mean_q: -39.635781, mean_eps: 0.062958\n",
      "  98787/200000: episode: 564, duration: 0.186s, episode steps: 105, steps per second: 565, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 1.796152, mae: 27.237585, mean_q: -39.759418, mean_eps: 0.062008\n",
      "  98857/200000: episode: 565, duration: 0.129s, episode steps:  70, steps per second: 542, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.464383, mae: 27.171865, mean_q: -39.690187, mean_eps: 0.061172\n",
      "  98954/200000: episode: 566, duration: 0.176s, episode steps:  97, steps per second: 551, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 0.912140, mae: 26.970638, mean_q: -39.389470, mean_eps: 0.060374\n",
      "  99034/200000: episode: 567, duration: 0.146s, episode steps:  80, steps per second: 547, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.520654, mae: 26.576224, mean_q: -38.763475, mean_eps: 0.059538\n",
      "  99123/200000: episode: 568, duration: 0.159s, episode steps:  89, steps per second: 560, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.729803, mae: 26.992895, mean_q: -39.333396, mean_eps: 0.058740\n",
      "  99203/200000: episode: 569, duration: 0.141s, episode steps:  80, steps per second: 567, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 2.227417, mae: 26.537523, mean_q: -38.646864, mean_eps: 0.057942\n",
      "  99297/200000: episode: 570, duration: 0.172s, episode steps:  94, steps per second: 546, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 1.446187, mae: 27.419238, mean_q: -39.985744, mean_eps: 0.057106\n",
      "  99392/200000: episode: 571, duration: 0.170s, episode steps:  95, steps per second: 558, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 0.894132, mae: 27.340555, mean_q: -39.872760, mean_eps: 0.056232\n",
      "  99487/200000: episode: 572, duration: 0.171s, episode steps:  95, steps per second: 555, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.623108, mae: 26.909304, mean_q: -39.278014, mean_eps: 0.055358\n",
      "  99564/200000: episode: 573, duration: 0.149s, episode steps:  77, steps per second: 517, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.796362, mae: 27.009234, mean_q: -39.388107, mean_eps: 0.054522\n",
      "  99643/200000: episode: 574, duration: 0.142s, episode steps:  79, steps per second: 556, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 0.615576, mae: 26.784662, mean_q: -39.067636, mean_eps: 0.053762\n",
      "  99718/200000: episode: 575, duration: 0.129s, episode steps:  75, steps per second: 580, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.469537, mae: 27.143686, mean_q: -39.691909, mean_eps: 0.053040\n",
      "  99795/200000: episode: 576, duration: 0.145s, episode steps:  77, steps per second: 532, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.532031, mae: 27.224595, mean_q: -39.849228, mean_eps: 0.052318\n",
      "  99875/200000: episode: 577, duration: 0.147s, episode steps:  80, steps per second: 545, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 1.931116, mae: 27.098196, mean_q: -39.613848, mean_eps: 0.051558\n",
      "  99959/200000: episode: 578, duration: 0.148s, episode steps:  84, steps per second: 567, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.539258, mae: 26.903982, mean_q: -39.245559, mean_eps: 0.050798\n",
      " 100036/200000: episode: 579, duration: 0.142s, episode steps:  77, steps per second: 543, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.792 [0.000, 2.000],  loss: 0.558154, mae: 26.559487, mean_q: -38.739919, mean_eps: 0.050114\n",
      " 100138/200000: episode: 580, duration: 0.186s, episode steps: 102, steps per second: 549, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.705134, mae: 26.555727, mean_q: -38.726817, mean_eps: 0.050000\n",
      " 100222/200000: episode: 581, duration: 0.141s, episode steps:  84, steps per second: 595, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.740674, mae: 26.133861, mean_q: -38.105026, mean_eps: 0.050000\n",
      " 100294/200000: episode: 582, duration: 0.128s, episode steps:  72, steps per second: 562, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.500420, mae: 26.282402, mean_q: -38.360382, mean_eps: 0.050000\n",
      " 100370/200000: episode: 583, duration: 0.139s, episode steps:  76, steps per second: 545, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.512732, mae: 27.041603, mean_q: -39.483629, mean_eps: 0.050000\n",
      " 100456/200000: episode: 584, duration: 0.162s, episode steps:  86, steps per second: 531, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.627490, mae: 26.936675, mean_q: -39.335052, mean_eps: 0.050000\n",
      " 100555/200000: episode: 585, duration: 0.189s, episode steps:  99, steps per second: 524, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.778 [0.000, 2.000],  loss: 1.723783, mae: 26.202129, mean_q: -38.213668, mean_eps: 0.050000\n",
      " 100648/200000: episode: 586, duration: 0.173s, episode steps:  93, steps per second: 537, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.215 [0.000, 2.000],  loss: 0.541748, mae: 27.288610, mean_q: -39.886482, mean_eps: 0.050000\n",
      " 100725/200000: episode: 587, duration: 0.144s, episode steps:  77, steps per second: 535, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.860559, mae: 26.662849, mean_q: -38.898377, mean_eps: 0.050000\n",
      " 100815/200000: episode: 588, duration: 0.165s, episode steps:  90, steps per second: 546, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 1.774126, mae: 27.022568, mean_q: -39.382550, mean_eps: 0.050000\n",
      " 100905/200000: episode: 589, duration: 0.174s, episode steps:  90, steps per second: 518, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 1.295410, mae: 26.887873, mean_q: -39.262161, mean_eps: 0.050000\n",
      " 100999/200000: episode: 590, duration: 0.165s, episode steps:  94, steps per second: 570, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.627797, mae: 26.682624, mean_q: -38.906847, mean_eps: 0.050000\n",
      " 101077/200000: episode: 591, duration: 0.145s, episode steps:  78, steps per second: 538, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.601904, mae: 26.434050, mean_q: -38.509785, mean_eps: 0.050000\n",
      " 101156/200000: episode: 592, duration: 0.145s, episode steps:  79, steps per second: 546, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 4.459656, mae: 26.825993, mean_q: -38.924422, mean_eps: 0.050000\n",
      " 101243/200000: episode: 593, duration: 0.164s, episode steps:  87, steps per second: 531, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.479892, mae: 27.477792, mean_q: -40.051757, mean_eps: 0.050000\n",
      " 101321/200000: episode: 594, duration: 0.154s, episode steps:  78, steps per second: 505, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.613562, mae: 27.529227, mean_q: -40.162976, mean_eps: 0.050000\n",
      " 101404/200000: episode: 595, duration: 0.145s, episode steps:  83, steps per second: 574, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.643677, mae: 27.057106, mean_q: -39.449541, mean_eps: 0.050000\n",
      " 101475/200000: episode: 596, duration: 0.130s, episode steps:  71, steps per second: 545, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.829861, mae: 27.595847, mean_q: -40.335913, mean_eps: 0.050000\n",
      " 101546/200000: episode: 597, duration: 0.129s, episode steps:  71, steps per second: 550, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 2.348836, mae: 26.808572, mean_q: -39.146482, mean_eps: 0.050000\n",
      " 101639/200000: episode: 598, duration: 0.165s, episode steps:  93, steps per second: 563, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.806 [0.000, 2.000],  loss: 1.360440, mae: 26.974859, mean_q: -39.309391, mean_eps: 0.050000\n",
      " 101723/200000: episode: 599, duration: 0.156s, episode steps:  84, steps per second: 540, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 2.582941, mae: 27.149873, mean_q: -39.595036, mean_eps: 0.050000\n",
      " 101812/200000: episode: 600, duration: 0.163s, episode steps:  89, steps per second: 547, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.691295, mae: 26.384231, mean_q: -38.496561, mean_eps: 0.050000\n",
      " 101907/200000: episode: 601, duration: 0.170s, episode steps:  95, steps per second: 560, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 1.793223, mae: 26.644099, mean_q: -38.952918, mean_eps: 0.050000\n",
      " 101985/200000: episode: 602, duration: 0.142s, episode steps:  78, steps per second: 549, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 1.113806, mae: 26.414192, mean_q: -38.578935, mean_eps: 0.050000\n",
      " 102074/200000: episode: 603, duration: 0.163s, episode steps:  89, steps per second: 546, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.822199, mae: 27.012316, mean_q: -39.436918, mean_eps: 0.050000\n",
      " 102144/200000: episode: 604, duration: 0.133s, episode steps:  70, steps per second: 525, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.612503, mae: 26.451589, mean_q: -38.608299, mean_eps: 0.050000\n",
      " 102266/200000: episode: 605, duration: 0.231s, episode steps: 122, steps per second: 528, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.713 [0.000, 2.000],  loss: 1.375534, mae: 26.401670, mean_q: -38.528766, mean_eps: 0.050000\n",
      " 102345/200000: episode: 606, duration: 0.161s, episode steps:  79, steps per second: 491, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.503918, mae: 26.529154, mean_q: -38.736925, mean_eps: 0.050000\n",
      " 102426/200000: episode: 607, duration: 0.154s, episode steps:  81, steps per second: 526, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 0.407776, mae: 26.999275, mean_q: -39.450721, mean_eps: 0.050000\n",
      " 102517/200000: episode: 608, duration: 0.166s, episode steps:  91, steps per second: 548, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 2.318692, mae: 26.382062, mean_q: -38.426703, mean_eps: 0.050000\n",
      " 102605/200000: episode: 609, duration: 0.158s, episode steps:  88, steps per second: 555, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.643821, mae: 26.012448, mean_q: -37.971840, mean_eps: 0.050000\n",
      " 102706/200000: episode: 610, duration: 0.182s, episode steps: 101, steps per second: 556, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.792 [0.000, 2.000],  loss: 0.461388, mae: 26.079368, mean_q: -38.055830, mean_eps: 0.050000\n",
      " 102799/200000: episode: 611, duration: 0.167s, episode steps:  93, steps per second: 558, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 1.274592, mae: 26.340315, mean_q: -38.482685, mean_eps: 0.050000\n",
      " 102880/200000: episode: 612, duration: 0.157s, episode steps:  81, steps per second: 517, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.688037, mae: 26.610399, mean_q: -38.886745, mean_eps: 0.050000\n",
      " 102956/200000: episode: 613, duration: 0.144s, episode steps:  76, steps per second: 527, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 0.526575, mae: 25.788750, mean_q: -37.611435, mean_eps: 0.050000\n",
      " 103065/200000: episode: 614, duration: 0.212s, episode steps: 109, steps per second: 513, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.679 [0.000, 2.000],  loss: 0.522461, mae: 25.826922, mean_q: -37.631220, mean_eps: 0.050000\n",
      " 103163/200000: episode: 615, duration: 0.178s, episode steps:  98, steps per second: 552, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.847 [0.000, 2.000],  loss: 0.461324, mae: 25.929533, mean_q: -37.770998, mean_eps: 0.050000\n",
      " 103257/200000: episode: 616, duration: 0.169s, episode steps:  94, steps per second: 557, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.880737, mae: 26.413251, mean_q: -38.552571, mean_eps: 0.050000\n",
      " 103346/200000: episode: 617, duration: 0.156s, episode steps:  89, steps per second: 572, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.840598, mae: 26.419082, mean_q: -38.575165, mean_eps: 0.050000\n",
      " 103421/200000: episode: 618, duration: 0.131s, episode steps:  75, steps per second: 571, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.493313, mae: 26.205048, mean_q: -38.323989, mean_eps: 0.050000\n",
      " 103524/200000: episode: 619, duration: 0.189s, episode steps: 103, steps per second: 546, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.155 [0.000, 2.000],  loss: 0.613751, mae: 26.179740, mean_q: -38.311535, mean_eps: 0.050000\n",
      " 103612/200000: episode: 620, duration: 0.161s, episode steps:  88, steps per second: 545, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.761 [0.000, 2.000],  loss: 0.845459, mae: 25.211162, mean_q: -36.690412, mean_eps: 0.050000\n",
      " 103720/200000: episode: 621, duration: 0.199s, episode steps: 108, steps per second: 542, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.517766, mae: 26.377359, mean_q: -38.525039, mean_eps: 0.050000\n",
      " 103804/200000: episode: 622, duration: 0.157s, episode steps:  84, steps per second: 536, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 0.572343, mae: 26.393120, mean_q: -38.587182, mean_eps: 0.050000\n",
      " 103930/200000: episode: 623, duration: 0.232s, episode steps: 126, steps per second: 544, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 0.625702, mae: 26.062506, mean_q: -38.021202, mean_eps: 0.050000\n",
      " 104035/200000: episode: 624, duration: 0.193s, episode steps: 105, steps per second: 543, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.478534, mae: 26.540819, mean_q: -38.752831, mean_eps: 0.050000\n",
      " 104120/200000: episode: 625, duration: 0.165s, episode steps:  85, steps per second: 516, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.561572, mae: 25.777485, mean_q: -37.614531, mean_eps: 0.050000\n",
      " 104199/200000: episode: 626, duration: 0.145s, episode steps:  79, steps per second: 544, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 1.606396, mae: 25.857238, mean_q: -37.733043, mean_eps: 0.050000\n",
      " 104285/200000: episode: 627, duration: 0.160s, episode steps:  86, steps per second: 538, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.826 [0.000, 2.000],  loss: 0.426447, mae: 26.344891, mean_q: -38.549085, mean_eps: 0.050000\n",
      " 104705/200000: episode: 628, duration: 0.767s, episode steps: 420, steps per second: 548, episode reward: -419.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.357 [0.000, 2.000],  loss: 0.581538, mae: 25.989782, mean_q: -37.938929, mean_eps: 0.050000\n",
      " 104786/200000: episode: 629, duration: 0.148s, episode steps:  81, steps per second: 546, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.301367, mae: 26.299923, mean_q: -38.410290, mean_eps: 0.050000\n",
      " 104857/200000: episode: 630, duration: 0.138s, episode steps:  71, steps per second: 516, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.501289, mae: 25.226733, mean_q: -36.707112, mean_eps: 0.050000\n",
      " 104931/200000: episode: 631, duration: 0.131s, episode steps:  74, steps per second: 564, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.625190, mae: 26.061338, mean_q: -37.975355, mean_eps: 0.050000\n",
      " 105029/200000: episode: 632, duration: 0.185s, episode steps:  98, steps per second: 531, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.776 [0.000, 2.000],  loss: 0.581604, mae: 25.871685, mean_q: -37.729511, mean_eps: 0.050000\n",
      " 105118/200000: episode: 633, duration: 0.163s, episode steps:  89, steps per second: 545, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.536654, mae: 25.903772, mean_q: -37.775427, mean_eps: 0.050000\n",
      " 105204/200000: episode: 634, duration: 0.160s, episode steps:  86, steps per second: 537, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 1.775579, mae: 26.058087, mean_q: -37.966302, mean_eps: 0.050000\n",
      " 105278/200000: episode: 635, duration: 0.134s, episode steps:  74, steps per second: 551, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.484077, mae: 25.798254, mean_q: -37.629075, mean_eps: 0.050000\n",
      " 105355/200000: episode: 636, duration: 0.148s, episode steps:  77, steps per second: 521, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.323315, mae: 26.584849, mean_q: -38.866105, mean_eps: 0.050000\n",
      " 105472/200000: episode: 637, duration: 0.223s, episode steps: 117, steps per second: 526, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.179 [0.000, 2.000],  loss: 0.520002, mae: 26.489027, mean_q: -38.656397, mean_eps: 0.050000\n",
      " 105570/200000: episode: 638, duration: 0.189s, episode steps:  98, steps per second: 518, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.603553, mae: 26.621685, mean_q: -38.905338, mean_eps: 0.050000\n",
      " 105812/200000: episode: 639, duration: 0.432s, episode steps: 242, steps per second: 560, episode reward: -241.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.380 [0.000, 2.000],  loss: 1.451530, mae: 26.151527, mean_q: -38.148017, mean_eps: 0.050000\n",
      " 105889/200000: episode: 640, duration: 0.142s, episode steps:  77, steps per second: 542, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.499609, mae: 26.420643, mean_q: -38.617400, mean_eps: 0.050000\n",
      " 106022/200000: episode: 641, duration: 0.235s, episode steps: 133, steps per second: 566, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.703140, mae: 26.525564, mean_q: -38.703130, mean_eps: 0.050000\n",
      " 106096/200000: episode: 642, duration: 0.141s, episode steps:  74, steps per second: 526, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.518338, mae: 25.620896, mean_q: -37.410219, mean_eps: 0.050000\n",
      " 106317/200000: episode: 643, duration: 0.409s, episode steps: 221, steps per second: 540, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.642190, mae: 26.095025, mean_q: -38.109685, mean_eps: 0.050000\n",
      " 106419/200000: episode: 644, duration: 0.187s, episode steps: 102, steps per second: 546, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.746469, mae: 25.457850, mean_q: -37.083786, mean_eps: 0.050000\n",
      " 106494/200000: episode: 645, duration: 0.136s, episode steps:  75, steps per second: 553, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.533081, mae: 25.968292, mean_q: -37.953913, mean_eps: 0.050000\n",
      " 106571/200000: episode: 646, duration: 0.156s, episode steps:  77, steps per second: 492, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.658008, mae: 25.875921, mean_q: -37.731113, mean_eps: 0.050000\n",
      " 106662/200000: episode: 647, duration: 0.172s, episode steps:  91, steps per second: 528, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.154 [0.000, 2.000],  loss: 0.552890, mae: 25.745431, mean_q: -37.480790, mean_eps: 0.050000\n",
      " 106840/200000: episode: 648, duration: 0.334s, episode steps: 178, steps per second: 534, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.579 [0.000, 2.000],  loss: 1.330938, mae: 25.999643, mean_q: -37.919726, mean_eps: 0.050000\n",
      " 106927/200000: episode: 649, duration: 0.162s, episode steps:  87, steps per second: 536, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.531594, mae: 25.586428, mean_q: -37.254176, mean_eps: 0.050000\n",
      " 107009/200000: episode: 650, duration: 0.163s, episode steps:  82, steps per second: 503, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.844371, mae: 25.981347, mean_q: -37.828366, mean_eps: 0.050000\n",
      " 107085/200000: episode: 651, duration: 0.134s, episode steps:  76, steps per second: 565, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.184 [0.000, 2.000],  loss: 0.570448, mae: 26.081939, mean_q: -38.039861, mean_eps: 0.050000\n",
      " 107203/200000: episode: 652, duration: 0.221s, episode steps: 118, steps per second: 535, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.364 [0.000, 2.000],  loss: 0.830518, mae: 26.036739, mean_q: -37.954366, mean_eps: 0.050000\n",
      " 107276/200000: episode: 653, duration: 0.132s, episode steps:  73, steps per second: 555, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.775743, mae: 26.000094, mean_q: -37.815583, mean_eps: 0.050000\n",
      " 107339/200000: episode: 654, duration: 0.118s, episode steps:  63, steps per second: 535, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.560364, mae: 26.124840, mean_q: -38.134108, mean_eps: 0.050000\n",
      " 107431/200000: episode: 655, duration: 0.160s, episode steps:  92, steps per second: 573, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.538807, mae: 26.386615, mean_q: -38.513675, mean_eps: 0.050000\n",
      " 107508/200000: episode: 656, duration: 0.148s, episode steps:  77, steps per second: 520, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.463232, mae: 26.122936, mean_q: -38.122532, mean_eps: 0.050000\n",
      " 107591/200000: episode: 657, duration: 0.146s, episode steps:  83, steps per second: 570, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.477234, mae: 25.812900, mean_q: -37.622630, mean_eps: 0.050000\n",
      " 107661/200000: episode: 658, duration: 0.133s, episode steps:  70, steps per second: 527, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.456706, mae: 25.792807, mean_q: -37.637602, mean_eps: 0.050000\n",
      " 107747/200000: episode: 659, duration: 0.159s, episode steps:  86, steps per second: 542, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.553278, mae: 25.793468, mean_q: -37.619587, mean_eps: 0.050000\n",
      " 107822/200000: episode: 660, duration: 0.137s, episode steps:  75, steps per second: 549, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.535658, mae: 25.601113, mean_q: -37.324259, mean_eps: 0.050000\n",
      " 107893/200000: episode: 661, duration: 0.131s, episode steps:  71, steps per second: 542, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.506266, mae: 25.638936, mean_q: -37.399435, mean_eps: 0.050000\n",
      " 107984/200000: episode: 662, duration: 0.175s, episode steps:  91, steps per second: 521, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.209 [0.000, 2.000],  loss: 0.400768, mae: 25.679051, mean_q: -37.453709, mean_eps: 0.050000\n",
      " 108074/200000: episode: 663, duration: 0.177s, episode steps:  90, steps per second: 507, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 2.280863, mae: 25.574799, mean_q: -37.226129, mean_eps: 0.050000\n",
      " 108163/200000: episode: 664, duration: 0.162s, episode steps:  89, steps per second: 550, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.624634, mae: 26.071007, mean_q: -37.989400, mean_eps: 0.050000\n",
      " 108242/200000: episode: 665, duration: 0.141s, episode steps:  79, steps per second: 559, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.439551, mae: 26.034278, mean_q: -37.946982, mean_eps: 0.050000\n",
      " 108330/200000: episode: 666, duration: 0.166s, episode steps:  88, steps per second: 529, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.159 [0.000, 2.000],  loss: 0.528820, mae: 25.746727, mean_q: -37.519473, mean_eps: 0.050000\n",
      " 108401/200000: episode: 667, duration: 0.144s, episode steps:  71, steps per second: 493, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.609890, mae: 25.628525, mean_q: -37.344769, mean_eps: 0.050000\n",
      " 108495/200000: episode: 668, duration: 0.168s, episode steps:  94, steps per second: 559, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.502907, mae: 26.152034, mean_q: -38.262639, mean_eps: 0.050000\n",
      " 108574/200000: episode: 669, duration: 0.151s, episode steps:  79, steps per second: 523, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.994482, mae: 26.238621, mean_q: -38.184238, mean_eps: 0.050000\n",
      " 108653/200000: episode: 670, duration: 0.155s, episode steps:  79, steps per second: 511, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 0.475928, mae: 25.862053, mean_q: -37.690759, mean_eps: 0.050000\n",
      " 108746/200000: episode: 671, duration: 0.172s, episode steps:  93, steps per second: 542, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.480225, mae: 26.753429, mean_q: -39.108307, mean_eps: 0.050000\n",
      " 108833/200000: episode: 672, duration: 0.162s, episode steps:  87, steps per second: 539, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.149 [0.000, 2.000],  loss: 0.795299, mae: 25.424661, mean_q: -36.995334, mean_eps: 0.050000\n",
      " 108924/200000: episode: 673, duration: 0.164s, episode steps:  91, steps per second: 555, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 1.193137, mae: 26.250108, mean_q: -38.247520, mean_eps: 0.050000\n",
      " 109012/200000: episode: 674, duration: 0.165s, episode steps:  88, steps per second: 534, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.484120, mae: 26.144436, mean_q: -38.138930, mean_eps: 0.050000\n",
      " 109109/200000: episode: 675, duration: 0.172s, episode steps:  97, steps per second: 565, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.650401, mae: 25.560094, mean_q: -37.256146, mean_eps: 0.050000\n",
      " 109191/200000: episode: 676, duration: 0.150s, episode steps:  82, steps per second: 548, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 1.327380, mae: 26.410036, mean_q: -38.543578, mean_eps: 0.050000\n",
      " 109290/200000: episode: 677, duration: 0.191s, episode steps:  99, steps per second: 519, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 1.547232, mae: 26.008572, mean_q: -37.950530, mean_eps: 0.050000\n",
      " 109396/200000: episode: 678, duration: 0.194s, episode steps: 106, steps per second: 547, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.504995, mae: 26.200354, mean_q: -38.280632, mean_eps: 0.050000\n",
      " 109486/200000: episode: 679, duration: 0.158s, episode steps:  90, steps per second: 571, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 2.405839, mae: 26.052290, mean_q: -37.989185, mean_eps: 0.050000\n",
      " 109579/200000: episode: 680, duration: 0.177s, episode steps:  93, steps per second: 527, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 0.630697, mae: 26.085127, mean_q: -38.003546, mean_eps: 0.050000\n",
      " 109661/200000: episode: 681, duration: 0.145s, episode steps:  82, steps per second: 565, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.445068, mae: 25.585644, mean_q: -37.315928, mean_eps: 0.050000\n",
      " 109748/200000: episode: 682, duration: 0.159s, episode steps:  87, steps per second: 547, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.543757, mae: 26.405882, mean_q: -38.519285, mean_eps: 0.050000\n",
      " 109831/200000: episode: 683, duration: 0.149s, episode steps:  83, steps per second: 559, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.390796, mae: 25.478817, mean_q: -37.139643, mean_eps: 0.050000\n",
      " 109928/200000: episode: 684, duration: 0.187s, episode steps:  97, steps per second: 518, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.804 [0.000, 2.000],  loss: 0.451904, mae: 25.938133, mean_q: -37.805776, mean_eps: 0.050000\n",
      " 110020/200000: episode: 685, duration: 0.173s, episode steps:  92, steps per second: 533, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 0.478027, mae: 25.420997, mean_q: -37.063730, mean_eps: 0.050000\n",
      " 110116/200000: episode: 686, duration: 0.184s, episode steps:  96, steps per second: 522, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 0.980367, mae: 26.549647, mean_q: -38.718088, mean_eps: 0.050000\n",
      " 110210/200000: episode: 687, duration: 0.182s, episode steps:  94, steps per second: 518, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.767782, mae: 25.522020, mean_q: -37.146059, mean_eps: 0.050000\n",
      " 110281/200000: episode: 688, duration: 0.138s, episode steps:  71, steps per second: 514, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 0.631456, mae: 25.606812, mean_q: -37.300949, mean_eps: 0.050000\n",
      " 110372/200000: episode: 689, duration: 0.169s, episode steps:  91, steps per second: 538, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 1.201072, mae: 25.871390, mean_q: -37.735469, mean_eps: 0.050000\n",
      " 110447/200000: episode: 690, duration: 0.159s, episode steps:  75, steps per second: 471, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.749620, mae: 26.134366, mean_q: -38.071612, mean_eps: 0.050000\n",
      " 110542/200000: episode: 691, duration: 0.174s, episode steps:  95, steps per second: 546, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.532267, mae: 26.421076, mean_q: -38.524074, mean_eps: 0.050000\n",
      " 110609/200000: episode: 692, duration: 0.128s, episode steps:  67, steps per second: 523, episode reward: -66.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.599718, mae: 25.654036, mean_q: -37.332746, mean_eps: 0.050000\n",
      " 110687/200000: episode: 693, duration: 0.133s, episode steps:  78, steps per second: 588, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.128 [0.000, 2.000],  loss: 0.577596, mae: 25.967524, mean_q: -37.870325, mean_eps: 0.050000\n",
      " 110765/200000: episode: 694, duration: 0.145s, episode steps:  78, steps per second: 537, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 0.685034, mae: 25.692862, mean_q: -37.452093, mean_eps: 0.050000\n",
      " 110864/200000: episode: 695, duration: 0.185s, episode steps:  99, steps per second: 536, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 1.106466, mae: 26.253217, mean_q: -38.267026, mean_eps: 0.050000\n",
      " 110933/200000: episode: 696, duration: 0.129s, episode steps:  69, steps per second: 536, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 2.246799, mae: 25.801339, mean_q: -37.523734, mean_eps: 0.050000\n",
      " 111009/200000: episode: 697, duration: 0.149s, episode steps:  76, steps per second: 510, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.678345, mae: 26.565673, mean_q: -38.658401, mean_eps: 0.050000\n",
      " 111083/200000: episode: 698, duration: 0.135s, episode steps:  74, steps per second: 549, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 3.491618, mae: 25.734934, mean_q: -37.331888, mean_eps: 0.050000\n",
      " 111175/200000: episode: 699, duration: 0.160s, episode steps:  92, steps per second: 576, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.734353, mae: 25.870436, mean_q: -37.724707, mean_eps: 0.050000\n",
      " 111270/200000: episode: 700, duration: 0.175s, episode steps:  95, steps per second: 542, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.576457, mae: 25.359100, mean_q: -37.002275, mean_eps: 0.050000\n",
      " 111363/200000: episode: 701, duration: 0.173s, episode steps:  93, steps per second: 539, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 3.061788, mae: 25.839379, mean_q: -37.636572, mean_eps: 0.050000\n",
      " 111454/200000: episode: 702, duration: 0.168s, episode steps:  91, steps per second: 543, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 2.442050, mae: 25.615692, mean_q: -37.299025, mean_eps: 0.050000\n",
      " 111531/200000: episode: 703, duration: 0.148s, episode steps:  77, steps per second: 519, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.559753, mae: 25.721106, mean_q: -37.453996, mean_eps: 0.050000\n",
      " 111621/200000: episode: 704, duration: 0.159s, episode steps:  90, steps per second: 565, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 2.217662, mae: 25.454427, mean_q: -36.994869, mean_eps: 0.050000\n",
      " 111695/200000: episode: 705, duration: 0.131s, episode steps:  74, steps per second: 563, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.560411, mae: 25.612435, mean_q: -37.343899, mean_eps: 0.050000\n",
      " 111772/200000: episode: 706, duration: 0.148s, episode steps:  77, steps per second: 519, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.811987, mae: 25.930668, mean_q: -37.762923, mean_eps: 0.050000\n",
      " 111857/200000: episode: 707, duration: 0.161s, episode steps:  85, steps per second: 527, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.503729, mae: 25.366298, mean_q: -36.994107, mean_eps: 0.050000\n",
      " 111932/200000: episode: 708, duration: 0.136s, episode steps:  75, steps per second: 552, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.472114, mae: 25.528058, mean_q: -37.218546, mean_eps: 0.050000\n",
      " 111995/200000: episode: 709, duration: 0.117s, episode steps:  63, steps per second: 537, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 0.574677, mae: 25.887822, mean_q: -37.777651, mean_eps: 0.050000\n",
      " 112078/200000: episode: 710, duration: 0.157s, episode steps:  83, steps per second: 529, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.782349, mae: 26.201451, mean_q: -38.200190, mean_eps: 0.050000\n",
      " 112148/200000: episode: 711, duration: 0.132s, episode steps:  70, steps per second: 529, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.877767, mae: 25.323095, mean_q: -36.929198, mean_eps: 0.050000\n",
      " 112226/200000: episode: 712, duration: 0.150s, episode steps:  78, steps per second: 519, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.567468, mae: 26.190041, mean_q: -38.213610, mean_eps: 0.050000\n",
      " 112297/200000: episode: 713, duration: 0.130s, episode steps:  71, steps per second: 544, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 1.807644, mae: 25.883462, mean_q: -37.724036, mean_eps: 0.050000\n",
      " 112391/200000: episode: 714, duration: 0.168s, episode steps:  94, steps per second: 560, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 1.292747, mae: 25.194086, mean_q: -36.681303, mean_eps: 0.050000\n",
      " 112467/200000: episode: 715, duration: 0.142s, episode steps:  76, steps per second: 534, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 0.790576, mae: 25.960472, mean_q: -37.948422, mean_eps: 0.050000\n",
      " 112552/200000: episode: 716, duration: 0.166s, episode steps:  85, steps per second: 513, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.670801, mae: 26.182652, mean_q: -38.225959, mean_eps: 0.050000\n",
      " 112624/200000: episode: 717, duration: 0.142s, episode steps:  72, steps per second: 506, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 1.103895, mae: 25.256627, mean_q: -36.773934, mean_eps: 0.050000\n",
      " 112713/200000: episode: 718, duration: 0.178s, episode steps:  89, steps per second: 499, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 2.133993, mae: 25.903068, mean_q: -37.752663, mean_eps: 0.050000\n",
      " 112804/200000: episode: 719, duration: 0.163s, episode steps:  91, steps per second: 559, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.902233, mae: 25.892217, mean_q: -37.764062, mean_eps: 0.050000\n",
      " 112933/200000: episode: 720, duration: 0.236s, episode steps: 129, steps per second: 546, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 1.379433, mae: 25.553785, mean_q: -37.319875, mean_eps: 0.050000\n",
      " 113015/200000: episode: 721, duration: 0.149s, episode steps:  82, steps per second: 551, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.793 [0.000, 2.000],  loss: 0.685937, mae: 25.872065, mean_q: -37.815689, mean_eps: 0.050000\n",
      " 113112/200000: episode: 722, duration: 0.187s, episode steps:  97, steps per second: 519, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.856 [0.000, 2.000],  loss: 0.588359, mae: 25.374160, mean_q: -36.976718, mean_eps: 0.050000\n",
      " 113203/200000: episode: 723, duration: 0.173s, episode steps:  91, steps per second: 525, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 1.882690, mae: 25.110847, mean_q: -36.495396, mean_eps: 0.050000\n",
      " 113301/200000: episode: 724, duration: 0.179s, episode steps:  98, steps per second: 548, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.611023, mae: 25.813874, mean_q: -37.605992, mean_eps: 0.050000\n",
      " 113383/200000: episode: 725, duration: 0.147s, episode steps:  82, steps per second: 558, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.572498, mae: 25.270813, mean_q: -36.809381, mean_eps: 0.050000\n",
      " 113489/200000: episode: 726, duration: 0.218s, episode steps: 106, steps per second: 486, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.642107, mae: 25.321283, mean_q: -36.896084, mean_eps: 0.050000\n",
      " 113580/200000: episode: 727, duration: 0.175s, episode steps:  91, steps per second: 521, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.911310, mae: 25.558130, mean_q: -37.151618, mean_eps: 0.050000\n",
      " 113662/200000: episode: 728, duration: 0.164s, episode steps:  82, steps per second: 499, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 1.213379, mae: 25.779648, mean_q: -37.460928, mean_eps: 0.050000\n",
      " 113746/200000: episode: 729, duration: 0.166s, episode steps:  84, steps per second: 506, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.929909, mae: 26.049650, mean_q: -37.944823, mean_eps: 0.050000\n",
      " 113814/200000: episode: 730, duration: 0.125s, episode steps:  68, steps per second: 546, episode reward: -67.000, mean reward: -0.985 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.621857, mae: 26.124856, mean_q: -38.081410, mean_eps: 0.050000\n",
      " 113913/200000: episode: 731, duration: 0.189s, episode steps:  99, steps per second: 523, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 1.520921, mae: 24.950172, mean_q: -36.182568, mean_eps: 0.050000\n",
      " 114002/200000: episode: 732, duration: 0.175s, episode steps:  89, steps per second: 509, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.527743, mae: 24.909123, mean_q: -36.183740, mean_eps: 0.050000\n",
      " 114080/200000: episode: 733, duration: 0.151s, episode steps:  78, steps per second: 518, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.609538, mae: 25.670086, mean_q: -37.308812, mean_eps: 0.050000\n",
      " 114160/200000: episode: 734, duration: 0.166s, episode steps:  80, steps per second: 481, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.509277, mae: 25.495429, mean_q: -37.085078, mean_eps: 0.050000\n",
      " 114231/200000: episode: 735, duration: 0.135s, episode steps:  71, steps per second: 525, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.724867, mae: 24.801999, mean_q: -36.108507, mean_eps: 0.050000\n",
      " 114324/200000: episode: 736, duration: 0.181s, episode steps:  93, steps per second: 512, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.633097, mae: 25.563643, mean_q: -37.239781, mean_eps: 0.050000\n",
      " 114413/200000: episode: 737, duration: 0.164s, episode steps:  89, steps per second: 542, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.478826, mae: 25.659098, mean_q: -37.393312, mean_eps: 0.050000\n",
      " 114496/200000: episode: 738, duration: 0.168s, episode steps:  83, steps per second: 493, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 1.000342, mae: 25.585503, mean_q: -37.305241, mean_eps: 0.050000\n",
      " 114603/200000: episode: 739, duration: 0.215s, episode steps: 107, steps per second: 497, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.695382, mae: 25.884980, mean_q: -37.656978, mean_eps: 0.050000\n",
      " 114698/200000: episode: 740, duration: 0.183s, episode steps:  95, steps per second: 520, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 2.166585, mae: 25.716315, mean_q: -37.361356, mean_eps: 0.050000\n",
      " 114776/200000: episode: 741, duration: 0.152s, episode steps:  78, steps per second: 513, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.768392, mae: 25.587610, mean_q: -37.326125, mean_eps: 0.050000\n",
      " 114873/200000: episode: 742, duration: 0.195s, episode steps:  97, steps per second: 497, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.690336, mae: 25.559632, mean_q: -37.238814, mean_eps: 0.050000\n",
      " 114949/200000: episode: 743, duration: 0.152s, episode steps:  76, steps per second: 500, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.495212, mae: 25.159330, mean_q: -36.701795, mean_eps: 0.050000\n",
      " 115025/200000: episode: 744, duration: 0.158s, episode steps:  76, steps per second: 482, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.775098, mae: 24.649193, mean_q: -35.819218, mean_eps: 0.050000\n",
      " 115107/200000: episode: 745, duration: 0.156s, episode steps:  82, steps per second: 526, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.577612, mae: 24.777946, mean_q: -36.046068, mean_eps: 0.050000\n",
      " 115199/200000: episode: 746, duration: 0.168s, episode steps:  92, steps per second: 549, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.499645, mae: 24.854505, mean_q: -36.203011, mean_eps: 0.050000\n",
      " 115282/200000: episode: 747, duration: 0.162s, episode steps:  83, steps per second: 512, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 1.539617, mae: 24.551594, mean_q: -35.670520, mean_eps: 0.050000\n",
      " 115372/200000: episode: 748, duration: 0.165s, episode steps:  90, steps per second: 546, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 0.473899, mae: 25.285612, mean_q: -36.840627, mean_eps: 0.050000\n",
      " 115539/200000: episode: 749, duration: 0.322s, episode steps: 167, steps per second: 519, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.515 [0.000, 2.000],  loss: 0.554897, mae: 25.393811, mean_q: -37.055095, mean_eps: 0.050000\n",
      " 115627/200000: episode: 750, duration: 0.162s, episode steps:  88, steps per second: 542, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.749334, mae: 24.792267, mean_q: -36.050131, mean_eps: 0.050000\n",
      " 115702/200000: episode: 751, duration: 0.141s, episode steps:  75, steps per second: 533, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.642334, mae: 25.519849, mean_q: -37.190810, mean_eps: 0.050000\n",
      " 115776/200000: episode: 752, duration: 0.144s, episode steps:  74, steps per second: 513, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.691162, mae: 24.393151, mean_q: -35.482590, mean_eps: 0.050000\n",
      " 115860/200000: episode: 753, duration: 0.164s, episode steps:  84, steps per second: 512, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 2.279130, mae: 25.645680, mean_q: -37.378777, mean_eps: 0.050000\n",
      " 115938/200000: episode: 754, duration: 0.145s, episode steps:  78, steps per second: 537, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.795 [0.000, 2.000],  loss: 0.783228, mae: 25.161699, mean_q: -36.627422, mean_eps: 0.050000\n",
      " 116038/200000: episode: 755, duration: 0.180s, episode steps: 100, steps per second: 555, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.600240, mae: 25.038467, mean_q: -36.461321, mean_eps: 0.050000\n",
      " 116116/200000: episode: 756, duration: 0.148s, episode steps:  78, steps per second: 526, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.485913, mae: 25.077242, mean_q: -36.588259, mean_eps: 0.050000\n",
      " 116193/200000: episode: 757, duration: 0.153s, episode steps:  77, steps per second: 503, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.084375, mae: 25.318618, mean_q: -36.895406, mean_eps: 0.050000\n",
      " 116266/200000: episode: 758, duration: 0.138s, episode steps:  73, steps per second: 529, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 1.694634, mae: 25.087848, mean_q: -36.500565, mean_eps: 0.050000\n",
      " 116347/200000: episode: 759, duration: 0.151s, episode steps:  81, steps per second: 536, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 0.711755, mae: 25.580991, mean_q: -37.350968, mean_eps: 0.050000\n",
      " 116468/200000: episode: 760, duration: 0.235s, episode steps: 121, steps per second: 514, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.650960, mae: 24.683587, mean_q: -36.010197, mean_eps: 0.050000\n",
      " 116568/200000: episode: 761, duration: 0.198s, episode steps: 100, steps per second: 506, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 1.001729, mae: 25.085978, mean_q: -36.574286, mean_eps: 0.050000\n",
      " 116655/200000: episode: 762, duration: 0.170s, episode steps:  87, steps per second: 512, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 2.407293, mae: 24.783627, mean_q: -36.077785, mean_eps: 0.050000\n",
      " 116733/200000: episode: 763, duration: 0.153s, episode steps:  78, steps per second: 510, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 1.863818, mae: 25.427517, mean_q: -37.062610, mean_eps: 0.050000\n",
      " 116824/200000: episode: 764, duration: 0.176s, episode steps:  91, steps per second: 516, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.398116, mae: 25.217712, mean_q: -36.800182, mean_eps: 0.050000\n",
      " 116906/200000: episode: 765, duration: 0.165s, episode steps:  82, steps per second: 496, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.485352, mae: 24.980217, mean_q: -36.439807, mean_eps: 0.050000\n",
      " 116993/200000: episode: 766, duration: 0.166s, episode steps:  87, steps per second: 523, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.511885, mae: 25.175940, mean_q: -36.646519, mean_eps: 0.050000\n",
      " 117091/200000: episode: 767, duration: 0.189s, episode steps:  98, steps per second: 517, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.525777, mae: 25.952615, mean_q: -37.865930, mean_eps: 0.050000\n",
      " 117164/200000: episode: 768, duration: 0.138s, episode steps:  73, steps per second: 529, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.646512, mae: 25.262680, mean_q: -36.763102, mean_eps: 0.050000\n",
      " 117246/200000: episode: 769, duration: 0.158s, episode steps:  82, steps per second: 517, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 2.533936, mae: 25.284976, mean_q: -36.774785, mean_eps: 0.050000\n",
      " 117322/200000: episode: 770, duration: 0.151s, episode steps:  76, steps per second: 503, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.571558, mae: 25.468236, mean_q: -37.163535, mean_eps: 0.050000\n",
      " 117433/200000: episode: 771, duration: 0.209s, episode steps: 111, steps per second: 531, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.676 [0.000, 2.000],  loss: 0.530500, mae: 26.047374, mean_q: -38.040524, mean_eps: 0.050000\n",
      " 117568/200000: episode: 772, duration: 0.259s, episode steps: 135, steps per second: 521, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 0.660011, mae: 25.140647, mean_q: -36.623501, mean_eps: 0.050000\n",
      " 117651/200000: episode: 773, duration: 0.157s, episode steps:  83, steps per second: 527, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.834672, mae: 24.708687, mean_q: -35.975147, mean_eps: 0.050000\n",
      " 117744/200000: episode: 774, duration: 0.175s, episode steps:  93, steps per second: 531, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.215 [0.000, 2.000],  loss: 0.506636, mae: 25.323515, mean_q: -36.882709, mean_eps: 0.050000\n",
      " 117860/200000: episode: 775, duration: 0.221s, episode steps: 116, steps per second: 526, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.843197, mae: 25.043784, mean_q: -36.460944, mean_eps: 0.050000\n",
      " 117926/200000: episode: 776, duration: 0.129s, episode steps:  66, steps per second: 512, episode reward: -65.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.721100, mae: 25.053571, mean_q: -36.513919, mean_eps: 0.050000\n",
      " 118007/200000: episode: 777, duration: 0.161s, episode steps:  81, steps per second: 503, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 0.492896, mae: 25.121910, mean_q: -36.660413, mean_eps: 0.050000\n",
      " 118096/200000: episode: 778, duration: 0.180s, episode steps:  89, steps per second: 495, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.416271, mae: 25.041981, mean_q: -36.523665, mean_eps: 0.050000\n",
      " 118178/200000: episode: 779, duration: 0.168s, episode steps:  82, steps per second: 488, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.896950, mae: 24.827101, mean_q: -36.143611, mean_eps: 0.050000\n",
      " 118287/200000: episode: 780, duration: 0.221s, episode steps: 109, steps per second: 493, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 1.205172, mae: 24.710553, mean_q: -35.942804, mean_eps: 0.050000\n",
      " 118403/200000: episode: 781, duration: 0.237s, episode steps: 116, steps per second: 489, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.675830, mae: 24.943904, mean_q: -36.277924, mean_eps: 0.050000\n",
      " 118492/200000: episode: 782, duration: 0.188s, episode steps:  89, steps per second: 474, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.500644, mae: 24.769628, mean_q: -36.061601, mean_eps: 0.050000\n",
      " 118598/200000: episode: 783, duration: 0.215s, episode steps: 106, steps per second: 493, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.686852, mae: 25.437543, mean_q: -37.054355, mean_eps: 0.050000\n",
      " 118673/200000: episode: 784, duration: 0.151s, episode steps:  75, steps per second: 495, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.227 [0.000, 2.000],  loss: 0.561975, mae: 25.194184, mean_q: -36.705661, mean_eps: 0.050000\n",
      " 118752/200000: episode: 785, duration: 0.155s, episode steps:  79, steps per second: 511, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 2.707058, mae: 24.995518, mean_q: -36.357090, mean_eps: 0.050000\n",
      " 118846/200000: episode: 786, duration: 0.182s, episode steps:  94, steps per second: 516, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.580160, mae: 24.664365, mean_q: -36.002699, mean_eps: 0.050000\n",
      " 118925/200000: episode: 787, duration: 0.153s, episode steps:  79, steps per second: 515, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.820630, mae: 25.064497, mean_q: -36.655993, mean_eps: 0.050000\n",
      " 119002/200000: episode: 788, duration: 0.164s, episode steps:  77, steps per second: 470, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 1.295972, mae: 24.927627, mean_q: -36.363694, mean_eps: 0.050000\n",
      " 119098/200000: episode: 789, duration: 0.182s, episode steps:  96, steps per second: 526, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.511739, mae: 24.898568, mean_q: -36.313959, mean_eps: 0.050000\n",
      " 119174/200000: episode: 790, duration: 0.142s, episode steps:  76, steps per second: 535, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.794081, mae: 25.377210, mean_q: -37.054869, mean_eps: 0.050000\n",
      " 119267/200000: episode: 791, duration: 0.196s, episode steps:  93, steps per second: 474, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.912801, mae: 24.910492, mean_q: -36.314820, mean_eps: 0.050000\n",
      " 119355/200000: episode: 792, duration: 0.177s, episode steps:  88, steps per second: 497, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 1.289728, mae: 24.923898, mean_q: -36.356006, mean_eps: 0.050000\n",
      " 119437/200000: episode: 793, duration: 0.158s, episode steps:  82, steps per second: 520, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.447119, mae: 25.324746, mean_q: -37.065121, mean_eps: 0.050000\n",
      " 119513/200000: episode: 794, duration: 0.156s, episode steps:  76, steps per second: 488, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 2.105627, mae: 24.363630, mean_q: -35.570611, mean_eps: 0.050000\n",
      " 119598/200000: episode: 795, duration: 0.153s, episode steps:  85, steps per second: 556, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.463452, mae: 25.148588, mean_q: -36.712076, mean_eps: 0.050000\n",
      " 119677/200000: episode: 796, duration: 0.150s, episode steps:  79, steps per second: 526, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.825928, mae: 24.339749, mean_q: -35.493767, mean_eps: 0.050000\n",
      " 119746/200000: episode: 797, duration: 0.139s, episode steps:  69, steps per second: 497, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.631185, mae: 24.888412, mean_q: -36.379420, mean_eps: 0.050000\n",
      " 119903/200000: episode: 798, duration: 0.288s, episode steps: 157, steps per second: 545, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.401 [0.000, 2.000],  loss: 0.622225, mae: 24.185336, mean_q: -35.280487, mean_eps: 0.050000\n",
      " 119997/200000: episode: 799, duration: 0.181s, episode steps:  94, steps per second: 520, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.665660, mae: 24.925705, mean_q: -36.375585, mean_eps: 0.050000\n",
      " 120073/200000: episode: 800, duration: 0.152s, episode steps:  76, steps per second: 499, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.752002, mae: 24.821135, mean_q: -36.104436, mean_eps: 0.050000\n",
      " 120149/200000: episode: 801, duration: 0.134s, episode steps:  76, steps per second: 567, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 2.707330, mae: 25.057310, mean_q: -36.497016, mean_eps: 0.050000\n",
      " 120248/200000: episode: 802, duration: 0.189s, episode steps:  99, steps per second: 523, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.635681, mae: 24.875727, mean_q: -36.332677, mean_eps: 0.050000\n",
      " 120320/200000: episode: 803, duration: 0.139s, episode steps:  72, steps per second: 518, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.618300, mae: 24.189565, mean_q: -35.301910, mean_eps: 0.050000\n",
      " 120399/200000: episode: 804, duration: 0.154s, episode steps:  79, steps per second: 514, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.466455, mae: 25.142142, mean_q: -36.744810, mean_eps: 0.050000\n",
      " 120481/200000: episode: 805, duration: 0.162s, episode steps:  82, steps per second: 506, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 1.726762, mae: 24.366024, mean_q: -35.488022, mean_eps: 0.050000\n",
      " 120555/200000: episode: 806, duration: 0.144s, episode steps:  74, steps per second: 513, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.576023, mae: 25.737959, mean_q: -37.572701, mean_eps: 0.050000\n",
      " 120631/200000: episode: 807, duration: 0.137s, episode steps:  76, steps per second: 556, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.485948, mae: 25.283826, mean_q: -36.961321, mean_eps: 0.050000\n",
      " 120712/200000: episode: 808, duration: 0.170s, episode steps:  81, steps per second: 476, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.130493, mae: 24.938754, mean_q: -36.302778, mean_eps: 0.050000\n",
      " 120788/200000: episode: 809, duration: 0.151s, episode steps:  76, steps per second: 502, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.411548, mae: 25.637643, mean_q: -37.557458, mean_eps: 0.050000\n",
      " 120859/200000: episode: 810, duration: 0.137s, episode steps:  71, steps per second: 520, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.523926, mae: 25.120237, mean_q: -36.695679, mean_eps: 0.050000\n",
      " 120959/200000: episode: 811, duration: 0.181s, episode steps: 100, steps per second: 554, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.506012, mae: 25.349233, mean_q: -36.997916, mean_eps: 0.050000\n",
      " 121052/200000: episode: 812, duration: 0.186s, episode steps:  93, steps per second: 500, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.561412, mae: 25.312535, mean_q: -36.974110, mean_eps: 0.050000\n",
      " 121129/200000: episode: 813, duration: 0.151s, episode steps:  77, steps per second: 509, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.576953, mae: 24.075442, mean_q: -35.061150, mean_eps: 0.050000\n",
      " 121211/200000: episode: 814, duration: 0.158s, episode steps:  82, steps per second: 520, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.956738, mae: 24.534763, mean_q: -35.705148, mean_eps: 0.050000\n",
      " 121304/200000: episode: 815, duration: 0.183s, episode steps:  93, steps per second: 509, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.683794, mae: 24.835975, mean_q: -36.195870, mean_eps: 0.050000\n",
      " 121436/200000: episode: 816, duration: 0.261s, episode steps: 132, steps per second: 507, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.674 [0.000, 2.000],  loss: 2.112118, mae: 24.565858, mean_q: -35.786444, mean_eps: 0.050000\n",
      " 121513/200000: episode: 817, duration: 0.163s, episode steps:  77, steps per second: 472, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.539478, mae: 24.722128, mean_q: -36.025480, mean_eps: 0.050000\n",
      " 121588/200000: episode: 818, duration: 0.146s, episode steps:  75, steps per second: 513, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.599908, mae: 24.887085, mean_q: -36.285139, mean_eps: 0.050000\n",
      " 121656/200000: episode: 819, duration: 0.137s, episode steps:  68, steps per second: 498, episode reward: -67.000, mean reward: -0.985 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.529205, mae: 24.030601, mean_q: -35.038785, mean_eps: 0.050000\n",
      " 121718/200000: episode: 820, duration: 0.122s, episode steps:  62, steps per second: 508, episode reward: -61.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.791763, mae: 24.652648, mean_q: -35.957529, mean_eps: 0.050000\n",
      " 121789/200000: episode: 821, duration: 0.138s, episode steps:  71, steps per second: 515, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.400201, mae: 25.196078, mean_q: -36.837708, mean_eps: 0.050000\n",
      " 121870/200000: episode: 822, duration: 0.155s, episode steps:  81, steps per second: 522, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 0.735120, mae: 24.626713, mean_q: -35.938937, mean_eps: 0.050000\n",
      " 121954/200000: episode: 823, duration: 0.165s, episode steps:  84, steps per second: 508, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 1.931086, mae: 24.480121, mean_q: -35.659382, mean_eps: 0.050000\n",
      " 122057/200000: episode: 824, duration: 0.203s, episode steps: 103, steps per second: 509, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.491727, mae: 24.675599, mean_q: -36.002862, mean_eps: 0.050000\n",
      " 122126/200000: episode: 825, duration: 0.125s, episode steps:  69, steps per second: 553, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 0.735031, mae: 24.687037, mean_q: -36.009587, mean_eps: 0.050000\n",
      " 122224/200000: episode: 826, duration: 0.205s, episode steps:  98, steps per second: 479, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 0.643463, mae: 25.038788, mean_q: -36.516518, mean_eps: 0.050000\n",
      " 122325/200000: episode: 827, duration: 0.207s, episode steps: 101, steps per second: 487, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.178 [0.000, 2.000],  loss: 1.766526, mae: 24.512921, mean_q: -35.705047, mean_eps: 0.050000\n",
      " 122442/200000: episode: 828, duration: 0.234s, episode steps: 117, steps per second: 500, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.803 [0.000, 2.000],  loss: 0.608716, mae: 24.911828, mean_q: -36.348841, mean_eps: 0.050000\n",
      " 122544/200000: episode: 829, duration: 0.213s, episode steps: 102, steps per second: 479, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.706 [0.000, 2.000],  loss: 0.969401, mae: 24.920247, mean_q: -36.308199, mean_eps: 0.050000\n",
      " 122614/200000: episode: 830, duration: 0.136s, episode steps:  70, steps per second: 516, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.761095, mae: 24.555641, mean_q: -35.669593, mean_eps: 0.050000\n",
      " 122689/200000: episode: 831, duration: 0.150s, episode steps:  75, steps per second: 501, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.550342, mae: 25.320925, mean_q: -37.019766, mean_eps: 0.050000\n",
      " 122761/200000: episode: 832, duration: 0.140s, episode steps:  72, steps per second: 514, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.642198, mae: 25.279919, mean_q: -36.867090, mean_eps: 0.050000\n",
      " 122824/200000: episode: 833, duration: 0.127s, episode steps:  63, steps per second: 494, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.500802, mae: 24.851412, mean_q: -36.298678, mean_eps: 0.050000\n",
      " 122907/200000: episode: 834, duration: 0.164s, episode steps:  83, steps per second: 507, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.679887, mae: 24.599975, mean_q: -35.836637, mean_eps: 0.050000\n",
      " 122983/200000: episode: 835, duration: 0.142s, episode steps:  76, steps per second: 535, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 1.007514, mae: 24.372295, mean_q: -35.459862, mean_eps: 0.050000\n",
      " 123046/200000: episode: 836, duration: 0.124s, episode steps:  63, steps per second: 508, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.744385, mae: 24.617719, mean_q: -35.748283, mean_eps: 0.050000\n",
      " 123120/200000: episode: 837, duration: 0.154s, episode steps:  74, steps per second: 480, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.784 [0.000, 2.000],  loss: 0.635498, mae: 24.657756, mean_q: -35.925140, mean_eps: 0.050000\n",
      " 123199/200000: episode: 838, duration: 0.158s, episode steps:  79, steps per second: 501, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.643872, mae: 25.023590, mean_q: -36.448998, mean_eps: 0.050000\n",
      " 123289/200000: episode: 839, duration: 0.197s, episode steps:  90, steps per second: 458, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.647970, mae: 24.586900, mean_q: -35.796107, mean_eps: 0.050000\n",
      " 123378/200000: episode: 840, duration: 0.164s, episode steps:  89, steps per second: 541, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.446345, mae: 24.751900, mean_q: -36.110712, mean_eps: 0.050000\n",
      " 123457/200000: episode: 841, duration: 0.156s, episode steps:  79, steps per second: 507, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.810 [0.000, 2.000],  loss: 0.618237, mae: 24.565807, mean_q: -35.762845, mean_eps: 0.050000\n",
      " 123533/200000: episode: 842, duration: 0.143s, episode steps:  76, steps per second: 531, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.776 [0.000, 2.000],  loss: 0.720133, mae: 24.469022, mean_q: -35.600272, mean_eps: 0.050000\n",
      " 123608/200000: episode: 843, duration: 0.147s, episode steps:  75, steps per second: 509, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 1.330648, mae: 25.033824, mean_q: -36.413617, mean_eps: 0.050000\n",
      " 123679/200000: episode: 844, duration: 0.134s, episode steps:  71, steps per second: 532, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.877414, mae: 25.195707, mean_q: -36.618853, mean_eps: 0.050000\n",
      " 123758/200000: episode: 845, duration: 0.159s, episode steps:  79, steps per second: 495, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 1.640723, mae: 24.534121, mean_q: -35.607806, mean_eps: 0.050000\n",
      " 123822/200000: episode: 846, duration: 0.123s, episode steps:  64, steps per second: 519, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.528351, mae: 24.460447, mean_q: -35.540998, mean_eps: 0.050000\n",
      " 123900/200000: episode: 847, duration: 0.152s, episode steps:  78, steps per second: 515, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.608704, mae: 25.040164, mean_q: -36.459531, mean_eps: 0.050000\n",
      " 123970/200000: episode: 848, duration: 0.134s, episode steps:  70, steps per second: 522, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.506388, mae: 24.941822, mean_q: -36.319902, mean_eps: 0.050000\n",
      " 124054/200000: episode: 849, duration: 0.157s, episode steps:  84, steps per second: 535, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.581921, mae: 24.927670, mean_q: -36.267550, mean_eps: 0.050000\n",
      " 124128/200000: episode: 850, duration: 0.148s, episode steps:  74, steps per second: 500, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.558051, mae: 24.580119, mean_q: -35.758737, mean_eps: 0.050000\n",
      " 124217/200000: episode: 851, duration: 0.177s, episode steps:  89, steps per second: 504, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.561371, mae: 24.617626, mean_q: -35.787237, mean_eps: 0.050000\n",
      " 124309/200000: episode: 852, duration: 0.181s, episode steps:  92, steps per second: 509, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.658536, mae: 25.155730, mean_q: -36.574734, mean_eps: 0.050000\n",
      " 124401/200000: episode: 853, duration: 0.178s, episode steps:  92, steps per second: 518, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.758525, mae: 24.117246, mean_q: -34.937023, mean_eps: 0.050000\n",
      " 124492/200000: episode: 854, duration: 0.168s, episode steps:  91, steps per second: 541, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.775879, mae: 24.435199, mean_q: -35.451289, mean_eps: 0.050000\n",
      " 124579/200000: episode: 855, duration: 0.177s, episode steps:  87, steps per second: 493, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 0.640647, mae: 25.231034, mean_q: -36.690947, mean_eps: 0.050000\n",
      " 124676/200000: episode: 856, duration: 0.182s, episode steps:  97, steps per second: 533, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.780518, mae: 24.803806, mean_q: -36.011804, mean_eps: 0.050000\n",
      " 124775/200000: episode: 857, duration: 0.183s, episode steps:  99, steps per second: 540, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.669637, mae: 24.684843, mean_q: -35.854920, mean_eps: 0.050000\n",
      " 124905/200000: episode: 858, duration: 0.250s, episode steps: 130, steps per second: 521, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 0.598963, mae: 24.610925, mean_q: -35.742920, mean_eps: 0.050000\n",
      " 124985/200000: episode: 859, duration: 0.150s, episode steps:  80, steps per second: 532, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.577588, mae: 24.516665, mean_q: -35.577531, mean_eps: 0.050000\n",
      " 125064/200000: episode: 860, duration: 0.162s, episode steps:  79, steps per second: 488, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.571343, mae: 24.286811, mean_q: -35.153442, mean_eps: 0.050000\n",
      " 125202/200000: episode: 861, duration: 0.282s, episode steps: 138, steps per second: 489, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.275 [0.000, 2.000],  loss: 0.927979, mae: 24.285136, mean_q: -35.236690, mean_eps: 0.050000\n",
      " 125273/200000: episode: 862, duration: 0.144s, episode steps:  71, steps per second: 493, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.127 [0.000, 2.000],  loss: 0.698134, mae: 24.693881, mean_q: -35.938905, mean_eps: 0.050000\n",
      " 125356/200000: episode: 863, duration: 0.159s, episode steps:  83, steps per second: 522, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.855 [0.000, 2.000],  loss: 0.564697, mae: 25.065774, mean_q: -36.491423, mean_eps: 0.050000\n",
      " 125443/200000: episode: 864, duration: 0.165s, episode steps:  87, steps per second: 528, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 1.120628, mae: 25.629126, mean_q: -37.305930, mean_eps: 0.050000\n",
      " 125521/200000: episode: 865, duration: 0.156s, episode steps:  78, steps per second: 499, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 1.747949, mae: 24.500844, mean_q: -35.633127, mean_eps: 0.050000\n",
      " 125612/200000: episode: 866, duration: 0.164s, episode steps:  91, steps per second: 554, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.993652, mae: 23.517347, mean_q: -34.028238, mean_eps: 0.050000\n",
      " 125689/200000: episode: 867, duration: 0.148s, episode steps:  77, steps per second: 521, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.451123, mae: 24.572653, mean_q: -35.814646, mean_eps: 0.050000\n",
      " 125758/200000: episode: 868, duration: 0.122s, episode steps:  69, steps per second: 568, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.635040, mae: 24.247973, mean_q: -35.281040, mean_eps: 0.050000\n",
      " 125836/200000: episode: 869, duration: 0.159s, episode steps:  78, steps per second: 491, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.612793, mae: 24.301572, mean_q: -35.393097, mean_eps: 0.050000\n",
      " 125915/200000: episode: 870, duration: 0.155s, episode steps:  79, steps per second: 510, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.411719, mae: 25.173688, mean_q: -36.704812, mean_eps: 0.050000\n",
      " 125990/200000: episode: 871, duration: 0.144s, episode steps:  75, steps per second: 521, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.495470, mae: 23.716628, mean_q: -34.455979, mean_eps: 0.050000\n",
      " 126083/200000: episode: 872, duration: 0.187s, episode steps:  93, steps per second: 498, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 0.760844, mae: 24.343616, mean_q: -35.291379, mean_eps: 0.050000\n",
      " 126159/200000: episode: 873, duration: 0.141s, episode steps:  76, steps per second: 541, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.587429, mae: 24.401376, mean_q: -35.462656, mean_eps: 0.050000\n",
      " 126317/200000: episode: 874, duration: 0.306s, episode steps: 158, steps per second: 516, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.481 [0.000, 2.000],  loss: 0.491382, mae: 24.703307, mean_q: -35.958693, mean_eps: 0.050000\n",
      " 126396/200000: episode: 875, duration: 0.155s, episode steps:  79, steps per second: 510, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 1.019116, mae: 24.942749, mean_q: -36.323042, mean_eps: 0.050000\n",
      " 126467/200000: episode: 876, duration: 0.138s, episode steps:  71, steps per second: 516, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.726183, mae: 24.657393, mean_q: -35.923201, mean_eps: 0.050000\n",
      " 126538/200000: episode: 877, duration: 0.138s, episode steps:  71, steps per second: 516, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 0.532227, mae: 24.160570, mean_q: -35.153935, mean_eps: 0.050000\n",
      " 126614/200000: episode: 878, duration: 0.139s, episode steps:  76, steps per second: 549, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 1.617567, mae: 24.373005, mean_q: -35.422042, mean_eps: 0.050000\n",
      " 126684/200000: episode: 879, duration: 0.138s, episode steps:  70, steps per second: 509, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.834907, mae: 24.259482, mean_q: -35.314029, mean_eps: 0.050000\n",
      " 126766/200000: episode: 880, duration: 0.156s, episode steps:  82, steps per second: 527, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.632764, mae: 24.784825, mean_q: -36.127296, mean_eps: 0.050000\n",
      " 126850/200000: episode: 881, duration: 0.196s, episode steps:  84, steps per second: 428, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 0.493675, mae: 25.367388, mean_q: -37.115806, mean_eps: 0.050000\n",
      " 126926/200000: episode: 882, duration: 0.149s, episode steps:  76, steps per second: 509, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 1.865370, mae: 24.365118, mean_q: -35.504342, mean_eps: 0.050000\n",
      " 127020/200000: episode: 883, duration: 0.188s, episode steps:  94, steps per second: 500, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.160 [0.000, 2.000],  loss: 0.459605, mae: 25.266046, mean_q: -36.891658, mean_eps: 0.050000\n",
      " 127103/200000: episode: 884, duration: 0.176s, episode steps:  83, steps per second: 473, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 3.387549, mae: 24.681839, mean_q: -35.805756, mean_eps: 0.050000\n",
      " 127226/200000: episode: 885, duration: 0.250s, episode steps: 123, steps per second: 493, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.797 [0.000, 2.000],  loss: 0.618881, mae: 24.646138, mean_q: -35.832308, mean_eps: 0.050000\n",
      " 127315/200000: episode: 886, duration: 0.169s, episode steps:  89, steps per second: 526, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.526045, mae: 24.888496, mean_q: -36.280750, mean_eps: 0.050000\n",
      " 127406/200000: episode: 887, duration: 0.179s, episode steps:  91, steps per second: 509, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.462491, mae: 24.995717, mean_q: -36.404140, mean_eps: 0.050000\n",
      " 127479/200000: episode: 888, duration: 0.140s, episode steps:  73, steps per second: 522, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.822 [0.000, 2.000],  loss: 0.502957, mae: 25.578844, mean_q: -37.300049, mean_eps: 0.050000\n",
      " 127566/200000: episode: 889, duration: 0.182s, episode steps:  87, steps per second: 478, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.683427, mae: 24.934146, mean_q: -36.241467, mean_eps: 0.050000\n",
      " 127656/200000: episode: 890, duration: 0.181s, episode steps:  90, steps per second: 497, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 1.866799, mae: 24.780417, mean_q: -36.005404, mean_eps: 0.050000\n",
      " 127720/200000: episode: 891, duration: 0.137s, episode steps:  64, steps per second: 468, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.772339, mae: 24.868742, mean_q: -36.172771, mean_eps: 0.050000\n",
      " 127799/200000: episode: 892, duration: 0.157s, episode steps:  79, steps per second: 504, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 1.096960, mae: 23.653343, mean_q: -34.406615, mean_eps: 0.050000\n",
      " 127892/200000: episode: 893, duration: 0.188s, episode steps:  93, steps per second: 495, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 1.043671, mae: 23.883687, mean_q: -34.764415, mean_eps: 0.050000\n",
      " 127978/200000: episode: 894, duration: 0.170s, episode steps:  86, steps per second: 507, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.685536, mae: 24.602500, mean_q: -35.752511, mean_eps: 0.050000\n",
      " 128062/200000: episode: 895, duration: 0.165s, episode steps:  84, steps per second: 508, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.513794, mae: 24.519729, mean_q: -35.653490, mean_eps: 0.050000\n",
      " 128149/200000: episode: 896, duration: 0.173s, episode steps:  87, steps per second: 502, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 0.561235, mae: 24.867760, mean_q: -36.118992, mean_eps: 0.050000\n",
      " 128226/200000: episode: 897, duration: 0.158s, episode steps:  77, steps per second: 487, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.560681, mae: 24.761172, mean_q: -35.957165, mean_eps: 0.050000\n",
      " 128317/200000: episode: 898, duration: 0.174s, episode steps:  91, steps per second: 522, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.813 [0.000, 2.000],  loss: 0.570024, mae: 24.427125, mean_q: -35.482280, mean_eps: 0.050000\n",
      " 128414/200000: episode: 899, duration: 0.193s, episode steps:  97, steps per second: 503, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.742 [0.000, 2.000],  loss: 0.511505, mae: 24.013650, mean_q: -34.934120, mean_eps: 0.050000\n",
      " 128516/200000: episode: 900, duration: 0.203s, episode steps: 102, steps per second: 503, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.765 [0.000, 2.000],  loss: 0.620671, mae: 23.815789, mean_q: -34.554083, mean_eps: 0.050000\n",
      " 128595/200000: episode: 901, duration: 0.155s, episode steps:  79, steps per second: 508, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 0.523022, mae: 25.272223, mean_q: -36.761224, mean_eps: 0.050000\n",
      " 128690/200000: episode: 902, duration: 0.186s, episode steps:  95, steps per second: 510, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.789 [0.000, 2.000],  loss: 0.541087, mae: 24.967326, mean_q: -36.378559, mean_eps: 0.050000\n",
      " 128777/200000: episode: 903, duration: 0.162s, episode steps:  87, steps per second: 536, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.770 [0.000, 2.000],  loss: 0.434071, mae: 25.011491, mean_q: -36.478809, mean_eps: 0.050000\n",
      " 128862/200000: episode: 904, duration: 0.154s, episode steps:  85, steps per second: 551, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.847 [0.000, 2.000],  loss: 1.254529, mae: 25.120304, mean_q: -36.565919, mean_eps: 0.050000\n",
      " 128953/200000: episode: 905, duration: 0.184s, episode steps:  91, steps per second: 493, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.664530, mae: 23.778760, mean_q: -34.623106, mean_eps: 0.050000\n",
      " 129045/200000: episode: 906, duration: 0.180s, episode steps:  92, steps per second: 511, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.542769, mae: 25.057528, mean_q: -36.522903, mean_eps: 0.050000\n",
      " 129107/200000: episode: 907, duration: 0.125s, episode steps:  62, steps per second: 494, episode reward: -61.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.669708, mae: 24.274950, mean_q: -35.295219, mean_eps: 0.050000\n",
      " 129185/200000: episode: 908, duration: 0.155s, episode steps:  78, steps per second: 505, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 1.904541, mae: 24.445482, mean_q: -35.536002, mean_eps: 0.050000\n",
      " 129259/200000: episode: 909, duration: 0.139s, episode steps:  74, steps per second: 533, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 0.509413, mae: 24.560180, mean_q: -35.795101, mean_eps: 0.050000\n",
      " 129322/200000: episode: 910, duration: 0.124s, episode steps:  63, steps per second: 510, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.825 [0.000, 2.000],  loss: 0.525818, mae: 24.108641, mean_q: -35.021059, mean_eps: 0.050000\n",
      " 129423/200000: episode: 911, duration: 0.187s, episode steps: 101, steps per second: 540, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.723 [0.000, 2.000],  loss: 0.773336, mae: 24.899387, mean_q: -36.268536, mean_eps: 0.050000\n",
      " 129513/200000: episode: 912, duration: 0.191s, episode steps:  90, steps per second: 472, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.733 [0.000, 2.000],  loss: 0.568054, mae: 23.837065, mean_q: -34.671572, mean_eps: 0.050000\n",
      " 129610/200000: episode: 913, duration: 0.197s, episode steps:  97, steps per second: 492, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.732 [0.000, 2.000],  loss: 0.644918, mae: 25.495584, mean_q: -37.128463, mean_eps: 0.050000\n",
      " 129703/200000: episode: 914, duration: 0.173s, episode steps:  93, steps per second: 538, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.503085, mae: 24.670472, mean_q: -35.843512, mean_eps: 0.050000\n",
      " 129783/200000: episode: 915, duration: 0.157s, episode steps:  80, steps per second: 509, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 0.532654, mae: 24.853333, mean_q: -36.110460, mean_eps: 0.050000\n",
      " 129869/200000: episode: 916, duration: 0.172s, episode steps:  86, steps per second: 499, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.721 [0.000, 2.000],  loss: 0.703458, mae: 24.150399, mean_q: -35.035955, mean_eps: 0.050000\n",
      " 129934/200000: episode: 917, duration: 0.126s, episode steps:  65, steps per second: 515, episode reward: -64.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.559372, mae: 24.748978, mean_q: -35.967272, mean_eps: 0.050000\n",
      " 130010/200000: episode: 918, duration: 0.157s, episode steps:  76, steps per second: 484, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.763 [0.000, 2.000],  loss: 0.396753, mae: 25.065242, mean_q: -36.498946, mean_eps: 0.050000\n",
      " 130096/200000: episode: 919, duration: 0.173s, episode steps:  86, steps per second: 496, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.802 [0.000, 2.000],  loss: 2.002344, mae: 24.672859, mean_q: -35.836012, mean_eps: 0.050000\n",
      " 130185/200000: episode: 920, duration: 0.185s, episode steps:  89, steps per second: 481, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.421377, mae: 25.459491, mean_q: -37.110557, mean_eps: 0.050000\n",
      " 130362/200000: episode: 921, duration: 0.345s, episode steps: 177, steps per second: 513, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.514 [0.000, 2.000],  loss: 0.575212, mae: 24.592600, mean_q: -35.802096, mean_eps: 0.050000\n",
      " 130448/200000: episode: 922, duration: 0.170s, episode steps:  86, steps per second: 506, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.591528, mae: 24.104823, mean_q: -34.995716, mean_eps: 0.050000\n",
      " 130539/200000: episode: 923, duration: 0.192s, episode steps:  91, steps per second: 473, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.724670, mae: 24.635723, mean_q: -35.809149, mean_eps: 0.050000\n",
      " 130614/200000: episode: 924, duration: 0.145s, episode steps:  75, steps per second: 516, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.797390, mae: 24.823658, mean_q: -36.059443, mean_eps: 0.050000\n",
      " 130698/200000: episode: 925, duration: 0.178s, episode steps:  84, steps per second: 472, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.422840, mae: 24.628509, mean_q: -35.830338, mean_eps: 0.050000\n",
      " 130769/200000: episode: 926, duration: 0.138s, episode steps:  71, steps per second: 513, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.559896, mae: 24.750969, mean_q: -36.061998, mean_eps: 0.050000\n",
      " 130845/200000: episode: 927, duration: 0.147s, episode steps:  76, steps per second: 517, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.399685, mae: 24.981187, mean_q: -36.364606, mean_eps: 0.050000\n",
      " 131345/200000: episode: 928, duration: 0.998s, episode steps: 500, steps per second: 501, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.396 [0.000, 2.000],  loss: 1.689426, mae: 24.501933, mean_q: -35.591518, mean_eps: 0.050000\n",
      " 131440/200000: episode: 929, duration: 0.189s, episode steps:  95, steps per second: 503, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.447454, mae: 25.683026, mean_q: -37.452319, mean_eps: 0.050000\n",
      " 131534/200000: episode: 930, duration: 0.192s, episode steps:  94, steps per second: 490, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 2.699473, mae: 24.407976, mean_q: -35.403268, mean_eps: 0.050000\n",
      " 131616/200000: episode: 931, duration: 0.167s, episode steps:  82, steps per second: 492, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.722974, mae: 24.445251, mean_q: -35.537364, mean_eps: 0.050000\n",
      " 131686/200000: episode: 932, duration: 0.141s, episode steps:  70, steps per second: 495, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.661730, mae: 24.396047, mean_q: -35.486195, mean_eps: 0.050000\n",
      " 131768/200000: episode: 933, duration: 0.166s, episode steps:  82, steps per second: 495, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.608630, mae: 24.973235, mean_q: -36.315409, mean_eps: 0.050000\n",
      " 131861/200000: episode: 934, duration: 0.188s, episode steps:  93, steps per second: 495, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.645162, mae: 24.877614, mean_q: -36.192904, mean_eps: 0.050000\n",
      " 131950/200000: episode: 935, duration: 0.173s, episode steps:  89, steps per second: 515, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.180 [0.000, 2.000],  loss: 0.542148, mae: 24.262414, mean_q: -35.271066, mean_eps: 0.050000\n",
      " 132027/200000: episode: 936, duration: 0.161s, episode steps:  77, steps per second: 478, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 1.981641, mae: 24.469659, mean_q: -35.468835, mean_eps: 0.050000\n",
      " 132164/200000: episode: 937, duration: 0.268s, episode steps: 137, steps per second: 512, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.544606, mae: 24.860228, mean_q: -36.080018, mean_eps: 0.050000\n",
      " 132227/200000: episode: 938, duration: 0.124s, episode steps:  63, steps per second: 506, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.434586, mae: 24.969682, mean_q: -36.389661, mean_eps: 0.050000\n",
      " 132327/200000: episode: 939, duration: 0.190s, episode steps: 100, steps per second: 527, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.780 [0.000, 2.000],  loss: 0.544495, mae: 24.089737, mean_q: -35.016233, mean_eps: 0.050000\n",
      " 132412/200000: episode: 940, duration: 0.175s, episode steps:  85, steps per second: 486, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 0.444169, mae: 24.441177, mean_q: -35.650785, mean_eps: 0.050000\n",
      " 132489/200000: episode: 941, duration: 0.158s, episode steps:  77, steps per second: 489, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.430713, mae: 24.249014, mean_q: -35.316172, mean_eps: 0.050000\n",
      " 132567/200000: episode: 942, duration: 0.150s, episode steps:  78, steps per second: 519, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 1.024278, mae: 23.614691, mean_q: -34.271551, mean_eps: 0.050000\n",
      " 132635/200000: episode: 943, duration: 0.148s, episode steps:  68, steps per second: 461, episode reward: -67.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.593235, mae: 25.305701, mean_q: -36.858387, mean_eps: 0.050000\n",
      " 132732/200000: episode: 944, duration: 0.191s, episode steps:  97, steps per second: 509, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 0.517558, mae: 24.143147, mean_q: -35.065494, mean_eps: 0.050000\n",
      " 132839/200000: episode: 945, duration: 0.205s, episode steps: 107, steps per second: 521, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.441237, mae: 24.522753, mean_q: -35.697616, mean_eps: 0.050000\n",
      " 132918/200000: episode: 946, duration: 0.159s, episode steps:  79, steps per second: 496, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.445325, mae: 23.941918, mean_q: -34.810292, mean_eps: 0.050000\n",
      " 133013/200000: episode: 947, duration: 0.190s, episode steps:  95, steps per second: 499, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.661896, mae: 24.326142, mean_q: -35.392092, mean_eps: 0.050000\n",
      " 133106/200000: episode: 948, duration: 0.194s, episode steps:  93, steps per second: 480, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.083272, mae: 24.122447, mean_q: -35.073440, mean_eps: 0.050000\n",
      " 133181/200000: episode: 949, duration: 0.151s, episode steps:  75, steps per second: 496, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.651557, mae: 24.182429, mean_q: -35.188122, mean_eps: 0.050000\n",
      " 133265/200000: episode: 950, duration: 0.171s, episode steps:  84, steps per second: 491, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 1.902299, mae: 24.281099, mean_q: -35.311379, mean_eps: 0.050000\n",
      " 133366/200000: episode: 951, duration: 0.195s, episode steps: 101, steps per second: 519, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.674886, mae: 23.964455, mean_q: -34.864000, mean_eps: 0.050000\n",
      " 133473/200000: episode: 952, duration: 0.220s, episode steps: 107, steps per second: 487, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.757 [0.000, 2.000],  loss: 1.557704, mae: 24.016566, mean_q: -34.953390, mean_eps: 0.050000\n",
      " 133551/200000: episode: 953, duration: 0.147s, episode steps:  78, steps per second: 530, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.796929, mae: 24.271393, mean_q: -35.316154, mean_eps: 0.050000\n",
      " 133646/200000: episode: 954, duration: 0.192s, episode steps:  95, steps per second: 495, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.605367, mae: 23.936280, mean_q: -34.751069, mean_eps: 0.050000\n",
      " 133741/200000: episode: 955, duration: 0.188s, episode steps:  95, steps per second: 505, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.562948, mae: 24.308420, mean_q: -35.380113, mean_eps: 0.050000\n",
      " 133817/200000: episode: 956, duration: 0.159s, episode steps:  76, steps per second: 478, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.569434, mae: 23.629513, mean_q: -34.330563, mean_eps: 0.050000\n",
      " 133908/200000: episode: 957, duration: 0.175s, episode steps:  91, steps per second: 519, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.458319, mae: 24.315465, mean_q: -35.407729, mean_eps: 0.050000\n",
      " 133979/200000: episode: 958, duration: 0.148s, episode steps:  71, steps per second: 481, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.455458, mae: 23.726852, mean_q: -34.527355, mean_eps: 0.050000\n",
      " 134065/200000: episode: 959, duration: 0.174s, episode steps:  86, steps per second: 493, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 1.645286, mae: 23.717274, mean_q: -34.458561, mean_eps: 0.050000\n",
      " 134144/200000: episode: 960, duration: 0.161s, episode steps:  79, steps per second: 491, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 0.776421, mae: 23.535423, mean_q: -34.142235, mean_eps: 0.050000\n",
      " 134220/200000: episode: 961, duration: 0.157s, episode steps:  76, steps per second: 483, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.842664, mae: 24.625908, mean_q: -35.815635, mean_eps: 0.050000\n",
      " 134310/200000: episode: 962, duration: 0.180s, episode steps:  90, steps per second: 499, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.844 [0.000, 2.000],  loss: 0.606279, mae: 24.361037, mean_q: -35.471487, mean_eps: 0.050000\n",
      " 134379/200000: episode: 963, duration: 0.137s, episode steps:  69, steps per second: 502, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.529405, mae: 23.974078, mean_q: -34.952796, mean_eps: 0.050000\n",
      " 134480/200000: episode: 964, duration: 0.209s, episode steps: 101, steps per second: 483, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.577596, mae: 24.133234, mean_q: -35.176826, mean_eps: 0.050000\n",
      " 134550/200000: episode: 965, duration: 0.147s, episode steps:  70, steps per second: 476, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.409722, mae: 24.528861, mean_q: -35.753304, mean_eps: 0.050000\n",
      " 134625/200000: episode: 966, duration: 0.156s, episode steps:  75, steps per second: 480, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 0.485132, mae: 24.548830, mean_q: -35.709496, mean_eps: 0.050000\n",
      " 134711/200000: episode: 967, duration: 0.156s, episode steps:  86, steps per second: 552, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.443506, mae: 24.142722, mean_q: -35.134692, mean_eps: 0.050000\n",
      " 134789/200000: episode: 968, duration: 0.152s, episode steps:  78, steps per second: 512, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.795 [0.000, 2.000],  loss: 0.430334, mae: 23.666734, mean_q: -34.436916, mean_eps: 0.050000\n",
      " 134863/200000: episode: 969, duration: 0.137s, episode steps:  74, steps per second: 540, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 0.431532, mae: 23.980432, mean_q: -34.871866, mean_eps: 0.050000\n",
      " 134934/200000: episode: 970, duration: 0.147s, episode steps:  71, steps per second: 482, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.500543, mae: 24.574100, mean_q: -35.780908, mean_eps: 0.050000\n",
      " 135131/200000: episode: 971, duration: 0.384s, episode steps: 197, steps per second: 513, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.569 [0.000, 2.000],  loss: 0.607031, mae: 23.812613, mean_q: -34.583023, mean_eps: 0.050000\n",
      " 135210/200000: episode: 972, duration: 0.153s, episode steps:  79, steps per second: 517, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 0.416064, mae: 24.204365, mean_q: -35.195985, mean_eps: 0.050000\n",
      " 135281/200000: episode: 973, duration: 0.149s, episode steps:  71, steps per second: 475, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 1.232992, mae: 24.408815, mean_q: -35.494932, mean_eps: 0.050000\n",
      " 135359/200000: episode: 974, duration: 0.140s, episode steps:  78, steps per second: 555, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 1.355360, mae: 24.154945, mean_q: -35.132133, mean_eps: 0.050000\n",
      " 135447/200000: episode: 975, duration: 0.174s, episode steps:  88, steps per second: 507, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.529907, mae: 24.639443, mean_q: -35.966576, mean_eps: 0.050000\n",
      " 135527/200000: episode: 976, duration: 0.157s, episode steps:  80, steps per second: 510, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.555151, mae: 25.032073, mean_q: -36.466697, mean_eps: 0.050000\n",
      " 135623/200000: episode: 977, duration: 0.186s, episode steps:  96, steps per second: 517, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.592875, mae: 24.377628, mean_q: -35.483669, mean_eps: 0.050000\n",
      " 135726/200000: episode: 978, duration: 0.196s, episode steps: 103, steps per second: 525, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 0.361206, mae: 24.907058, mean_q: -36.341487, mean_eps: 0.050000\n",
      " 135812/200000: episode: 979, duration: 0.168s, episode steps:  86, steps per second: 512, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.422452, mae: 24.565892, mean_q: -35.750981, mean_eps: 0.050000\n",
      " 135936/200000: episode: 980, duration: 0.246s, episode steps: 124, steps per second: 503, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 0.390682, mae: 24.786178, mean_q: -36.099255, mean_eps: 0.050000\n",
      " 136020/200000: episode: 981, duration: 0.167s, episode steps:  84, steps per second: 502, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.468195, mae: 23.649861, mean_q: -34.319819, mean_eps: 0.050000\n",
      " 136095/200000: episode: 982, duration: 0.146s, episode steps:  75, steps per second: 513, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 2.151367, mae: 23.909116, mean_q: -34.619490, mean_eps: 0.050000\n",
      " 136166/200000: episode: 983, duration: 0.131s, episode steps:  71, steps per second: 542, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.470188, mae: 24.112600, mean_q: -34.992547, mean_eps: 0.050000\n",
      " 136282/200000: episode: 984, duration: 0.230s, episode steps: 116, steps per second: 505, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.190 [0.000, 2.000],  loss: 0.685352, mae: 23.805879, mean_q: -34.554937, mean_eps: 0.050000\n",
      " 136375/200000: episode: 985, duration: 0.171s, episode steps:  93, steps per second: 542, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.536932, mae: 24.558135, mean_q: -35.740949, mean_eps: 0.050000\n",
      " 136453/200000: episode: 986, duration: 0.166s, episode steps:  78, steps per second: 470, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 1.177490, mae: 25.222709, mean_q: -36.754856, mean_eps: 0.050000\n",
      " 136545/200000: episode: 987, duration: 0.193s, episode steps:  92, steps per second: 478, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.185 [0.000, 2.000],  loss: 0.520030, mae: 23.932185, mean_q: -34.748282, mean_eps: 0.050000\n",
      " 136635/200000: episode: 988, duration: 0.177s, episode steps:  90, steps per second: 509, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.548251, mae: 23.963427, mean_q: -34.886215, mean_eps: 0.050000\n",
      " 136720/200000: episode: 989, duration: 0.174s, episode steps:  85, steps per second: 490, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 0.452954, mae: 23.330309, mean_q: -33.912945, mean_eps: 0.050000\n",
      " 136817/200000: episode: 990, duration: 0.199s, episode steps:  97, steps per second: 488, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 0.529391, mae: 23.771305, mean_q: -34.516987, mean_eps: 0.050000\n",
      " 136902/200000: episode: 991, duration: 0.157s, episode steps:  85, steps per second: 542, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.573926, mae: 23.721391, mean_q: -34.428571, mean_eps: 0.050000\n",
      " 136980/200000: episode: 992, duration: 0.158s, episode steps:  78, steps per second: 495, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.432410, mae: 24.429615, mean_q: -35.535150, mean_eps: 0.050000\n",
      " 137051/200000: episode: 993, duration: 0.137s, episode steps:  71, steps per second: 518, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.715251, mae: 23.759901, mean_q: -34.421718, mean_eps: 0.050000\n",
      " 137152/200000: episode: 994, duration: 0.202s, episode steps: 101, steps per second: 501, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.317 [0.000, 2.000],  loss: 0.597127, mae: 23.479358, mean_q: -34.094733, mean_eps: 0.050000\n",
      " 137228/200000: episode: 995, duration: 0.148s, episode steps:  76, steps per second: 514, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 0.547583, mae: 24.163822, mean_q: -35.207700, mean_eps: 0.050000\n",
      " 137300/200000: episode: 996, duration: 0.138s, episode steps:  72, steps per second: 523, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.449599, mae: 23.503833, mean_q: -34.199348, mean_eps: 0.050000\n",
      " 137419/200000: episode: 997, duration: 0.235s, episode steps: 119, steps per second: 506, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.468742, mae: 23.794641, mean_q: -34.645402, mean_eps: 0.050000\n",
      " 137512/200000: episode: 998, duration: 0.185s, episode steps:  93, steps per second: 504, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.280 [0.000, 2.000],  loss: 1.010787, mae: 23.875909, mean_q: -34.667983, mean_eps: 0.050000\n",
      " 137601/200000: episode: 999, duration: 0.187s, episode steps:  89, steps per second: 476, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.794983, mae: 23.707899, mean_q: -34.327669, mean_eps: 0.050000\n",
      " 137757/200000: episode: 1000, duration: 0.303s, episode steps: 156, steps per second: 514, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.474 [0.000, 2.000],  loss: 0.979672, mae: 24.326673, mean_q: -35.389280, mean_eps: 0.050000\n",
      " 137895/200000: episode: 1001, duration: 0.270s, episode steps: 138, steps per second: 512, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 0.464427, mae: 24.267517, mean_q: -35.341954, mean_eps: 0.050000\n",
      " 137990/200000: episode: 1002, duration: 0.188s, episode steps:  95, steps per second: 506, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.689351, mae: 23.894795, mean_q: -34.770240, mean_eps: 0.050000\n",
      " 138087/200000: episode: 1003, duration: 0.190s, episode steps:  97, steps per second: 510, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.206 [0.000, 2.000],  loss: 1.504232, mae: 24.191545, mean_q: -35.109911, mean_eps: 0.050000\n",
      " 138162/200000: episode: 1004, duration: 0.156s, episode steps:  75, steps per second: 482, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.628174, mae: 23.896656, mean_q: -34.649831, mean_eps: 0.050000\n",
      " 138307/200000: episode: 1005, duration: 0.281s, episode steps: 145, steps per second: 516, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.855 [0.000, 2.000],  loss: 0.657301, mae: 24.172679, mean_q: -35.164704, mean_eps: 0.050000\n",
      " 138395/200000: episode: 1006, duration: 0.169s, episode steps:  88, steps per second: 522, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 0.514127, mae: 24.111079, mean_q: -35.109147, mean_eps: 0.050000\n",
      " 138489/200000: episode: 1007, duration: 0.188s, episode steps:  94, steps per second: 499, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.787 [0.000, 2.000],  loss: 0.493296, mae: 24.272799, mean_q: -35.333372, mean_eps: 0.050000\n",
      " 138580/200000: episode: 1008, duration: 0.178s, episode steps:  91, steps per second: 510, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.612793, mae: 24.710795, mean_q: -35.863580, mean_eps: 0.050000\n",
      " 138652/200000: episode: 1009, duration: 0.145s, episode steps:  72, steps per second: 497, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.533339, mae: 24.759719, mean_q: -35.975842, mean_eps: 0.050000\n",
      " 138751/200000: episode: 1010, duration: 0.193s, episode steps:  99, steps per second: 512, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.519816, mae: 24.123322, mean_q: -35.075893, mean_eps: 0.050000\n",
      " 138854/200000: episode: 1011, duration: 0.206s, episode steps: 103, steps per second: 501, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.517315, mae: 24.662887, mean_q: -35.932819, mean_eps: 0.050000\n",
      " 138939/200000: episode: 1012, duration: 0.173s, episode steps:  85, steps per second: 492, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.628695, mae: 24.261122, mean_q: -35.337334, mean_eps: 0.050000\n",
      " 139015/200000: episode: 1013, duration: 0.155s, episode steps:  76, steps per second: 491, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.531928, mae: 24.095095, mean_q: -35.091824, mean_eps: 0.050000\n",
      " 139088/200000: episode: 1014, duration: 0.152s, episode steps:  73, steps per second: 480, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.526042, mae: 24.109065, mean_q: -35.041768, mean_eps: 0.050000\n",
      " 139186/200000: episode: 1015, duration: 0.205s, episode steps:  98, steps per second: 478, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.618314, mae: 24.677646, mean_q: -35.883959, mean_eps: 0.050000\n",
      " 139277/200000: episode: 1016, duration: 0.181s, episode steps:  91, steps per second: 502, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.954301, mae: 24.443403, mean_q: -35.500784, mean_eps: 0.050000\n",
      " 139354/200000: episode: 1017, duration: 0.162s, episode steps:  77, steps per second: 476, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.545703, mae: 23.973018, mean_q: -34.851024, mean_eps: 0.050000\n",
      " 139430/200000: episode: 1018, duration: 0.140s, episode steps:  76, steps per second: 544, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.624186, mae: 23.643542, mean_q: -34.424401, mean_eps: 0.050000\n",
      " 139538/200000: episode: 1019, duration: 0.223s, episode steps: 108, steps per second: 483, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.828430, mae: 24.029682, mean_q: -34.974014, mean_eps: 0.050000\n",
      " 139617/200000: episode: 1020, duration: 0.161s, episode steps:  79, steps per second: 489, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.559204, mae: 24.679784, mean_q: -35.900663, mean_eps: 0.050000\n",
      " 139704/200000: episode: 1021, duration: 0.178s, episode steps:  87, steps per second: 490, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.642139, mae: 24.016082, mean_q: -34.905280, mean_eps: 0.050000\n",
      " 139774/200000: episode: 1022, duration: 0.145s, episode steps:  70, steps per second: 484, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.568793, mae: 24.720350, mean_q: -35.924181, mean_eps: 0.050000\n",
      " 139855/200000: episode: 1023, duration: 0.166s, episode steps:  81, steps per second: 487, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.317957, mae: 25.415416, mean_q: -37.049332, mean_eps: 0.050000\n",
      " 139930/200000: episode: 1024, duration: 0.155s, episode steps:  75, steps per second: 483, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 1.199951, mae: 25.203783, mean_q: -36.656492, mean_eps: 0.050000\n",
      " 140034/200000: episode: 1025, duration: 0.214s, episode steps: 104, steps per second: 486, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.530978, mae: 23.988017, mean_q: -34.907955, mean_eps: 0.050000\n",
      " 140110/200000: episode: 1026, duration: 0.149s, episode steps:  76, steps per second: 511, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.548882, mae: 24.662929, mean_q: -35.857113, mean_eps: 0.050000\n",
      " 140185/200000: episode: 1027, duration: 0.163s, episode steps:  75, steps per second: 459, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.505200, mae: 24.103292, mean_q: -35.124751, mean_eps: 0.050000\n",
      " 140256/200000: episode: 1028, duration: 0.146s, episode steps:  71, steps per second: 486, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.915802, mae: 24.354386, mean_q: -35.396068, mean_eps: 0.050000\n",
      " 140347/200000: episode: 1029, duration: 0.199s, episode steps:  91, steps per second: 457, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.220 [0.000, 2.000],  loss: 0.479604, mae: 23.587856, mean_q: -34.303212, mean_eps: 0.050000\n",
      " 140424/200000: episode: 1030, duration: 0.162s, episode steps:  77, steps per second: 475, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.599609, mae: 23.237775, mean_q: -33.858074, mean_eps: 0.050000\n",
      " 140496/200000: episode: 1031, duration: 0.158s, episode steps:  72, steps per second: 456, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 1.752740, mae: 24.157042, mean_q: -35.200331, mean_eps: 0.050000\n",
      " 140698/200000: episode: 1032, duration: 0.420s, episode steps: 202, steps per second: 481, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.495 [0.000, 2.000],  loss: 0.492854, mae: 23.879646, mean_q: -34.716474, mean_eps: 0.050000\n",
      " 140791/200000: episode: 1033, duration: 0.180s, episode steps:  93, steps per second: 517, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.408791, mae: 24.424068, mean_q: -35.532939, mean_eps: 0.050000\n",
      " 140873/200000: episode: 1034, duration: 0.172s, episode steps:  82, steps per second: 477, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 3.066761, mae: 23.842832, mean_q: -34.563595, mean_eps: 0.050000\n",
      " 140969/200000: episode: 1035, duration: 0.194s, episode steps:  96, steps per second: 494, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.812 [0.000, 2.000],  loss: 0.561523, mae: 23.895507, mean_q: -34.779240, mean_eps: 0.050000\n",
      " 141079/200000: episode: 1036, duration: 0.216s, episode steps: 110, steps per second: 509, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.127 [0.000, 2.000],  loss: 1.768667, mae: 23.139170, mean_q: -33.566115, mean_eps: 0.050000\n",
      " 141169/200000: episode: 1037, duration: 0.188s, episode steps:  90, steps per second: 480, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.919454, mae: 23.403570, mean_q: -33.953365, mean_eps: 0.050000\n",
      " 141252/200000: episode: 1038, duration: 0.167s, episode steps:  83, steps per second: 498, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.626489, mae: 23.698291, mean_q: -34.386604, mean_eps: 0.050000\n",
      " 141328/200000: episode: 1039, duration: 0.161s, episode steps:  76, steps per second: 473, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 1.438748, mae: 23.623671, mean_q: -34.234166, mean_eps: 0.050000\n",
      " 141422/200000: episode: 1040, duration: 0.199s, episode steps:  94, steps per second: 472, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.551758, mae: 23.940123, mean_q: -34.790093, mean_eps: 0.050000\n",
      " 141515/200000: episode: 1041, duration: 0.202s, episode steps:  93, steps per second: 459, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.237 [0.000, 2.000],  loss: 0.447378, mae: 23.823757, mean_q: -34.650754, mean_eps: 0.050000\n",
      " 141584/200000: episode: 1042, duration: 0.167s, episode steps:  69, steps per second: 412, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.855 [0.000, 2.000],  loss: 3.385498, mae: 23.802712, mean_q: -34.487967, mean_eps: 0.050000\n",
      " 141655/200000: episode: 1043, duration: 0.148s, episode steps:  71, steps per second: 480, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.529324, mae: 24.002929, mean_q: -34.801894, mean_eps: 0.050000\n",
      " 141739/200000: episode: 1044, duration: 0.171s, episode steps:  84, steps per second: 490, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.663885, mae: 23.363660, mean_q: -33.883360, mean_eps: 0.050000\n",
      " 141816/200000: episode: 1045, duration: 0.163s, episode steps:  77, steps per second: 473, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.577243, mae: 23.167334, mean_q: -33.570051, mean_eps: 0.050000\n",
      " 141890/200000: episode: 1046, duration: 0.162s, episode steps:  74, steps per second: 457, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.614990, mae: 22.829338, mean_q: -33.131576, mean_eps: 0.050000\n",
      " 141964/200000: episode: 1047, duration: 0.154s, episode steps:  74, steps per second: 480, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 0.431369, mae: 23.187869, mean_q: -33.638602, mean_eps: 0.050000\n",
      " 142028/200000: episode: 1048, duration: 0.136s, episode steps:  64, steps per second: 471, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 0.685638, mae: 23.046422, mean_q: -33.419133, mean_eps: 0.050000\n",
      " 142105/200000: episode: 1049, duration: 0.162s, episode steps:  77, steps per second: 475, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 1.107288, mae: 23.879098, mean_q: -34.719943, mean_eps: 0.050000\n",
      " 142191/200000: episode: 1050, duration: 0.166s, episode steps:  86, steps per second: 519, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.517529, mae: 24.256928, mean_q: -35.330182, mean_eps: 0.050000\n",
      " 142268/200000: episode: 1051, duration: 0.162s, episode steps:  77, steps per second: 475, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.475110, mae: 24.035723, mean_q: -35.060078, mean_eps: 0.050000\n",
      " 142354/200000: episode: 1052, duration: 0.181s, episode steps:  86, steps per second: 476, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.849 [0.000, 2.000],  loss: 0.538441, mae: 23.347118, mean_q: -33.987673, mean_eps: 0.050000\n",
      " 142424/200000: episode: 1053, duration: 0.150s, episode steps:  70, steps per second: 468, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 0.450439, mae: 23.040543, mean_q: -33.565147, mean_eps: 0.050000\n",
      " 142518/200000: episode: 1054, duration: 0.199s, episode steps:  94, steps per second: 473, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.383321, mae: 23.454208, mean_q: -34.111556, mean_eps: 0.050000\n",
      " 142595/200000: episode: 1055, duration: 0.164s, episode steps:  77, steps per second: 468, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.738452, mae: 23.244298, mean_q: -33.695129, mean_eps: 0.050000\n",
      " 142749/200000: episode: 1056, duration: 0.297s, episode steps: 154, steps per second: 518, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 0.544202, mae: 23.256870, mean_q: -33.785606, mean_eps: 0.050000\n",
      " 142833/200000: episode: 1057, duration: 0.171s, episode steps:  84, steps per second: 491, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.798 [0.000, 2.000],  loss: 0.540871, mae: 23.495873, mean_q: -34.117147, mean_eps: 0.050000\n",
      " 142911/200000: episode: 1058, duration: 0.145s, episode steps:  78, steps per second: 538, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.554972, mae: 23.889310, mean_q: -34.700108, mean_eps: 0.050000\n",
      " 143000/200000: episode: 1059, duration: 0.192s, episode steps:  89, steps per second: 464, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.594127, mae: 23.958178, mean_q: -34.832739, mean_eps: 0.050000\n",
      " 143074/200000: episode: 1060, duration: 0.161s, episode steps:  74, steps per second: 459, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.515759, mae: 24.094681, mean_q: -34.997786, mean_eps: 0.050000\n",
      " 143151/200000: episode: 1061, duration: 0.142s, episode steps:  77, steps per second: 543, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.457126, mae: 23.895736, mean_q: -34.772730, mean_eps: 0.050000\n",
      " 143230/200000: episode: 1062, duration: 0.157s, episode steps:  79, steps per second: 504, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 0.587305, mae: 23.398061, mean_q: -34.015787, mean_eps: 0.050000\n",
      " 143293/200000: episode: 1063, duration: 0.124s, episode steps:  63, steps per second: 507, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.678040, mae: 23.436917, mean_q: -34.074688, mean_eps: 0.050000\n",
      " 143368/200000: episode: 1064, duration: 0.156s, episode steps:  75, steps per second: 482, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.391791, mae: 22.835760, mean_q: -33.108724, mean_eps: 0.050000\n",
      " 143431/200000: episode: 1065, duration: 0.128s, episode steps:  63, steps per second: 494, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.569168, mae: 23.641943, mean_q: -34.417039, mean_eps: 0.050000\n",
      " 143514/200000: episode: 1066, duration: 0.184s, episode steps:  83, steps per second: 452, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.871715, mae: 23.334976, mean_q: -33.933517, mean_eps: 0.050000\n",
      " 143585/200000: episode: 1067, duration: 0.145s, episode steps:  71, steps per second: 490, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.580349, mae: 23.157891, mean_q: -33.663496, mean_eps: 0.050000\n",
      " 143677/200000: episode: 1068, duration: 0.187s, episode steps:  92, steps per second: 492, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.496760, mae: 22.923419, mean_q: -33.340623, mean_eps: 0.050000\n",
      " 143767/200000: episode: 1069, duration: 0.178s, episode steps:  90, steps per second: 505, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.483243, mae: 23.888153, mean_q: -34.787258, mean_eps: 0.050000\n",
      " 143849/200000: episode: 1070, duration: 0.170s, episode steps:  82, steps per second: 482, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 0.479437, mae: 24.103454, mean_q: -35.099153, mean_eps: 0.050000\n",
      " 143931/200000: episode: 1071, duration: 0.164s, episode steps:  82, steps per second: 499, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.702344, mae: 23.112980, mean_q: -33.583724, mean_eps: 0.050000\n",
      " 144016/200000: episode: 1072, duration: 0.180s, episode steps:  85, steps per second: 473, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.188 [0.000, 2.000],  loss: 0.735059, mae: 23.125116, mean_q: -33.590347, mean_eps: 0.050000\n",
      " 144108/200000: episode: 1073, duration: 0.188s, episode steps:  92, steps per second: 489, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.513367, mae: 23.297082, mean_q: -33.849079, mean_eps: 0.050000\n",
      " 144206/200000: episode: 1074, duration: 0.194s, episode steps:  98, steps per second: 504, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.383677, mae: 23.731020, mean_q: -34.528988, mean_eps: 0.050000\n",
      " 144293/200000: episode: 1075, duration: 0.171s, episode steps:  87, steps per second: 510, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 1.527155, mae: 23.861470, mean_q: -34.611147, mean_eps: 0.050000\n",
      " 144383/200000: episode: 1076, duration: 0.180s, episode steps:  90, steps per second: 500, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.767 [0.000, 2.000],  loss: 0.681552, mae: 24.089371, mean_q: -35.025875, mean_eps: 0.050000\n",
      " 144494/200000: episode: 1077, duration: 0.224s, episode steps: 111, steps per second: 496, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.135 [0.000, 2.000],  loss: 0.573050, mae: 23.383234, mean_q: -33.976232, mean_eps: 0.050000\n",
      " 144585/200000: episode: 1078, duration: 0.193s, episode steps:  91, steps per second: 472, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 1.398071, mae: 23.430745, mean_q: -33.987764, mean_eps: 0.050000\n",
      " 144657/200000: episode: 1079, duration: 0.149s, episode steps:  72, steps per second: 485, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.568739, mae: 23.344877, mean_q: -33.920265, mean_eps: 0.050000\n",
      " 144732/200000: episode: 1080, duration: 0.147s, episode steps:  75, steps per second: 511, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.524387, mae: 23.470030, mean_q: -34.155055, mean_eps: 0.050000\n",
      " 144800/200000: episode: 1081, duration: 0.145s, episode steps:  68, steps per second: 469, episode reward: -67.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 1.392761, mae: 23.730791, mean_q: -34.504081, mean_eps: 0.050000\n",
      " 144871/200000: episode: 1082, duration: 0.142s, episode steps:  71, steps per second: 500, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.498915, mae: 24.532321, mean_q: -35.832959, mean_eps: 0.050000\n",
      " 144965/200000: episode: 1083, duration: 0.198s, episode steps:  94, steps per second: 475, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.438599, mae: 23.646044, mean_q: -34.411970, mean_eps: 0.050000\n",
      " 145107/200000: episode: 1084, duration: 0.290s, episode steps: 142, steps per second: 489, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.761 [0.000, 2.000],  loss: 0.404992, mae: 23.623429, mean_q: -34.377717, mean_eps: 0.050000\n",
      " 145189/200000: episode: 1085, duration: 0.161s, episode steps:  82, steps per second: 510, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.361951, mae: 24.300024, mean_q: -35.358249, mean_eps: 0.050000\n",
      " 145257/200000: episode: 1086, duration: 0.142s, episode steps:  68, steps per second: 479, episode reward: -67.000, mean reward: -0.985 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.602485, mae: 24.077303, mean_q: -35.026276, mean_eps: 0.050000\n",
      " 145332/200000: episode: 1087, duration: 0.154s, episode steps:  75, steps per second: 487, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.421631, mae: 23.511853, mean_q: -34.185631, mean_eps: 0.050000\n",
      " 145415/200000: episode: 1088, duration: 0.170s, episode steps:  83, steps per second: 489, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.579370, mae: 23.630245, mean_q: -34.371195, mean_eps: 0.050000\n",
      " 145485/200000: episode: 1089, duration: 0.158s, episode steps:  70, steps per second: 443, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.340563, mae: 23.117168, mean_q: -33.672926, mean_eps: 0.050000\n",
      " 145584/200000: episode: 1090, duration: 0.205s, episode steps:  99, steps per second: 483, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.152 [0.000, 2.000],  loss: 0.407104, mae: 23.229273, mean_q: -33.783162, mean_eps: 0.050000\n",
      " 145660/200000: episode: 1091, duration: 0.153s, episode steps:  76, steps per second: 495, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.448425, mae: 23.799742, mean_q: -34.575277, mean_eps: 0.050000\n",
      " 145741/200000: episode: 1092, duration: 0.153s, episode steps:  81, steps per second: 529, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 1.628857, mae: 23.541105, mean_q: -34.159074, mean_eps: 0.050000\n",
      " 145812/200000: episode: 1093, duration: 0.139s, episode steps:  71, steps per second: 511, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.775 [0.000, 2.000],  loss: 0.407905, mae: 23.512314, mean_q: -34.313683, mean_eps: 0.050000\n",
      " 145886/200000: episode: 1094, duration: 0.144s, episode steps:  74, steps per second: 513, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.464491, mae: 23.148849, mean_q: -33.745533, mean_eps: 0.050000\n",
      " 145979/200000: episode: 1095, duration: 0.188s, episode steps:  93, steps per second: 496, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.849 [0.000, 2.000],  loss: 0.361023, mae: 24.010292, mean_q: -35.019485, mean_eps: 0.050000\n",
      " 146056/200000: episode: 1096, duration: 0.158s, episode steps:  77, steps per second: 486, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.535943, mae: 23.747851, mean_q: -34.581159, mean_eps: 0.050000\n",
      " 146135/200000: episode: 1097, duration: 0.156s, episode steps:  79, steps per second: 507, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.497522, mae: 24.019582, mean_q: -34.972828, mean_eps: 0.050000\n",
      " 146199/200000: episode: 1098, duration: 0.126s, episode steps:  64, steps per second: 510, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 2.079880, mae: 23.618900, mean_q: -34.292392, mean_eps: 0.050000\n",
      " 146275/200000: episode: 1099, duration: 0.154s, episode steps:  76, steps per second: 494, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.497241, mae: 24.161730, mean_q: -35.150174, mean_eps: 0.050000\n",
      " 146350/200000: episode: 1100, duration: 0.145s, episode steps:  75, steps per second: 517, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.495633, mae: 23.982321, mean_q: -34.910620, mean_eps: 0.050000\n",
      " 146441/200000: episode: 1101, duration: 0.199s, episode steps:  91, steps per second: 458, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.441711, mae: 23.476045, mean_q: -34.185368, mean_eps: 0.050000\n",
      " 146504/200000: episode: 1102, duration: 0.147s, episode steps:  63, steps per second: 427, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.443464, mae: 23.508704, mean_q: -34.132227, mean_eps: 0.050000\n",
      " 146575/200000: episode: 1103, duration: 0.150s, episode steps:  71, steps per second: 475, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.427802, mae: 24.152483, mean_q: -35.175721, mean_eps: 0.050000\n",
      " 146647/200000: episode: 1104, duration: 0.150s, episode steps:  72, steps per second: 478, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.548923, mae: 23.702424, mean_q: -34.481769, mean_eps: 0.050000\n",
      " 146742/200000: episode: 1105, duration: 0.195s, episode steps:  95, steps per second: 487, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.370646, mae: 23.811234, mean_q: -34.710556, mean_eps: 0.050000\n",
      " 146825/200000: episode: 1106, duration: 0.174s, episode steps:  83, steps per second: 478, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.527532, mae: 23.812638, mean_q: -34.672062, mean_eps: 0.050000\n",
      " 146914/200000: episode: 1107, duration: 0.179s, episode steps:  89, steps per second: 497, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.455578, mae: 23.603168, mean_q: -34.317183, mean_eps: 0.050000\n",
      " 146995/200000: episode: 1108, duration: 0.167s, episode steps:  81, steps per second: 485, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.136 [0.000, 2.000],  loss: 0.355054, mae: 23.961069, mean_q: -34.885255, mean_eps: 0.050000\n",
      " 147085/200000: episode: 1109, duration: 0.180s, episode steps:  90, steps per second: 501, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 1.574574, mae: 23.340245, mean_q: -33.761593, mean_eps: 0.050000\n",
      " 147168/200000: episode: 1110, duration: 0.173s, episode steps:  83, steps per second: 479, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.500854, mae: 23.968900, mean_q: -34.815819, mean_eps: 0.050000\n",
      " 147328/200000: episode: 1111, duration: 0.333s, episode steps: 160, steps per second: 481, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 1.666614, mae: 23.899531, mean_q: -34.643705, mean_eps: 0.050000\n",
      " 147400/200000: episode: 1112, duration: 0.153s, episode steps:  72, steps per second: 472, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.345703, mae: 24.531068, mean_q: -35.717737, mean_eps: 0.050000\n",
      " 147473/200000: episode: 1113, duration: 0.155s, episode steps:  73, steps per second: 470, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.628857, mae: 23.562008, mean_q: -34.236762, mean_eps: 0.050000\n",
      " 147551/200000: episode: 1114, duration: 0.147s, episode steps:  78, steps per second: 529, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 0.503418, mae: 24.075858, mean_q: -35.004256, mean_eps: 0.050000\n",
      " 147638/200000: episode: 1115, duration: 0.181s, episode steps:  87, steps per second: 481, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.793 [0.000, 2.000],  loss: 0.592973, mae: 24.289950, mean_q: -35.281824, mean_eps: 0.050000\n",
      " 147716/200000: episode: 1116, duration: 0.160s, episode steps:  78, steps per second: 486, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.479529, mae: 24.153999, mean_q: -35.137341, mean_eps: 0.050000\n",
      " 147809/200000: episode: 1117, duration: 0.194s, episode steps:  93, steps per second: 480, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.951274, mae: 23.370529, mean_q: -33.960615, mean_eps: 0.050000\n",
      " 147886/200000: episode: 1118, duration: 0.151s, episode steps:  77, steps per second: 509, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 1.328830, mae: 24.139520, mean_q: -35.052193, mean_eps: 0.050000\n",
      " 147957/200000: episode: 1119, duration: 0.153s, episode steps:  71, steps per second: 464, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.484904, mae: 24.515973, mean_q: -35.711896, mean_eps: 0.050000\n",
      " 148039/200000: episode: 1120, duration: 0.165s, episode steps:  82, steps per second: 497, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 0.492224, mae: 24.049866, mean_q: -34.990388, mean_eps: 0.050000\n",
      " 148125/200000: episode: 1121, duration: 0.175s, episode steps:  86, steps per second: 490, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.709 [0.000, 2.000],  loss: 0.953458, mae: 23.434759, mean_q: -34.007110, mean_eps: 0.050000\n",
      " 148218/200000: episode: 1122, duration: 0.183s, episode steps:  93, steps per second: 508, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 0.645752, mae: 23.238407, mean_q: -33.754431, mean_eps: 0.050000\n",
      " 148313/200000: episode: 1123, duration: 0.189s, episode steps:  95, steps per second: 502, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.821 [0.000, 2.000],  loss: 0.512339, mae: 23.674994, mean_q: -34.429034, mean_eps: 0.050000\n",
      " 148389/200000: episode: 1124, duration: 0.145s, episode steps:  76, steps per second: 523, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.542250, mae: 23.816209, mean_q: -34.612245, mean_eps: 0.050000\n",
      " 148479/200000: episode: 1125, duration: 0.185s, episode steps:  90, steps per second: 486, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.630704, mae: 23.358583, mean_q: -33.877004, mean_eps: 0.050000\n",
      " 148563/200000: episode: 1126, duration: 0.181s, episode steps:  84, steps per second: 464, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.509877, mae: 23.228727, mean_q: -33.751206, mean_eps: 0.050000\n",
      " 148640/200000: episode: 1127, duration: 0.163s, episode steps:  77, steps per second: 473, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.655219, mae: 23.045065, mean_q: -33.467199, mean_eps: 0.050000\n",
      " 148766/200000: episode: 1128, duration: 0.270s, episode steps: 126, steps per second: 467, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.841 [0.000, 2.000],  loss: 0.477394, mae: 23.573840, mean_q: -34.286920, mean_eps: 0.050000\n",
      " 148844/200000: episode: 1129, duration: 0.168s, episode steps:  78, steps per second: 464, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.538733, mae: 22.382736, mean_q: -32.473231, mean_eps: 0.050000\n",
      " 148937/200000: episode: 1130, duration: 0.199s, episode steps:  93, steps per second: 467, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.796 [0.000, 2.000],  loss: 0.384399, mae: 23.517534, mean_q: -34.243237, mean_eps: 0.050000\n",
      " 149029/200000: episode: 1131, duration: 0.189s, episode steps:  92, steps per second: 487, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.432728, mae: 23.140091, mean_q: -33.631358, mean_eps: 0.050000\n",
      " 149111/200000: episode: 1132, duration: 0.171s, episode steps:  82, steps per second: 479, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.509473, mae: 23.111054, mean_q: -33.535321, mean_eps: 0.050000\n",
      " 149182/200000: episode: 1133, duration: 0.148s, episode steps:  71, steps per second: 481, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.506958, mae: 23.121358, mean_q: -33.629067, mean_eps: 0.050000\n",
      " 149269/200000: episode: 1134, duration: 0.183s, episode steps:  87, steps per second: 476, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.485218, mae: 22.662773, mean_q: -32.862655, mean_eps: 0.050000\n",
      " 149400/200000: episode: 1135, duration: 0.271s, episode steps: 131, steps per second: 483, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.530739, mae: 22.997735, mean_q: -33.447916, mean_eps: 0.050000\n",
      " 149476/200000: episode: 1136, duration: 0.154s, episode steps:  76, steps per second: 495, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.524731, mae: 23.068584, mean_q: -33.558632, mean_eps: 0.050000\n",
      " 149548/200000: episode: 1137, duration: 0.153s, episode steps:  72, steps per second: 469, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.530165, mae: 23.392937, mean_q: -34.028342, mean_eps: 0.050000\n",
      " 149633/200000: episode: 1138, duration: 0.175s, episode steps:  85, steps per second: 485, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.512396, mae: 23.359905, mean_q: -33.961944, mean_eps: 0.050000\n",
      " 149702/200000: episode: 1139, duration: 0.136s, episode steps:  69, steps per second: 507, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.443237, mae: 23.316901, mean_q: -33.894242, mean_eps: 0.050000\n",
      " 149779/200000: episode: 1140, duration: 0.166s, episode steps:  77, steps per second: 465, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 0.407593, mae: 24.417078, mean_q: -35.556934, mean_eps: 0.050000\n",
      " 149874/200000: episode: 1141, duration: 0.203s, episode steps:  95, steps per second: 467, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.242 [0.000, 2.000],  loss: 1.033264, mae: 23.167134, mean_q: -33.602386, mean_eps: 0.050000\n",
      " 149957/200000: episode: 1142, duration: 0.166s, episode steps:  83, steps per second: 501, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.587622, mae: 23.098602, mean_q: -33.555694, mean_eps: 0.050000\n",
      " 150052/200000: episode: 1143, duration: 0.202s, episode steps:  95, steps per second: 471, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.505127, mae: 22.852648, mean_q: -33.145561, mean_eps: 0.050000\n",
      " 150151/200000: episode: 1144, duration: 0.196s, episode steps:  99, steps per second: 504, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.511190, mae: 23.148259, mean_q: -33.646295, mean_eps: 0.050000\n",
      " 150266/200000: episode: 1145, duration: 0.248s, episode steps: 115, steps per second: 464, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.424626, mae: 23.846533, mean_q: -34.713545, mean_eps: 0.050000\n",
      " 150359/200000: episode: 1146, duration: 0.184s, episode steps:  93, steps per second: 505, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.237 [0.000, 2.000],  loss: 0.446977, mae: 23.386823, mean_q: -34.022189, mean_eps: 0.050000\n",
      " 150459/200000: episode: 1147, duration: 0.213s, episode steps: 100, steps per second: 469, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.446796, mae: 23.327821, mean_q: -33.947238, mean_eps: 0.050000\n",
      " 150533/200000: episode: 1148, duration: 0.150s, episode steps:  74, steps per second: 492, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.532864, mae: 23.800506, mean_q: -34.676628, mean_eps: 0.050000\n",
      " 150611/200000: episode: 1149, duration: 0.165s, episode steps:  78, steps per second: 472, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.476709, mae: 23.772268, mean_q: -34.614140, mean_eps: 0.050000\n",
      " 150716/200000: episode: 1150, duration: 0.218s, episode steps: 105, steps per second: 483, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 1.078200, mae: 22.897239, mean_q: -33.220335, mean_eps: 0.050000\n",
      " 150809/200000: episode: 1151, duration: 0.201s, episode steps:  93, steps per second: 464, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.445048, mae: 23.141061, mean_q: -33.667086, mean_eps: 0.050000\n",
      " 150901/200000: episode: 1152, duration: 0.187s, episode steps:  92, steps per second: 492, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 1.727872, mae: 23.528952, mean_q: -34.168496, mean_eps: 0.050000\n",
      " 150984/200000: episode: 1153, duration: 0.181s, episode steps:  83, steps per second: 460, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.632617, mae: 23.464720, mean_q: -34.102508, mean_eps: 0.050000\n",
      " 151065/200000: episode: 1154, duration: 0.185s, episode steps:  81, steps per second: 437, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.667547, mae: 22.935062, mean_q: -33.263162, mean_eps: 0.050000\n",
      " 151148/200000: episode: 1155, duration: 0.167s, episode steps:  83, steps per second: 498, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.391479, mae: 23.430638, mean_q: -34.049813, mean_eps: 0.050000\n",
      " 151234/200000: episode: 1156, duration: 0.185s, episode steps:  86, steps per second: 465, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.431097, mae: 23.323707, mean_q: -33.934115, mean_eps: 0.050000\n",
      " 151296/200000: episode: 1157, duration: 0.130s, episode steps:  62, steps per second: 475, episode reward: -61.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.523647, mae: 23.243106, mean_q: -33.795922, mean_eps: 0.050000\n",
      " 151371/200000: episode: 1158, duration: 0.158s, episode steps:  75, steps per second: 475, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.776685, mae: 22.788623, mean_q: -33.136322, mean_eps: 0.050000\n",
      " 151448/200000: episode: 1159, duration: 0.159s, episode steps:  77, steps per second: 485, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.586263, mae: 22.499295, mean_q: -32.646388, mean_eps: 0.050000\n",
      " 151521/200000: episode: 1160, duration: 0.156s, episode steps:  73, steps per second: 469, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.577698, mae: 23.965311, mean_q: -34.901136, mean_eps: 0.050000\n",
      " 151602/200000: episode: 1161, duration: 0.162s, episode steps:  81, steps per second: 501, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 1.743945, mae: 23.396677, mean_q: -33.986156, mean_eps: 0.050000\n",
      " 151771/200000: episode: 1162, duration: 0.347s, episode steps: 169, steps per second: 488, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.385 [0.000, 2.000],  loss: 0.508504, mae: 23.328416, mean_q: -33.943878, mean_eps: 0.050000\n",
      " 151863/200000: episode: 1163, duration: 0.174s, episode steps:  92, steps per second: 530, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.593217, mae: 22.933029, mean_q: -33.380560, mean_eps: 0.050000\n",
      " 151940/200000: episode: 1164, duration: 0.166s, episode steps:  77, steps per second: 464, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.534814, mae: 23.450556, mean_q: -34.107700, mean_eps: 0.050000\n",
      " 152039/200000: episode: 1165, duration: 0.194s, episode steps:  99, steps per second: 510, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 1.482381, mae: 22.874513, mean_q: -33.176125, mean_eps: 0.050000\n",
      " 152109/200000: episode: 1166, duration: 0.142s, episode steps:  70, steps per second: 492, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.704942, mae: 22.966037, mean_q: -33.318492, mean_eps: 0.050000\n",
      " 152191/200000: episode: 1167, duration: 0.161s, episode steps:  82, steps per second: 509, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.577148, mae: 23.104084, mean_q: -33.529925, mean_eps: 0.050000\n",
      " 152265/200000: episode: 1168, duration: 0.155s, episode steps:  74, steps per second: 478, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 2.071240, mae: 23.263815, mean_q: -33.704319, mean_eps: 0.050000\n",
      " 152354/200000: episode: 1169, duration: 0.180s, episode steps:  89, steps per second: 494, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 2.342352, mae: 23.536395, mean_q: -34.177238, mean_eps: 0.050000\n",
      " 152454/200000: episode: 1170, duration: 0.195s, episode steps: 100, steps per second: 514, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.150 [0.000, 2.000],  loss: 0.681030, mae: 22.930805, mean_q: -33.231293, mean_eps: 0.050000\n",
      " 152562/200000: episode: 1171, duration: 0.228s, episode steps: 108, steps per second: 475, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.451399, mae: 23.204955, mean_q: -33.706033, mean_eps: 0.050000\n",
      " 152644/200000: episode: 1172, duration: 0.161s, episode steps:  82, steps per second: 511, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.534473, mae: 23.016671, mean_q: -33.431793, mean_eps: 0.050000\n",
      " 152738/200000: episode: 1173, duration: 0.195s, episode steps:  94, steps per second: 483, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.539876, mae: 22.964387, mean_q: -33.403441, mean_eps: 0.050000\n",
      " 152813/200000: episode: 1174, duration: 0.141s, episode steps:  75, steps per second: 532, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.476752, mae: 23.301320, mean_q: -33.896797, mean_eps: 0.050000\n",
      " 152899/200000: episode: 1175, duration: 0.183s, episode steps:  86, steps per second: 469, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 1.366610, mae: 23.573461, mean_q: -34.211026, mean_eps: 0.050000\n",
      " 152983/200000: episode: 1176, duration: 0.164s, episode steps:  84, steps per second: 513, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.514722, mae: 23.084567, mean_q: -33.502210, mean_eps: 0.050000\n",
      " 153076/200000: episode: 1177, duration: 0.204s, episode steps:  93, steps per second: 456, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 0.562581, mae: 23.198206, mean_q: -33.652318, mean_eps: 0.050000\n",
      " 153167/200000: episode: 1178, duration: 0.179s, episode steps:  91, steps per second: 507, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.571777, mae: 23.258948, mean_q: -33.896523, mean_eps: 0.050000\n",
      " 153237/200000: episode: 1179, duration: 0.148s, episode steps:  70, steps per second: 471, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.814 [0.000, 2.000],  loss: 0.484660, mae: 23.257384, mean_q: -33.888505, mean_eps: 0.050000\n",
      " 153323/200000: episode: 1180, duration: 0.178s, episode steps:  86, steps per second: 483, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.802 [0.000, 2.000],  loss: 0.528564, mae: 23.099749, mean_q: -33.675992, mean_eps: 0.050000\n",
      " 153420/200000: episode: 1181, duration: 0.199s, episode steps:  97, steps per second: 488, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.528198, mae: 23.628212, mean_q: -34.414794, mean_eps: 0.050000\n",
      " 153557/200000: episode: 1182, duration: 0.289s, episode steps: 137, steps per second: 475, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.803 [0.000, 2.000],  loss: 0.484447, mae: 23.297460, mean_q: -33.934155, mean_eps: 0.050000\n",
      " 153641/200000: episode: 1183, duration: 0.177s, episode steps:  84, steps per second: 474, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 2.182662, mae: 23.099888, mean_q: -33.497608, mean_eps: 0.050000\n",
      " 153734/200000: episode: 1184, duration: 0.185s, episode steps:  93, steps per second: 504, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.281605, mae: 23.050619, mean_q: -33.415069, mean_eps: 0.050000\n",
      " 153811/200000: episode: 1185, duration: 0.166s, episode steps:  77, steps per second: 464, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.729639, mae: 23.046554, mean_q: -33.537895, mean_eps: 0.050000\n",
      " 153894/200000: episode: 1186, duration: 0.173s, episode steps:  83, steps per second: 480, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.795 [0.000, 2.000],  loss: 0.463806, mae: 23.514419, mean_q: -34.181791, mean_eps: 0.050000\n",
      " 153968/200000: episode: 1187, duration: 0.158s, episode steps:  74, steps per second: 468, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.838 [0.000, 2.000],  loss: 1.705241, mae: 23.488304, mean_q: -34.165230, mean_eps: 0.050000\n",
      " 154063/200000: episode: 1188, duration: 0.204s, episode steps:  95, steps per second: 466, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.562805, mae: 23.667853, mean_q: -34.393641, mean_eps: 0.050000\n",
      " 154140/200000: episode: 1189, duration: 0.165s, episode steps:  77, steps per second: 467, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.604565, mae: 23.176059, mean_q: -33.622688, mean_eps: 0.050000\n",
      " 154204/200000: episode: 1190, duration: 0.133s, episode steps:  64, steps per second: 481, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.774994, mae: 23.569737, mean_q: -34.314893, mean_eps: 0.050000\n",
      " 154282/200000: episode: 1191, duration: 0.165s, episode steps:  78, steps per second: 472, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.655713, mae: 23.135005, mean_q: -33.580285, mean_eps: 0.050000\n",
      " 154419/200000: episode: 1192, duration: 0.301s, episode steps: 137, steps per second: 455, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.788 [0.000, 2.000],  loss: 0.681382, mae: 23.135200, mean_q: -33.647200, mean_eps: 0.050000\n",
      " 154531/200000: episode: 1193, duration: 0.242s, episode steps: 112, steps per second: 464, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.777 [0.000, 2.000],  loss: 0.556100, mae: 23.596415, mean_q: -34.346349, mean_eps: 0.050000\n",
      " 154618/200000: episode: 1194, duration: 0.181s, episode steps:  87, steps per second: 480, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.558616, mae: 23.919147, mean_q: -34.812261, mean_eps: 0.050000\n",
      " 154693/200000: episode: 1195, duration: 0.153s, episode steps:  75, steps per second: 490, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.726223, mae: 23.532412, mean_q: -34.235466, mean_eps: 0.050000\n",
      " 154786/200000: episode: 1196, duration: 0.201s, episode steps:  93, steps per second: 462, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.601013, mae: 23.636049, mean_q: -34.436176, mean_eps: 0.050000\n",
      " 154850/200000: episode: 1197, duration: 0.138s, episode steps:  64, steps per second: 464, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.434357, mae: 23.133934, mean_q: -33.689565, mean_eps: 0.050000\n",
      " 154933/200000: episode: 1198, duration: 0.171s, episode steps:  83, steps per second: 485, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.795 [0.000, 2.000],  loss: 0.715869, mae: 23.341055, mean_q: -33.974112, mean_eps: 0.050000\n",
      " 155015/200000: episode: 1199, duration: 0.173s, episode steps:  82, steps per second: 474, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 0.393433, mae: 23.184645, mean_q: -33.763730, mean_eps: 0.050000\n",
      " 155079/200000: episode: 1200, duration: 0.137s, episode steps:  64, steps per second: 467, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.769287, mae: 23.073802, mean_q: -33.454741, mean_eps: 0.050000\n",
      " 155156/200000: episode: 1201, duration: 0.164s, episode steps:  77, steps per second: 468, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.801660, mae: 22.959848, mean_q: -33.260255, mean_eps: 0.050000\n",
      " 155228/200000: episode: 1202, duration: 0.156s, episode steps:  72, steps per second: 462, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.490451, mae: 23.442148, mean_q: -33.957694, mean_eps: 0.050000\n",
      " 155303/200000: episode: 1203, duration: 0.151s, episode steps:  75, steps per second: 497, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.769640, mae: 23.299135, mean_q: -33.842842, mean_eps: 0.050000\n",
      " 155382/200000: episode: 1204, duration: 0.164s, episode steps:  79, steps per second: 482, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.661963, mae: 23.092307, mean_q: -33.497518, mean_eps: 0.050000\n",
      " 155474/200000: episode: 1205, duration: 0.194s, episode steps:  92, steps per second: 473, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 2.032898, mae: 23.489809, mean_q: -34.065768, mean_eps: 0.050000\n",
      " 155546/200000: episode: 1206, duration: 0.156s, episode steps:  72, steps per second: 462, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.529595, mae: 23.691928, mean_q: -34.439811, mean_eps: 0.050000\n",
      " 155655/200000: episode: 1207, duration: 0.212s, episode steps: 109, steps per second: 513, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.752 [0.000, 2.000],  loss: 0.569261, mae: 23.033929, mean_q: -33.504199, mean_eps: 0.050000\n",
      " 155726/200000: episode: 1208, duration: 0.150s, episode steps:  71, steps per second: 473, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 0.492052, mae: 23.570207, mean_q: -34.323022, mean_eps: 0.050000\n",
      " 155798/200000: episode: 1209, duration: 0.168s, episode steps:  72, steps per second: 427, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.469482, mae: 23.713270, mean_q: -34.517033, mean_eps: 0.050000\n",
      " 155875/200000: episode: 1210, duration: 0.158s, episode steps:  77, steps per second: 487, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.576746, mae: 23.831056, mean_q: -34.634153, mean_eps: 0.050000\n",
      " 155958/200000: episode: 1211, duration: 0.166s, episode steps:  83, steps per second: 499, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.602002, mae: 23.593683, mean_q: -34.254302, mean_eps: 0.050000\n",
      " 156058/200000: episode: 1212, duration: 0.216s, episode steps: 100, steps per second: 463, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.790 [0.000, 2.000],  loss: 0.624061, mae: 23.416763, mean_q: -33.975712, mean_eps: 0.050000\n",
      " 156148/200000: episode: 1213, duration: 0.182s, episode steps:  90, steps per second: 494, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.770841, mae: 23.454070, mean_q: -34.086934, mean_eps: 0.050000\n",
      " 156224/200000: episode: 1214, duration: 0.169s, episode steps:  76, steps per second: 451, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.497070, mae: 23.807976, mean_q: -34.642317, mean_eps: 0.050000\n",
      " 156309/200000: episode: 1215, duration: 0.181s, episode steps:  85, steps per second: 470, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.675093, mae: 23.024155, mean_q: -33.439189, mean_eps: 0.050000\n",
      " 156388/200000: episode: 1216, duration: 0.168s, episode steps:  79, steps per second: 469, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.637476, mae: 23.030666, mean_q: -33.436335, mean_eps: 0.050000\n",
      " 156513/200000: episode: 1217, duration: 0.272s, episode steps: 125, steps per second: 460, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.712 [0.000, 2.000],  loss: 0.522400, mae: 23.177409, mean_q: -33.644935, mean_eps: 0.050000\n",
      " 156595/200000: episode: 1218, duration: 0.165s, episode steps:  82, steps per second: 496, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.568921, mae: 22.886235, mean_q: -33.186303, mean_eps: 0.050000\n",
      " 156681/200000: episode: 1219, duration: 0.193s, episode steps:  86, steps per second: 447, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.525746, mae: 23.517753, mean_q: -34.233306, mean_eps: 0.050000\n",
      " 156768/200000: episode: 1220, duration: 0.181s, episode steps:  87, steps per second: 480, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 1.430347, mae: 22.963762, mean_q: -33.288445, mean_eps: 0.050000\n",
      " 156844/200000: episode: 1221, duration: 0.163s, episode steps:  76, steps per second: 466, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.237 [0.000, 2.000],  loss: 0.559180, mae: 23.799216, mean_q: -34.644322, mean_eps: 0.050000\n",
      " 156921/200000: episode: 1222, duration: 0.156s, episode steps:  77, steps per second: 495, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.602734, mae: 22.969356, mean_q: -33.400285, mean_eps: 0.050000\n",
      " 157007/200000: episode: 1223, duration: 0.169s, episode steps:  86, steps per second: 508, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 0.946289, mae: 23.612968, mean_q: -34.354308, mean_eps: 0.050000\n",
      " 157089/200000: episode: 1224, duration: 0.174s, episode steps:  82, steps per second: 471, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.650391, mae: 22.776184, mean_q: -33.039822, mean_eps: 0.050000\n",
      " 157160/200000: episode: 1225, duration: 0.145s, episode steps:  71, steps per second: 491, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.911987, mae: 23.196737, mean_q: -33.585353, mean_eps: 0.050000\n",
      " 157243/200000: episode: 1226, duration: 0.170s, episode steps:  83, steps per second: 488, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 1.835028, mae: 23.183147, mean_q: -33.602556, mean_eps: 0.050000\n",
      " 157326/200000: episode: 1227, duration: 0.171s, episode steps:  83, steps per second: 484, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.647510, mae: 23.449828, mean_q: -34.037281, mean_eps: 0.050000\n",
      " 157439/200000: episode: 1228, duration: 0.239s, episode steps: 113, steps per second: 473, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.363 [0.000, 2.000],  loss: 0.602382, mae: 22.958017, mean_q: -33.347089, mean_eps: 0.050000\n",
      " 157511/200000: episode: 1229, duration: 0.152s, episode steps:  72, steps per second: 474, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.643582, mae: 22.805074, mean_q: -33.126214, mean_eps: 0.050000\n",
      " 157629/200000: episode: 1230, duration: 0.246s, episode steps: 118, steps per second: 479, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.195 [0.000, 2.000],  loss: 0.677165, mae: 23.363000, mean_q: -33.875533, mean_eps: 0.050000\n",
      " 157705/200000: episode: 1231, duration: 0.162s, episode steps:  76, steps per second: 468, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 0.634204, mae: 23.161679, mean_q: -33.617964, mean_eps: 0.050000\n",
      " 157797/200000: episode: 1232, duration: 0.190s, episode steps:  92, steps per second: 484, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.659657, mae: 23.498101, mean_q: -34.123307, mean_eps: 0.050000\n",
      " 157889/200000: episode: 1233, duration: 0.199s, episode steps:  92, steps per second: 462, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.700073, mae: 23.882118, mean_q: -34.779299, mean_eps: 0.050000\n",
      " 157975/200000: episode: 1234, duration: 0.182s, episode steps:  86, steps per second: 471, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.174 [0.000, 2.000],  loss: 0.569202, mae: 22.811134, mean_q: -33.096394, mean_eps: 0.050000\n",
      " 158065/200000: episode: 1235, duration: 0.200s, episode steps:  90, steps per second: 450, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.492177, mae: 23.638628, mean_q: -34.355820, mean_eps: 0.050000\n",
      " 158184/200000: episode: 1236, duration: 0.249s, episode steps: 119, steps per second: 479, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 2.230539, mae: 23.836743, mean_q: -34.517612, mean_eps: 0.050000\n",
      " 158265/200000: episode: 1237, duration: 0.176s, episode steps:  81, steps per second: 461, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.448753, mae: 23.881967, mean_q: -34.687366, mean_eps: 0.050000\n",
      " 158360/200000: episode: 1238, duration: 0.199s, episode steps:  95, steps per second: 477, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 0.550870, mae: 23.586994, mean_q: -34.234272, mean_eps: 0.050000\n",
      " 158437/200000: episode: 1239, duration: 0.161s, episode steps:  77, steps per second: 478, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.660144, mae: 23.027650, mean_q: -33.390477, mean_eps: 0.050000\n",
      " 158526/200000: episode: 1240, duration: 0.185s, episode steps:  89, steps per second: 481, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.180 [0.000, 2.000],  loss: 0.476496, mae: 24.023928, mean_q: -34.962603, mean_eps: 0.050000\n",
      " 158599/200000: episode: 1241, duration: 0.152s, episode steps:  73, steps per second: 480, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 1.055366, mae: 23.519912, mean_q: -34.059046, mean_eps: 0.050000\n",
      " 158686/200000: episode: 1242, duration: 0.183s, episode steps:  87, steps per second: 475, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.533225, mae: 24.135571, mean_q: -35.040507, mean_eps: 0.050000\n",
      " 158794/200000: episode: 1243, duration: 0.232s, episode steps: 108, steps per second: 466, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.649257, mae: 23.687996, mean_q: -34.372021, mean_eps: 0.050000\n",
      " 158869/200000: episode: 1244, duration: 0.157s, episode steps:  75, steps per second: 478, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.672255, mae: 23.042552, mean_q: -33.363900, mean_eps: 0.050000\n",
      " 158944/200000: episode: 1245, duration: 0.163s, episode steps:  75, steps per second: 461, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.709310, mae: 23.165397, mean_q: -33.619568, mean_eps: 0.050000\n",
      " 159032/200000: episode: 1246, duration: 0.197s, episode steps:  88, steps per second: 446, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.722767, mae: 23.532446, mean_q: -34.227008, mean_eps: 0.050000\n",
      " 159150/200000: episode: 1247, duration: 0.252s, episode steps: 118, steps per second: 467, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.627 [0.000, 2.000],  loss: 0.609782, mae: 23.740687, mean_q: -34.525095, mean_eps: 0.050000\n",
      " 159240/200000: episode: 1248, duration: 0.197s, episode steps:  90, steps per second: 457, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.919767, mae: 23.384723, mean_q: -33.890907, mean_eps: 0.050000\n",
      " 159318/200000: episode: 1249, duration: 0.170s, episode steps:  78, steps per second: 460, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.936768, mae: 23.339979, mean_q: -33.886881, mean_eps: 0.050000\n",
      " 159395/200000: episode: 1250, duration: 0.161s, episode steps:  77, steps per second: 478, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.894800, mae: 22.973714, mean_q: -33.295972, mean_eps: 0.050000\n",
      " 159471/200000: episode: 1251, duration: 0.154s, episode steps:  76, steps per second: 492, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.578125, mae: 23.466355, mean_q: -34.097469, mean_eps: 0.050000\n",
      " 159557/200000: episode: 1252, duration: 0.182s, episode steps:  86, steps per second: 473, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 1.414329, mae: 23.178019, mean_q: -33.587890, mean_eps: 0.050000\n",
      " 159640/200000: episode: 1253, duration: 0.188s, episode steps:  83, steps per second: 442, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.856104, mae: 22.892362, mean_q: -33.238248, mean_eps: 0.050000\n",
      " 159731/200000: episode: 1254, duration: 0.198s, episode steps:  91, steps per second: 460, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.646118, mae: 23.994440, mean_q: -34.979786, mean_eps: 0.050000\n",
      " 159809/200000: episode: 1255, duration: 0.174s, episode steps:  78, steps per second: 449, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 0.697485, mae: 23.392517, mean_q: -34.016967, mean_eps: 0.050000\n",
      " 159900/200000: episode: 1256, duration: 0.183s, episode steps:  91, steps per second: 498, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 0.605657, mae: 23.064220, mean_q: -33.545194, mean_eps: 0.050000\n",
      " 159988/200000: episode: 1257, duration: 0.183s, episode steps:  88, steps per second: 481, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.818 [0.000, 2.000],  loss: 0.408225, mae: 23.412315, mean_q: -34.133893, mean_eps: 0.050000\n",
      " 160080/200000: episode: 1258, duration: 0.206s, episode steps:  92, steps per second: 447, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.829901, mae: 23.697153, mean_q: -34.462373, mean_eps: 0.050000\n",
      " 160194/200000: episode: 1259, duration: 0.247s, episode steps: 114, steps per second: 462, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.737630, mae: 23.490776, mean_q: -34.081983, mean_eps: 0.050000\n",
      " 160286/200000: episode: 1260, duration: 0.185s, episode steps:  92, steps per second: 497, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.624490, mae: 23.915842, mean_q: -34.737104, mean_eps: 0.050000\n",
      " 160380/200000: episode: 1261, duration: 0.200s, episode steps:  94, steps per second: 471, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.477437, mae: 23.040412, mean_q: -33.439223, mean_eps: 0.050000\n",
      " 160443/200000: episode: 1262, duration: 0.135s, episode steps:  63, steps per second: 465, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.442062, mae: 23.609865, mean_q: -34.285167, mean_eps: 0.050000\n",
      " 160541/200000: episode: 1263, duration: 0.207s, episode steps:  98, steps per second: 474, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.715678, mae: 23.368112, mean_q: -33.918025, mean_eps: 0.050000\n",
      " 160635/200000: episode: 1264, duration: 0.201s, episode steps:  94, steps per second: 468, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.946513, mae: 22.931278, mean_q: -33.204210, mean_eps: 0.050000\n",
      " 160727/200000: episode: 1265, duration: 0.193s, episode steps:  92, steps per second: 478, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.740656, mae: 22.709488, mean_q: -32.880778, mean_eps: 0.050000\n",
      " 160846/200000: episode: 1266, duration: 0.248s, episode steps: 119, steps per second: 480, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.630 [0.000, 2.000],  loss: 0.716276, mae: 22.895964, mean_q: -33.187902, mean_eps: 0.050000\n",
      " 160922/200000: episode: 1267, duration: 0.163s, episode steps:  76, steps per second: 467, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.709912, mae: 23.394702, mean_q: -33.973462, mean_eps: 0.050000\n",
      " 161010/200000: episode: 1268, duration: 0.185s, episode steps:  88, steps per second: 475, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.467141, mae: 23.637706, mean_q: -34.369225, mean_eps: 0.050000\n",
      " 161110/200000: episode: 1269, duration: 0.198s, episode steps: 100, steps per second: 504, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.700 [0.000, 2.000],  loss: 2.299967, mae: 23.229484, mean_q: -33.639745, mean_eps: 0.050000\n",
      " 161234/200000: episode: 1270, duration: 0.263s, episode steps: 124, steps per second: 471, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.629 [0.000, 2.000],  loss: 0.767151, mae: 23.218379, mean_q: -33.722265, mean_eps: 0.050000\n",
      " 161336/200000: episode: 1271, duration: 0.217s, episode steps: 102, steps per second: 470, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.716 [0.000, 2.000],  loss: 1.949656, mae: 23.445912, mean_q: -34.083287, mean_eps: 0.050000\n",
      " 161543/200000: episode: 1272, duration: 0.455s, episode steps: 207, steps per second: 454, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.459 [0.000, 2.000],  loss: 0.612934, mae: 23.453808, mean_q: -34.103914, mean_eps: 0.050000\n",
      " 161632/200000: episode: 1273, duration: 0.206s, episode steps:  89, steps per second: 433, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.831 [0.000, 2.000],  loss: 0.707919, mae: 23.014431, mean_q: -33.449250, mean_eps: 0.050000\n",
      " 161704/200000: episode: 1274, duration: 0.169s, episode steps:  72, steps per second: 427, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.622097, mae: 23.527648, mean_q: -34.249164, mean_eps: 0.050000\n",
      " 161768/200000: episode: 1275, duration: 0.148s, episode steps:  64, steps per second: 432, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.608032, mae: 24.421297, mean_q: -35.536221, mean_eps: 0.050000\n",
      " 161846/200000: episode: 1276, duration: 0.163s, episode steps:  78, steps per second: 479, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.722681, mae: 23.381064, mean_q: -33.913663, mean_eps: 0.050000\n",
      " 161921/200000: episode: 1277, duration: 0.169s, episode steps:  75, steps per second: 443, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 1.286182, mae: 23.585974, mean_q: -34.177512, mean_eps: 0.050000\n",
      " 161996/200000: episode: 1278, duration: 0.150s, episode steps:  75, steps per second: 499, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.742405, mae: 23.598863, mean_q: -34.284226, mean_eps: 0.050000\n",
      " 162094/200000: episode: 1279, duration: 0.205s, episode steps:  98, steps per second: 477, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.153 [0.000, 2.000],  loss: 0.556427, mae: 23.365233, mean_q: -33.911711, mean_eps: 0.050000\n",
      " 162200/200000: episode: 1280, duration: 0.228s, episode steps: 106, steps per second: 466, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.736 [0.000, 2.000],  loss: 0.490328, mae: 23.171570, mean_q: -33.706781, mean_eps: 0.050000\n",
      " 162299/200000: episode: 1281, duration: 0.211s, episode steps:  99, steps per second: 470, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 0.474647, mae: 23.584775, mean_q: -34.318699, mean_eps: 0.050000\n",
      " 162369/200000: episode: 1282, duration: 0.147s, episode steps:  70, steps per second: 475, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 1.756022, mae: 23.019478, mean_q: -33.375039, mean_eps: 0.050000\n",
      " 162507/200000: episode: 1283, duration: 0.287s, episode steps: 138, steps per second: 481, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.826 [0.000, 2.000],  loss: 0.657255, mae: 23.433063, mean_q: -33.984773, mean_eps: 0.050000\n",
      " 162584/200000: episode: 1284, duration: 0.163s, episode steps:  77, steps per second: 471, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 0.777479, mae: 23.265951, mean_q: -33.749316, mean_eps: 0.050000\n",
      " 162657/200000: episode: 1285, duration: 0.157s, episode steps:  73, steps per second: 465, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 0.807837, mae: 23.174165, mean_q: -33.621347, mean_eps: 0.050000\n",
      " 162735/200000: episode: 1286, duration: 0.148s, episode steps:  78, steps per second: 526, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 0.838433, mae: 23.072570, mean_q: -33.438508, mean_eps: 0.050000\n",
      " 162830/200000: episode: 1287, duration: 0.195s, episode steps:  95, steps per second: 488, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.593648, mae: 23.291033, mean_q: -33.846558, mean_eps: 0.050000\n",
      " 162914/200000: episode: 1288, duration: 0.172s, episode steps:  84, steps per second: 489, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.620805, mae: 23.419038, mean_q: -33.991308, mean_eps: 0.050000\n",
      " 162997/200000: episode: 1289, duration: 0.164s, episode steps:  83, steps per second: 505, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.589966, mae: 23.551857, mean_q: -34.237960, mean_eps: 0.050000\n",
      " 163081/200000: episode: 1290, duration: 0.178s, episode steps:  84, steps per second: 472, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.628551, mae: 22.642246, mean_q: -32.854931, mean_eps: 0.050000\n",
      " 163163/200000: episode: 1291, duration: 0.168s, episode steps:  82, steps per second: 487, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.569006, mae: 23.694824, mean_q: -34.481958, mean_eps: 0.050000\n",
      " 163244/200000: episode: 1292, duration: 0.171s, episode steps:  81, steps per second: 472, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.539844, mae: 23.166377, mean_q: -33.664789, mean_eps: 0.050000\n",
      " 163316/200000: episode: 1293, duration: 0.151s, episode steps:  72, steps per second: 477, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.778537, mae: 23.178745, mean_q: -33.686122, mean_eps: 0.050000\n",
      " 163410/200000: episode: 1294, duration: 0.196s, episode steps:  94, steps per second: 479, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.160 [0.000, 2.000],  loss: 1.552795, mae: 23.718291, mean_q: -34.490144, mean_eps: 0.050000\n",
      " 163641/200000: episode: 1295, duration: 0.484s, episode steps: 231, steps per second: 477, episode reward: -230.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.545 [0.000, 2.000],  loss: 0.626566, mae: 23.783544, mean_q: -34.600305, mean_eps: 0.050000\n",
      " 163746/200000: episode: 1296, duration: 0.220s, episode steps: 105, steps per second: 476, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.190 [0.000, 2.000],  loss: 0.581524, mae: 24.031239, mean_q: -34.930762, mean_eps: 0.050000\n",
      " 163863/200000: episode: 1297, duration: 0.240s, episode steps: 117, steps per second: 487, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.495710, mae: 23.531485, mean_q: -34.244005, mean_eps: 0.050000\n",
      " 163941/200000: episode: 1298, duration: 0.169s, episode steps:  78, steps per second: 461, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.667151, mae: 24.093939, mean_q: -35.076776, mean_eps: 0.050000\n",
      " 164013/200000: episode: 1299, duration: 0.184s, episode steps:  72, steps per second: 391, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.706462, mae: 23.857903, mean_q: -34.695223, mean_eps: 0.050000\n",
      " 164091/200000: episode: 1300, duration: 0.182s, episode steps:  78, steps per second: 427, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 0.884265, mae: 23.243376, mean_q: -33.693079, mean_eps: 0.050000\n",
      " 164185/200000: episode: 1301, duration: 0.206s, episode steps:  94, steps per second: 457, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.741119, mae: 23.715834, mean_q: -34.424056, mean_eps: 0.050000\n",
      " 164267/200000: episode: 1302, duration: 0.171s, episode steps:  82, steps per second: 478, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.779712, mae: 23.670004, mean_q: -34.366421, mean_eps: 0.050000\n",
      " 164351/200000: episode: 1303, duration: 0.175s, episode steps:  84, steps per second: 480, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.753979, mae: 23.684266, mean_q: -34.375541, mean_eps: 0.050000\n",
      " 164438/200000: episode: 1304, duration: 0.188s, episode steps:  87, steps per second: 463, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.981578, mae: 23.134900, mean_q: -33.568275, mean_eps: 0.050000\n",
      " 164515/200000: episode: 1305, duration: 0.181s, episode steps:  77, steps per second: 426, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.792090, mae: 24.097166, mean_q: -34.955813, mean_eps: 0.050000\n",
      " 164587/200000: episode: 1306, duration: 0.156s, episode steps:  72, steps per second: 461, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.767551, mae: 24.012398, mean_q: -34.779608, mean_eps: 0.050000\n",
      " 164683/200000: episode: 1307, duration: 0.205s, episode steps:  96, steps per second: 469, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.627869, mae: 23.721019, mean_q: -34.451447, mean_eps: 0.050000\n",
      " 164776/200000: episode: 1308, duration: 0.200s, episode steps:  93, steps per second: 465, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.652066, mae: 24.306015, mean_q: -35.336214, mean_eps: 0.050000\n",
      " 164856/200000: episode: 1309, duration: 0.184s, episode steps:  80, steps per second: 435, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.722595, mae: 23.323175, mean_q: -33.846548, mean_eps: 0.050000\n",
      " 164919/200000: episode: 1310, duration: 0.135s, episode steps:  63, steps per second: 465, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.807190, mae: 23.932493, mean_q: -34.700197, mean_eps: 0.050000\n",
      " 164989/200000: episode: 1311, duration: 0.150s, episode steps:  70, steps per second: 467, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.626112, mae: 24.162530, mean_q: -35.114382, mean_eps: 0.050000\n",
      " 165071/200000: episode: 1312, duration: 0.175s, episode steps:  82, steps per second: 469, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.839380, mae: 23.691279, mean_q: -34.347509, mean_eps: 0.050000\n",
      " 165175/200000: episode: 1313, duration: 0.229s, episode steps: 104, steps per second: 454, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 1.708740, mae: 24.036789, mean_q: -34.931639, mean_eps: 0.050000\n",
      " 165271/200000: episode: 1314, duration: 0.206s, episode steps:  96, steps per second: 467, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 0.570017, mae: 24.087304, mean_q: -35.066741, mean_eps: 0.050000\n",
      " 165352/200000: episode: 1315, duration: 0.183s, episode steps:  81, steps per second: 442, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 1.111609, mae: 24.142518, mean_q: -35.112215, mean_eps: 0.050000\n",
      " 165453/200000: episode: 1316, duration: 0.227s, episode steps: 101, steps per second: 445, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.832 [0.000, 2.000],  loss: 0.577543, mae: 24.363653, mean_q: -35.525057, mean_eps: 0.050000\n",
      " 165554/200000: episode: 1317, duration: 0.223s, episode steps: 101, steps per second: 452, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.597619, mae: 24.091143, mean_q: -35.062690, mean_eps: 0.050000\n",
      " 165652/200000: episode: 1318, duration: 0.201s, episode steps:  98, steps per second: 489, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.653259, mae: 23.958687, mean_q: -34.886315, mean_eps: 0.050000\n",
      " 165748/200000: episode: 1319, duration: 0.199s, episode steps:  96, steps per second: 481, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.534139, mae: 24.302954, mean_q: -35.386755, mean_eps: 0.050000\n",
      " 165830/200000: episode: 1320, duration: 0.169s, episode steps:  82, steps per second: 484, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 2.779272, mae: 23.572446, mean_q: -34.229518, mean_eps: 0.050000\n",
      " 165911/200000: episode: 1321, duration: 0.165s, episode steps:  81, steps per second: 491, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.711768, mae: 24.339246, mean_q: -35.476841, mean_eps: 0.050000\n",
      " 165992/200000: episode: 1322, duration: 0.179s, episode steps:  81, steps per second: 453, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.619031, mae: 23.564607, mean_q: -34.374252, mean_eps: 0.050000\n",
      " 166069/200000: episode: 1323, duration: 0.171s, episode steps:  77, steps per second: 451, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.600195, mae: 23.696081, mean_q: -34.475872, mean_eps: 0.050000\n",
      " 166145/200000: episode: 1324, duration: 0.163s, episode steps:  76, steps per second: 467, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.978467, mae: 23.448532, mean_q: -34.102599, mean_eps: 0.050000\n",
      " 166247/200000: episode: 1325, duration: 0.205s, episode steps: 102, steps per second: 498, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.841471, mae: 23.659234, mean_q: -34.345941, mean_eps: 0.050000\n",
      " 166334/200000: episode: 1326, duration: 0.189s, episode steps:  87, steps per second: 461, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.655629, mae: 23.676817, mean_q: -34.416407, mean_eps: 0.050000\n",
      " 166449/200000: episode: 1327, duration: 0.260s, episode steps: 115, steps per second: 443, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.278 [0.000, 2.000],  loss: 0.660075, mae: 23.607415, mean_q: -34.352007, mean_eps: 0.050000\n",
      " 166512/200000: episode: 1328, duration: 0.142s, episode steps:  63, steps per second: 444, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 1.152483, mae: 23.204503, mean_q: -33.617579, mean_eps: 0.050000\n",
      " 166594/200000: episode: 1329, duration: 0.188s, episode steps:  82, steps per second: 437, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.870539, mae: 23.627698, mean_q: -34.219428, mean_eps: 0.050000\n",
      " 166691/200000: episode: 1330, duration: 0.200s, episode steps:  97, steps per second: 485, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.814 [0.000, 2.000],  loss: 1.059001, mae: 23.581038, mean_q: -34.189814, mean_eps: 0.050000\n",
      " 166780/200000: episode: 1331, duration: 0.202s, episode steps:  89, steps per second: 441, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.721169, mae: 24.138232, mean_q: -35.150311, mean_eps: 0.050000\n",
      " 166866/200000: episode: 1332, duration: 0.184s, episode steps:  86, steps per second: 467, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.743963, mae: 23.339655, mean_q: -33.887975, mean_eps: 0.050000\n",
      " 166960/200000: episode: 1333, duration: 0.206s, episode steps:  94, steps per second: 456, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.766 [0.000, 2.000],  loss: 0.818615, mae: 23.294722, mean_q: -33.853946, mean_eps: 0.050000\n",
      " 167048/200000: episode: 1334, duration: 0.215s, episode steps:  88, steps per second: 410, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.761 [0.000, 2.000],  loss: 0.818670, mae: 24.056653, mean_q: -34.902477, mean_eps: 0.050000\n",
      " 167137/200000: episode: 1335, duration: 0.206s, episode steps:  89, steps per second: 432, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.112 [0.000, 2.000],  loss: 3.362223, mae: 24.065742, mean_q: -34.794073, mean_eps: 0.050000\n",
      " 167203/200000: episode: 1336, duration: 0.145s, episode steps:  66, steps per second: 455, episode reward: -65.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.773 [0.000, 2.000],  loss: 0.817261, mae: 23.451835, mean_q: -33.939216, mean_eps: 0.050000\n",
      " 167301/200000: episode: 1337, duration: 0.212s, episode steps:  98, steps per second: 462, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.969930, mae: 23.984276, mean_q: -34.688222, mean_eps: 0.050000\n",
      " 167376/200000: episode: 1338, duration: 0.163s, episode steps:  75, steps per second: 460, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.693 [0.000, 2.000],  loss: 0.780843, mae: 23.637616, mean_q: -34.183552, mean_eps: 0.050000\n",
      " 167479/200000: episode: 1339, duration: 0.226s, episode steps: 103, steps per second: 456, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.964168, mae: 23.783267, mean_q: -34.463568, mean_eps: 0.050000\n",
      " 167619/200000: episode: 1340, duration: 0.307s, episode steps: 140, steps per second: 456, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.329 [0.000, 2.000],  loss: 0.696777, mae: 24.123989, mean_q: -34.980981, mean_eps: 0.050000\n",
      " 167696/200000: episode: 1341, duration: 0.169s, episode steps:  77, steps per second: 454, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.623834, mae: 24.343090, mean_q: -35.344503, mean_eps: 0.050000\n",
      " 167809/200000: episode: 1342, duration: 0.252s, episode steps: 113, steps per second: 449, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.580550, mae: 23.581690, mean_q: -34.295270, mean_eps: 0.050000\n",
      " 167901/200000: episode: 1343, duration: 0.188s, episode steps:  92, steps per second: 490, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 1.651156, mae: 23.967841, mean_q: -34.736109, mean_eps: 0.050000\n",
      " 167982/200000: episode: 1344, duration: 0.178s, episode steps:  81, steps per second: 456, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.832520, mae: 24.135543, mean_q: -35.104559, mean_eps: 0.050000\n",
      " 168057/200000: episode: 1345, duration: 0.167s, episode steps:  75, steps per second: 449, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.918213, mae: 23.755234, mean_q: -34.427930, mean_eps: 0.050000\n",
      " 168145/200000: episode: 1346, duration: 0.196s, episode steps:  88, steps per second: 448, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 1.051935, mae: 23.977055, mean_q: -34.738124, mean_eps: 0.050000\n",
      " 168235/200000: episode: 1347, duration: 0.190s, episode steps:  90, steps per second: 474, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.752242, mae: 23.905232, mean_q: -34.677610, mean_eps: 0.050000\n",
      " 168328/200000: episode: 1348, duration: 0.211s, episode steps:  93, steps per second: 442, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 0.692205, mae: 23.815551, mean_q: -34.551903, mean_eps: 0.050000\n",
      " 168468/200000: episode: 1349, duration: 0.311s, episode steps: 140, steps per second: 450, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.834188, mae: 24.464085, mean_q: -35.567105, mean_eps: 0.050000\n",
      " 168542/200000: episode: 1350, duration: 0.169s, episode steps:  74, steps per second: 438, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.837240, mae: 24.368116, mean_q: -35.367355, mean_eps: 0.050000\n",
      " 168621/200000: episode: 1351, duration: 0.170s, episode steps:  79, steps per second: 464, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.872046, mae: 23.776496, mean_q: -34.462105, mean_eps: 0.050000\n",
      " 168686/200000: episode: 1352, duration: 0.137s, episode steps:  65, steps per second: 476, episode reward: -64.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 1.015076, mae: 24.211148, mean_q: -35.167475, mean_eps: 0.050000\n",
      " 168802/200000: episode: 1353, duration: 0.259s, episode steps: 116, steps per second: 448, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.864665, mae: 23.896738, mean_q: -34.735483, mean_eps: 0.050000\n",
      " 168882/200000: episode: 1354, duration: 0.175s, episode steps:  80, steps per second: 456, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.807813, mae: 23.772999, mean_q: -34.540505, mean_eps: 0.050000\n",
      " 168985/200000: episode: 1355, duration: 0.228s, episode steps: 103, steps per second: 451, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 2.065880, mae: 24.012746, mean_q: -34.842081, mean_eps: 0.050000\n",
      " 169067/200000: episode: 1356, duration: 0.175s, episode steps:  82, steps per second: 469, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 0.820996, mae: 23.990030, mean_q: -34.769063, mean_eps: 0.050000\n",
      " 169269/200000: episode: 1357, duration: 0.427s, episode steps: 202, steps per second: 473, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.787 [0.000, 2.000],  loss: 0.967891, mae: 23.958452, mean_q: -34.837944, mean_eps: 0.050000\n",
      " 169354/200000: episode: 1358, duration: 0.183s, episode steps:  85, steps per second: 465, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.805353, mae: 24.696232, mean_q: -35.967227, mean_eps: 0.050000\n",
      " 169425/200000: episode: 1359, duration: 0.154s, episode steps:  71, steps per second: 461, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.141 [0.000, 2.000],  loss: 0.746094, mae: 23.809560, mean_q: -34.600982, mean_eps: 0.050000\n",
      " 169508/200000: episode: 1360, duration: 0.181s, episode steps:  83, steps per second: 457, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.809399, mae: 24.065114, mean_q: -34.975451, mean_eps: 0.050000\n",
      " 169584/200000: episode: 1361, duration: 0.187s, episode steps:  76, steps per second: 407, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 1.135796, mae: 23.951110, mean_q: -34.680400, mean_eps: 0.050000\n",
      " 169666/200000: episode: 1362, duration: 0.178s, episode steps:  82, steps per second: 461, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.670699, mae: 24.970429, mean_q: -36.311731, mean_eps: 0.050000\n",
      " 169747/200000: episode: 1363, duration: 0.172s, episode steps:  81, steps per second: 471, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 1.061328, mae: 24.431535, mean_q: -35.477060, mean_eps: 0.050000\n",
      " 169852/200000: episode: 1364, duration: 0.222s, episode steps: 105, steps per second: 474, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 1.618277, mae: 24.474723, mean_q: -35.577837, mean_eps: 0.050000\n",
      " 169940/200000: episode: 1365, duration: 0.189s, episode steps:  88, steps per second: 467, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.856401, mae: 24.019510, mean_q: -34.913066, mean_eps: 0.050000\n",
      " 170054/200000: episode: 1366, duration: 0.234s, episode steps: 114, steps per second: 487, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 0.765974, mae: 24.272546, mean_q: -35.316470, mean_eps: 0.050000\n",
      " 170152/200000: episode: 1367, duration: 0.210s, episode steps:  98, steps per second: 467, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.900187, mae: 24.494929, mean_q: -35.557230, mean_eps: 0.050000\n",
      " 170238/200000: episode: 1368, duration: 0.184s, episode steps:  86, steps per second: 467, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.977694, mae: 24.439352, mean_q: -35.395667, mean_eps: 0.050000\n",
      " 170315/200000: episode: 1369, duration: 0.166s, episode steps:  77, steps per second: 463, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.834766, mae: 24.450832, mean_q: -35.514832, mean_eps: 0.050000\n",
      " 170459/200000: episode: 1370, duration: 0.301s, episode steps: 144, steps per second: 478, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 1.847751, mae: 24.364525, mean_q: -35.270362, mean_eps: 0.050000\n",
      " 170531/200000: episode: 1371, duration: 0.147s, episode steps:  72, steps per second: 488, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.680311, mae: 24.872180, mean_q: -36.129497, mean_eps: 0.050000\n",
      " 170603/200000: episode: 1372, duration: 0.149s, episode steps:  72, steps per second: 484, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.840332, mae: 24.064307, mean_q: -34.869292, mean_eps: 0.050000\n",
      " 170715/200000: episode: 1373, duration: 0.242s, episode steps: 112, steps per second: 462, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 0.717739, mae: 24.549253, mean_q: -35.689116, mean_eps: 0.050000\n",
      " 170792/200000: episode: 1374, duration: 0.172s, episode steps:  77, steps per second: 447, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.565023, mae: 24.344426, mean_q: -35.400427, mean_eps: 0.050000\n",
      " 170885/200000: episode: 1375, duration: 0.207s, episode steps:  93, steps per second: 448, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 0.777242, mae: 24.514407, mean_q: -35.688596, mean_eps: 0.050000\n",
      " 170956/200000: episode: 1376, duration: 0.149s, episode steps:  71, steps per second: 478, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.784044, mae: 23.579869, mean_q: -34.209044, mean_eps: 0.050000\n",
      " 171026/200000: episode: 1377, duration: 0.159s, episode steps:  70, steps per second: 441, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.790283, mae: 25.011564, mean_q: -36.322815, mean_eps: 0.050000\n",
      " 171105/200000: episode: 1378, duration: 0.168s, episode steps:  79, steps per second: 470, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.883569, mae: 24.169297, mean_q: -34.972103, mean_eps: 0.050000\n",
      " 171187/200000: episode: 1379, duration: 0.171s, episode steps:  82, steps per second: 480, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 2.193091, mae: 23.589504, mean_q: -34.060058, mean_eps: 0.050000\n",
      " 171265/200000: episode: 1380, duration: 0.168s, episode steps:  78, steps per second: 464, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.835864, mae: 25.077494, mean_q: -36.388392, mean_eps: 0.050000\n",
      " 171392/200000: episode: 1381, duration: 0.272s, episode steps: 127, steps per second: 467, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.848682, mae: 25.009299, mean_q: -36.279857, mean_eps: 0.050000\n",
      " 171469/200000: episode: 1382, duration: 0.162s, episode steps:  77, steps per second: 475, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.699353, mae: 24.467093, mean_q: -35.512857, mean_eps: 0.050000\n",
      " 171562/200000: episode: 1383, duration: 0.202s, episode steps:  93, steps per second: 460, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 2.081685, mae: 24.486729, mean_q: -35.349742, mean_eps: 0.050000\n",
      " 171704/200000: episode: 1384, duration: 0.309s, episode steps: 142, steps per second: 460, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.239 [0.000, 2.000],  loss: 1.894775, mae: 24.450816, mean_q: -35.378190, mean_eps: 0.050000\n",
      " 171784/200000: episode: 1385, duration: 0.178s, episode steps:  80, steps per second: 450, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 1.739551, mae: 23.838586, mean_q: -34.405975, mean_eps: 0.050000\n",
      " 171865/200000: episode: 1386, duration: 0.184s, episode steps:  81, steps per second: 440, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.850164, mae: 24.380928, mean_q: -35.397822, mean_eps: 0.050000\n",
      " 171935/200000: episode: 1387, duration: 0.137s, episode steps:  70, steps per second: 510, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.827271, mae: 24.207842, mean_q: -35.161697, mean_eps: 0.050000\n",
      " 172009/200000: episode: 1388, duration: 0.169s, episode steps:  74, steps per second: 437, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.189 [0.000, 2.000],  loss: 0.692163, mae: 24.357750, mean_q: -35.318724, mean_eps: 0.050000\n",
      " 172183/200000: episode: 1389, duration: 0.365s, episode steps: 174, steps per second: 477, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.810 [0.000, 2.000],  loss: 1.598842, mae: 24.735698, mean_q: -35.873752, mean_eps: 0.050000\n",
      " 172265/200000: episode: 1390, duration: 0.181s, episode steps:  82, steps per second: 453, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 0.980136, mae: 24.610421, mean_q: -35.669346, mean_eps: 0.050000\n",
      " 172341/200000: episode: 1391, duration: 0.156s, episode steps:  76, steps per second: 486, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.693061, mae: 23.742768, mean_q: -34.497473, mean_eps: 0.050000\n",
      " 172433/200000: episode: 1392, duration: 0.200s, episode steps:  92, steps per second: 461, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.731405, mae: 24.660403, mean_q: -35.833464, mean_eps: 0.050000\n",
      " 172510/200000: episode: 1393, duration: 0.164s, episode steps:  77, steps per second: 468, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.984239, mae: 24.752584, mean_q: -35.926129, mean_eps: 0.050000\n",
      " 172577/200000: episode: 1394, duration: 0.154s, episode steps:  67, steps per second: 434, episode reward: -66.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 1.509006, mae: 24.927547, mean_q: -36.038529, mean_eps: 0.050000\n",
      " 172658/200000: episode: 1395, duration: 0.179s, episode steps:  81, steps per second: 452, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 2.030103, mae: 24.940017, mean_q: -36.129933, mean_eps: 0.050000\n",
      " 172736/200000: episode: 1396, duration: 0.169s, episode steps:  78, steps per second: 463, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 0.896430, mae: 24.444548, mean_q: -35.528002, mean_eps: 0.050000\n",
      " 172824/200000: episode: 1397, duration: 0.202s, episode steps:  88, steps per second: 435, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.807 [0.000, 2.000],  loss: 0.686457, mae: 24.775216, mean_q: -36.056785, mean_eps: 0.050000\n",
      " 172903/200000: episode: 1398, duration: 0.171s, episode steps:  79, steps per second: 461, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.772583, mae: 24.428575, mean_q: -35.579163, mean_eps: 0.050000\n",
      " 172999/200000: episode: 1399, duration: 0.210s, episode steps:  96, steps per second: 458, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.784098, mae: 24.837473, mean_q: -36.231701, mean_eps: 0.050000\n",
      " 173085/200000: episode: 1400, duration: 0.187s, episode steps:  86, steps per second: 460, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.895952, mae: 24.501202, mean_q: -35.552386, mean_eps: 0.050000\n",
      " 173162/200000: episode: 1401, duration: 0.177s, episode steps:  77, steps per second: 436, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 2.633008, mae: 24.967651, mean_q: -36.250880, mean_eps: 0.050000\n",
      " 173288/200000: episode: 1402, duration: 0.275s, episode steps: 126, steps per second: 459, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.714 [0.000, 2.000],  loss: 0.836182, mae: 24.346515, mean_q: -35.370322, mean_eps: 0.050000\n",
      " 173370/200000: episode: 1403, duration: 0.183s, episode steps:  82, steps per second: 449, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.753884, mae: 24.064438, mean_q: -34.991838, mean_eps: 0.050000\n",
      " 173451/200000: episode: 1404, duration: 0.190s, episode steps:  81, steps per second: 426, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.854211, mae: 24.513888, mean_q: -35.652539, mean_eps: 0.050000\n",
      " 173533/200000: episode: 1405, duration: 0.198s, episode steps:  82, steps per second: 414, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.134 [0.000, 2.000],  loss: 0.712585, mae: 24.568853, mean_q: -35.703526, mean_eps: 0.050000\n",
      " 173626/200000: episode: 1406, duration: 0.221s, episode steps:  93, steps per second: 421, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.766215, mae: 23.842842, mean_q: -34.554116, mean_eps: 0.050000\n",
      " 173714/200000: episode: 1407, duration: 0.183s, episode steps:  88, steps per second: 481, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.948642, mae: 23.735055, mean_q: -34.348605, mean_eps: 0.050000\n",
      " 173800/200000: episode: 1408, duration: 0.187s, episode steps:  86, steps per second: 459, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 2.068481, mae: 24.170114, mean_q: -34.958096, mean_eps: 0.050000\n",
      " 173886/200000: episode: 1409, duration: 0.190s, episode steps:  86, steps per second: 454, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.834140, mae: 24.700600, mean_q: -35.784662, mean_eps: 0.050000\n",
      " 173955/200000: episode: 1410, duration: 0.152s, episode steps:  69, steps per second: 455, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 1.239610, mae: 24.720907, mean_q: -35.861124, mean_eps: 0.050000\n",
      " 174032/200000: episode: 1411, duration: 0.168s, episode steps:  77, steps per second: 459, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.854953, mae: 24.292808, mean_q: -35.238809, mean_eps: 0.050000\n",
      " 174110/200000: episode: 1412, duration: 0.174s, episode steps:  78, steps per second: 448, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.896143, mae: 24.167580, mean_q: -35.059295, mean_eps: 0.050000\n",
      " 174205/200000: episode: 1413, duration: 0.208s, episode steps:  95, steps per second: 458, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.880554, mae: 23.948277, mean_q: -34.656576, mean_eps: 0.050000\n",
      " 174282/200000: episode: 1414, duration: 0.174s, episode steps:  77, steps per second: 443, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.783655, mae: 24.079303, mean_q: -34.904691, mean_eps: 0.050000\n",
      " 174376/200000: episode: 1415, duration: 0.211s, episode steps:  94, steps per second: 446, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.688521, mae: 24.418699, mean_q: -35.495566, mean_eps: 0.050000\n",
      " 174469/200000: episode: 1416, duration: 0.208s, episode steps:  93, steps per second: 448, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.806 [0.000, 2.000],  loss: 0.514292, mae: 24.084411, mean_q: -35.006521, mean_eps: 0.050000\n",
      " 174539/200000: episode: 1417, duration: 0.161s, episode steps:  70, steps per second: 435, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.900 [0.000, 2.000],  loss: 0.873345, mae: 24.351410, mean_q: -35.297383, mean_eps: 0.050000\n",
      " 174627/200000: episode: 1418, duration: 0.189s, episode steps:  88, steps per second: 466, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.828458, mae: 24.114428, mean_q: -34.967642, mean_eps: 0.050000\n",
      " 174735/200000: episode: 1419, duration: 0.227s, episode steps: 108, steps per second: 476, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 0.845083, mae: 23.610356, mean_q: -34.105445, mean_eps: 0.050000\n",
      " 174851/200000: episode: 1420, duration: 0.258s, episode steps: 116, steps per second: 450, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 0.647314, mae: 23.989487, mean_q: -34.765955, mean_eps: 0.050000\n",
      " 174944/200000: episode: 1421, duration: 0.195s, episode steps:  93, steps per second: 477, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.724565, mae: 24.913283, mean_q: -36.241725, mean_eps: 0.050000\n",
      " 175021/200000: episode: 1422, duration: 0.170s, episode steps:  77, steps per second: 453, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.978101, mae: 23.420093, mean_q: -33.865944, mean_eps: 0.050000\n",
      " 175109/200000: episode: 1423, duration: 0.180s, episode steps:  88, steps per second: 490, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.945956, mae: 23.312609, mean_q: -33.703171, mean_eps: 0.050000\n",
      " 175214/200000: episode: 1424, duration: 0.217s, episode steps: 105, steps per second: 485, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.829 [0.000, 2.000],  loss: 0.816143, mae: 23.731750, mean_q: -34.433217, mean_eps: 0.050000\n",
      " 175293/200000: episode: 1425, duration: 0.164s, episode steps:  79, steps per second: 481, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.761682, mae: 24.174632, mean_q: -35.137280, mean_eps: 0.050000\n",
      " 175381/200000: episode: 1426, duration: 0.182s, episode steps:  88, steps per second: 484, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 2.316562, mae: 23.705594, mean_q: -34.387204, mean_eps: 0.050000\n",
      " 175461/200000: episode: 1427, duration: 0.168s, episode steps:  80, steps per second: 475, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.752466, mae: 23.452735, mean_q: -33.995157, mean_eps: 0.050000\n",
      " 175539/200000: episode: 1428, duration: 0.171s, episode steps:  78, steps per second: 457, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.621875, mae: 24.279160, mean_q: -35.183989, mean_eps: 0.050000\n",
      " 175628/200000: episode: 1429, duration: 0.194s, episode steps:  89, steps per second: 459, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.820 [0.000, 2.000],  loss: 0.851318, mae: 24.160239, mean_q: -35.041896, mean_eps: 0.050000\n",
      " 175697/200000: episode: 1430, duration: 0.159s, episode steps:  69, steps per second: 435, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 0.634684, mae: 24.102591, mean_q: -35.029486, mean_eps: 0.050000\n",
      " 175791/200000: episode: 1431, duration: 0.190s, episode steps:  94, steps per second: 494, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.835116, mae: 24.047059, mean_q: -34.893559, mean_eps: 0.050000\n",
      " 175860/200000: episode: 1432, duration: 0.149s, episode steps:  69, steps per second: 463, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.174 [0.000, 2.000],  loss: 0.766886, mae: 24.157745, mean_q: -35.105326, mean_eps: 0.050000\n",
      " 175975/200000: episode: 1433, duration: 0.238s, episode steps: 115, steps per second: 484, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 1.791120, mae: 23.741462, mean_q: -34.477461, mean_eps: 0.050000\n",
      " 176064/200000: episode: 1434, duration: 0.208s, episode steps:  89, steps per second: 428, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.779119, mae: 23.765958, mean_q: -34.509798, mean_eps: 0.050000\n",
      " 176151/200000: episode: 1435, duration: 0.190s, episode steps:  87, steps per second: 458, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 0.839844, mae: 23.104537, mean_q: -33.497405, mean_eps: 0.050000\n",
      " 176235/200000: episode: 1436, duration: 0.191s, episode steps:  84, steps per second: 441, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.637551, mae: 23.710331, mean_q: -34.468466, mean_eps: 0.050000\n",
      " 176319/200000: episode: 1437, duration: 0.172s, episode steps:  84, steps per second: 488, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.792261, mae: 23.351516, mean_q: -33.957623, mean_eps: 0.050000\n",
      " 176400/200000: episode: 1438, duration: 0.190s, episode steps:  81, steps per second: 426, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 1.510107, mae: 24.562894, mean_q: -35.708174, mean_eps: 0.050000\n",
      " 176484/200000: episode: 1439, duration: 0.181s, episode steps:  84, steps per second: 465, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.449818, mae: 24.215859, mean_q: -35.283968, mean_eps: 0.050000\n",
      " 176590/200000: episode: 1440, duration: 0.220s, episode steps: 106, steps per second: 482, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 1.280950, mae: 23.945624, mean_q: -34.722197, mean_eps: 0.050000\n",
      " 176689/200000: episode: 1441, duration: 0.221s, episode steps:  99, steps per second: 447, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.152 [0.000, 2.000],  loss: 1.066143, mae: 23.973868, mean_q: -34.834509, mean_eps: 0.050000\n",
      " 176759/200000: episode: 1442, duration: 0.142s, episode steps:  70, steps per second: 493, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.565155, mae: 23.837903, mean_q: -34.604040, mean_eps: 0.050000\n",
      " 176851/200000: episode: 1443, duration: 0.208s, episode steps:  92, steps per second: 443, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.577799, mae: 23.910920, mean_q: -34.732965, mean_eps: 0.050000\n",
      " 176935/200000: episode: 1444, duration: 0.171s, episode steps:  84, steps per second: 493, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.637646, mae: 24.423739, mean_q: -35.533136, mean_eps: 0.050000\n",
      " 177013/200000: episode: 1445, duration: 0.179s, episode steps:  78, steps per second: 436, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 1.784521, mae: 24.135677, mean_q: -35.099171, mean_eps: 0.050000\n",
      " 177097/200000: episode: 1446, duration: 0.181s, episode steps:  84, steps per second: 463, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.155 [0.000, 2.000],  loss: 0.778098, mae: 23.725053, mean_q: -34.438808, mean_eps: 0.050000\n",
      " 177176/200000: episode: 1447, duration: 0.175s, episode steps:  79, steps per second: 451, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.139 [0.000, 2.000],  loss: 0.712185, mae: 24.036486, mean_q: -34.970842, mean_eps: 0.050000\n",
      " 177254/200000: episode: 1448, duration: 0.176s, episode steps:  78, steps per second: 443, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.970728, mae: 24.261195, mean_q: -35.213426, mean_eps: 0.050000\n",
      " 177330/200000: episode: 1449, duration: 0.173s, episode steps:  76, steps per second: 440, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.665576, mae: 24.537073, mean_q: -35.702378, mean_eps: 0.050000\n",
      " 177422/200000: episode: 1450, duration: 0.203s, episode steps:  92, steps per second: 452, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 0.916837, mae: 23.754497, mean_q: -34.470627, mean_eps: 0.050000\n",
      " 177495/200000: episode: 1451, duration: 0.156s, episode steps:  73, steps per second: 469, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 2.155653, mae: 23.603736, mean_q: -34.126506, mean_eps: 0.050000\n",
      " 177590/200000: episode: 1452, duration: 0.215s, episode steps:  95, steps per second: 443, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.828776, mae: 23.767718, mean_q: -34.388707, mean_eps: 0.050000\n",
      " 177683/200000: episode: 1453, duration: 0.201s, episode steps:  93, steps per second: 462, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 0.624695, mae: 23.651368, mean_q: -34.205518, mean_eps: 0.050000\n",
      " 177767/200000: episode: 1454, duration: 0.181s, episode steps:  84, steps per second: 464, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.694348, mae: 23.758620, mean_q: -34.453252, mean_eps: 0.050000\n",
      " 177846/200000: episode: 1455, duration: 0.174s, episode steps:  79, steps per second: 455, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.517859, mae: 23.359370, mean_q: -33.897102, mean_eps: 0.050000\n",
      " 177928/200000: episode: 1456, duration: 0.180s, episode steps:  82, steps per second: 456, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.696997, mae: 24.102810, mean_q: -35.033263, mean_eps: 0.050000\n",
      " 178002/200000: episode: 1457, duration: 0.167s, episode steps:  74, steps per second: 444, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 0.685352, mae: 23.979412, mean_q: -34.893260, mean_eps: 0.050000\n",
      " 178081/200000: episode: 1458, duration: 0.167s, episode steps:  79, steps per second: 472, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.955005, mae: 23.918699, mean_q: -34.695623, mean_eps: 0.050000\n",
      " 178174/200000: episode: 1459, duration: 0.190s, episode steps:  93, steps per second: 489, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.844838, mae: 24.045480, mean_q: -34.869120, mean_eps: 0.050000\n",
      " 178237/200000: episode: 1460, duration: 0.135s, episode steps:  63, steps per second: 465, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 1.746033, mae: 23.630774, mean_q: -34.198325, mean_eps: 0.050000\n",
      " 178324/200000: episode: 1461, duration: 0.204s, episode steps:  87, steps per second: 427, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.149 [0.000, 2.000],  loss: 0.768288, mae: 23.807094, mean_q: -34.606106, mean_eps: 0.050000\n",
      " 178396/200000: episode: 1462, duration: 0.175s, episode steps:  72, steps per second: 412, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.650174, mae: 24.174265, mean_q: -35.171929, mean_eps: 0.050000\n",
      " 178469/200000: episode: 1463, duration: 0.158s, episode steps:  73, steps per second: 461, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.096 [0.000, 2.000],  loss: 0.654107, mae: 24.152647, mean_q: -35.100189, mean_eps: 0.050000\n",
      " 178543/200000: episode: 1464, duration: 0.163s, episode steps:  74, steps per second: 453, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.149 [0.000, 2.000],  loss: 1.248508, mae: 23.787509, mean_q: -34.490072, mean_eps: 0.050000\n",
      " 178625/200000: episode: 1465, duration: 0.189s, episode steps:  82, steps per second: 435, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.171 [0.000, 2.000],  loss: 1.328458, mae: 24.329638, mean_q: -35.405949, mean_eps: 0.050000\n",
      " 178722/200000: episode: 1466, duration: 0.207s, episode steps:  97, steps per second: 468, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 1.138652, mae: 24.628090, mean_q: -35.849401, mean_eps: 0.050000\n",
      " 178808/200000: episode: 1467, duration: 0.198s, episode steps:  86, steps per second: 434, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.209 [0.000, 2.000],  loss: 0.833545, mae: 24.332529, mean_q: -35.392096, mean_eps: 0.050000\n",
      " 178890/200000: episode: 1468, duration: 0.189s, episode steps:  82, steps per second: 434, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.606756, mae: 24.018194, mean_q: -35.008909, mean_eps: 0.050000\n",
      " 178981/200000: episode: 1469, duration: 0.198s, episode steps:  91, steps per second: 460, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.775746, mae: 24.067542, mean_q: -35.019189, mean_eps: 0.050000\n",
      " 179080/200000: episode: 1470, duration: 0.231s, episode steps:  99, steps per second: 429, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 2.046448, mae: 23.602523, mean_q: -34.178500, mean_eps: 0.050000\n",
      " 179173/200000: episode: 1471, duration: 0.211s, episode steps:  93, steps per second: 442, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 2.075663, mae: 23.853153, mean_q: -34.721255, mean_eps: 0.050000\n",
      " 179263/200000: episode: 1472, duration: 0.201s, episode steps:  90, steps per second: 447, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 0.573975, mae: 24.468634, mean_q: -35.661093, mean_eps: 0.050000\n",
      " 179341/200000: episode: 1473, duration: 0.180s, episode steps:  78, steps per second: 434, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 2.093359, mae: 23.451396, mean_q: -34.048315, mean_eps: 0.050000\n",
      " 179424/200000: episode: 1474, duration: 0.199s, episode steps:  83, steps per second: 417, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.707739, mae: 23.440530, mean_q: -34.117618, mean_eps: 0.050000\n",
      " 179513/200000: episode: 1475, duration: 0.210s, episode steps:  89, steps per second: 424, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.844096, mae: 24.239535, mean_q: -35.276298, mean_eps: 0.050000\n",
      " 179605/200000: episode: 1476, duration: 0.196s, episode steps:  92, steps per second: 470, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 1.045011, mae: 24.558230, mean_q: -35.678219, mean_eps: 0.050000\n",
      " 179686/200000: episode: 1477, duration: 0.175s, episode steps:  81, steps per second: 462, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.920068, mae: 23.742727, mean_q: -34.420463, mean_eps: 0.050000\n",
      " 179769/200000: episode: 1478, duration: 0.189s, episode steps:  83, steps per second: 440, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.205 [0.000, 2.000],  loss: 0.740257, mae: 24.060143, mean_q: -35.070241, mean_eps: 0.050000\n",
      " 179862/200000: episode: 1479, duration: 0.199s, episode steps:  93, steps per second: 467, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.769354, mae: 24.271549, mean_q: -35.314845, mean_eps: 0.050000\n",
      " 179940/200000: episode: 1480, duration: 0.179s, episode steps:  78, steps per second: 435, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.932861, mae: 23.921434, mean_q: -34.699904, mean_eps: 0.050000\n",
      " 180014/200000: episode: 1481, duration: 0.164s, episode steps:  74, steps per second: 450, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 0.868001, mae: 24.191269, mean_q: -35.122883, mean_eps: 0.050000\n",
      " 180083/200000: episode: 1482, duration: 0.158s, episode steps:  69, steps per second: 436, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 1.479736, mae: 24.488629, mean_q: -35.605979, mean_eps: 0.050000\n",
      " 180166/200000: episode: 1483, duration: 0.180s, episode steps:  83, steps per second: 462, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.651758, mae: 23.482596, mean_q: -34.140279, mean_eps: 0.050000\n",
      " 180241/200000: episode: 1484, duration: 0.166s, episode steps:  75, steps per second: 451, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 1.013745, mae: 23.523260, mean_q: -34.217641, mean_eps: 0.050000\n",
      " 180320/200000: episode: 1485, duration: 0.172s, episode steps:  79, steps per second: 460, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.719265, mae: 24.289288, mean_q: -35.383642, mean_eps: 0.050000\n",
      " 180419/200000: episode: 1486, duration: 0.217s, episode steps:  99, steps per second: 457, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.776630, mae: 23.802208, mean_q: -34.612189, mean_eps: 0.050000\n",
      " 180518/200000: episode: 1487, duration: 0.210s, episode steps:  99, steps per second: 471, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.705994, mae: 23.754101, mean_q: -34.550725, mean_eps: 0.050000\n",
      " 180637/200000: episode: 1488, duration: 0.261s, episode steps: 119, steps per second: 455, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.783040, mae: 24.028701, mean_q: -34.894917, mean_eps: 0.050000\n",
      " 180748/200000: episode: 1489, duration: 0.245s, episode steps: 111, steps per second: 453, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.625793, mae: 24.000685, mean_q: -34.863429, mean_eps: 0.050000\n",
      " 180849/200000: episode: 1490, duration: 0.223s, episode steps: 101, steps per second: 453, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.620474, mae: 24.114153, mean_q: -35.059208, mean_eps: 0.050000\n",
      " 180937/200000: episode: 1491, duration: 0.204s, episode steps:  88, steps per second: 431, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.654918, mae: 23.888298, mean_q: -34.695647, mean_eps: 0.050000\n",
      " 181014/200000: episode: 1492, duration: 0.167s, episode steps:  77, steps per second: 460, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.544027, mae: 24.070301, mean_q: -35.000568, mean_eps: 0.050000\n",
      " 181092/200000: episode: 1493, duration: 0.185s, episode steps:  78, steps per second: 422, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.685059, mae: 23.762920, mean_q: -34.455341, mean_eps: 0.050000\n",
      " 181165/200000: episode: 1494, duration: 0.159s, episode steps:  73, steps per second: 459, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 1.588921, mae: 23.972405, mean_q: -34.729685, mean_eps: 0.050000\n",
      " 181246/200000: episode: 1495, duration: 0.182s, episode steps:  81, steps per second: 446, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.160 [0.000, 2.000],  loss: 0.806226, mae: 24.036266, mean_q: -34.854803, mean_eps: 0.050000\n",
      " 181325/200000: episode: 1496, duration: 0.171s, episode steps:  79, steps per second: 461, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.752075, mae: 23.760011, mean_q: -34.484084, mean_eps: 0.050000\n",
      " 181423/200000: episode: 1497, duration: 0.213s, episode steps:  98, steps per second: 460, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.255 [0.000, 2.000],  loss: 1.998179, mae: 23.953007, mean_q: -34.795410, mean_eps: 0.050000\n",
      " 181513/200000: episode: 1498, duration: 0.207s, episode steps:  90, steps per second: 435, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.772929, mae: 24.445546, mean_q: -35.610188, mean_eps: 0.050000\n",
      " 181617/200000: episode: 1499, duration: 0.242s, episode steps: 104, steps per second: 429, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.212 [0.000, 2.000],  loss: 1.554857, mae: 24.057439, mean_q: -34.933690, mean_eps: 0.050000\n",
      " 181704/200000: episode: 1500, duration: 0.193s, episode steps:  87, steps per second: 451, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.172 [0.000, 2.000],  loss: 0.555103, mae: 23.954090, mean_q: -34.808754, mean_eps: 0.050000\n",
      " 181780/200000: episode: 1501, duration: 0.177s, episode steps:  76, steps per second: 430, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.506360, mae: 24.822280, mean_q: -36.128570, mean_eps: 0.050000\n",
      " 181873/200000: episode: 1502, duration: 0.213s, episode steps:  93, steps per second: 437, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 1.994893, mae: 23.942527, mean_q: -34.759054, mean_eps: 0.050000\n",
      " 181943/200000: episode: 1503, duration: 0.146s, episode steps:  70, steps per second: 480, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.714798, mae: 24.075418, mean_q: -34.934239, mean_eps: 0.050000\n",
      " 182020/200000: episode: 1504, duration: 0.186s, episode steps:  77, steps per second: 413, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 1.016199, mae: 24.234742, mean_q: -35.164185, mean_eps: 0.050000\n",
      " 182104/200000: episode: 1505, duration: 0.181s, episode steps:  84, steps per second: 464, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.745483, mae: 24.688953, mean_q: -35.877111, mean_eps: 0.050000\n",
      " 182185/200000: episode: 1506, duration: 0.195s, episode steps:  81, steps per second: 415, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.653209, mae: 24.367785, mean_q: -35.494992, mean_eps: 0.050000\n",
      " 182263/200000: episode: 1507, duration: 0.170s, episode steps:  78, steps per second: 459, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.074748, mae: 23.787053, mean_q: -34.545558, mean_eps: 0.050000\n",
      " 182375/200000: episode: 1508, duration: 0.256s, episode steps: 112, steps per second: 438, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.205 [0.000, 2.000],  loss: 0.718541, mae: 24.347370, mean_q: -35.362756, mean_eps: 0.050000\n",
      " 182460/200000: episode: 1509, duration: 0.185s, episode steps:  85, steps per second: 459, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.725697, mae: 23.553768, mean_q: -34.202810, mean_eps: 0.050000\n",
      " 182549/200000: episode: 1510, duration: 0.198s, episode steps:  89, steps per second: 449, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.683971, mae: 24.636736, mean_q: -35.809720, mean_eps: 0.050000\n",
      " 182668/200000: episode: 1511, duration: 0.266s, episode steps: 119, steps per second: 447, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.798910, mae: 24.946282, mean_q: -36.318466, mean_eps: 0.050000\n",
      " 182756/200000: episode: 1512, duration: 0.202s, episode steps:  88, steps per second: 437, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.843173, mae: 23.944960, mean_q: -34.803497, mean_eps: 0.050000\n",
      " 182843/200000: episode: 1513, duration: 0.202s, episode steps:  87, steps per second: 430, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.184 [0.000, 2.000],  loss: 0.780251, mae: 24.380353, mean_q: -35.438964, mean_eps: 0.050000\n",
      " 182914/200000: episode: 1514, duration: 0.160s, episode steps:  71, steps per second: 443, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.700562, mae: 23.919136, mean_q: -34.758247, mean_eps: 0.050000\n",
      " 183016/200000: episode: 1515, duration: 0.246s, episode steps: 102, steps per second: 415, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 1.597738, mae: 23.803336, mean_q: -34.582588, mean_eps: 0.050000\n",
      " 183132/200000: episode: 1516, duration: 0.255s, episode steps: 116, steps per second: 455, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 1.823665, mae: 24.217812, mean_q: -35.205777, mean_eps: 0.050000\n",
      " 183206/200000: episode: 1517, duration: 0.163s, episode steps:  74, steps per second: 455, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 0.662326, mae: 24.255074, mean_q: -35.383776, mean_eps: 0.050000\n",
      " 183288/200000: episode: 1518, duration: 0.193s, episode steps:  82, steps per second: 426, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.841 [0.000, 2.000],  loss: 2.729272, mae: 23.987742, mean_q: -34.914557, mean_eps: 0.050000\n",
      " 183351/200000: episode: 1519, duration: 0.143s, episode steps:  63, steps per second: 440, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.679382, mae: 23.737734, mean_q: -34.605737, mean_eps: 0.050000\n",
      " 183428/200000: episode: 1520, duration: 0.175s, episode steps:  77, steps per second: 439, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.999683, mae: 24.102905, mean_q: -35.068083, mean_eps: 0.050000\n",
      " 183498/200000: episode: 1521, duration: 0.159s, episode steps:  70, steps per second: 441, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.704129, mae: 24.222917, mean_q: -35.293114, mean_eps: 0.050000\n",
      " 183576/200000: episode: 1522, duration: 0.178s, episode steps:  78, steps per second: 438, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.798557, mae: 24.127571, mean_q: -35.107209, mean_eps: 0.050000\n",
      " 183661/200000: episode: 1523, duration: 0.189s, episode steps:  85, steps per second: 449, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.744229, mae: 23.971598, mean_q: -34.936818, mean_eps: 0.050000\n",
      " 183750/200000: episode: 1524, duration: 0.200s, episode steps:  89, steps per second: 444, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.992720, mae: 23.468699, mean_q: -34.105087, mean_eps: 0.050000\n",
      " 183828/200000: episode: 1525, duration: 0.176s, episode steps:  78, steps per second: 444, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.795 [0.000, 2.000],  loss: 0.758081, mae: 25.168833, mean_q: -36.779392, mean_eps: 0.050000\n",
      " 183915/200000: episode: 1526, duration: 0.199s, episode steps:  87, steps per second: 438, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.724 [0.000, 2.000],  loss: 0.944247, mae: 23.866182, mean_q: -34.790900, mean_eps: 0.050000\n",
      " 184000/200000: episode: 1527, duration: 0.194s, episode steps:  85, steps per second: 437, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.791052, mae: 23.417695, mean_q: -34.033284, mean_eps: 0.050000\n",
      " 184090/200000: episode: 1528, duration: 0.216s, episode steps:  90, steps per second: 417, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.844 [0.000, 2.000],  loss: 0.960917, mae: 23.691002, mean_q: -34.354715, mean_eps: 0.050000\n",
      " 184161/200000: episode: 1529, duration: 0.156s, episode steps:  71, steps per second: 454, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.831 [0.000, 2.000],  loss: 0.731608, mae: 24.443557, mean_q: -35.583934, mean_eps: 0.050000\n",
      " 184235/200000: episode: 1530, duration: 0.156s, episode steps:  74, steps per second: 476, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.852756, mae: 24.521577, mean_q: -35.714329, mean_eps: 0.050000\n",
      " 184317/200000: episode: 1531, duration: 0.178s, episode steps:  82, steps per second: 461, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 1.723242, mae: 24.511615, mean_q: -35.627487, mean_eps: 0.050000\n",
      " 184434/200000: episode: 1532, duration: 0.262s, episode steps: 117, steps per second: 446, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.650358, mae: 24.505584, mean_q: -35.667505, mean_eps: 0.050000\n",
      " 184528/200000: episode: 1533, duration: 0.214s, episode steps:  94, steps per second: 439, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.213 [0.000, 2.000],  loss: 1.552868, mae: 24.712331, mean_q: -35.916093, mean_eps: 0.050000\n",
      " 184613/200000: episode: 1534, duration: 0.188s, episode steps:  85, steps per second: 452, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.895774, mae: 24.694078, mean_q: -35.913035, mean_eps: 0.050000\n",
      " 184745/200000: episode: 1535, duration: 0.302s, episode steps: 132, steps per second: 437, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.883444, mae: 24.962648, mean_q: -36.315029, mean_eps: 0.050000\n",
      " 184816/200000: episode: 1536, duration: 0.161s, episode steps:  71, steps per second: 441, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 1.079620, mae: 24.108666, mean_q: -34.998330, mean_eps: 0.050000\n",
      " 184910/200000: episode: 1537, duration: 0.206s, episode steps:  94, steps per second: 456, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.928772, mae: 24.240035, mean_q: -35.153206, mean_eps: 0.050000\n",
      " 185005/200000: episode: 1538, duration: 0.221s, episode steps:  95, steps per second: 430, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.326 [0.000, 2.000],  loss: 0.786438, mae: 24.380203, mean_q: -35.417402, mean_eps: 0.050000\n",
      " 185091/200000: episode: 1539, duration: 0.191s, episode steps:  86, steps per second: 449, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.970836, mae: 24.510904, mean_q: -35.535280, mean_eps: 0.050000\n",
      " 185161/200000: episode: 1540, duration: 0.160s, episode steps:  70, steps per second: 437, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.772434, mae: 24.965835, mean_q: -36.278178, mean_eps: 0.050000\n",
      " 185295/200000: episode: 1541, duration: 0.296s, episode steps: 134, steps per second: 453, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.836 [0.000, 2.000],  loss: 0.907333, mae: 24.603080, mean_q: -35.697067, mean_eps: 0.050000\n",
      " 185379/200000: episode: 1542, duration: 0.195s, episode steps:  84, steps per second: 431, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.349898, mae: 24.225851, mean_q: -35.029673, mean_eps: 0.050000\n",
      " 185462/200000: episode: 1543, duration: 0.189s, episode steps:  83, steps per second: 439, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 1.077588, mae: 24.495934, mean_q: -35.477858, mean_eps: 0.050000\n",
      " 185560/200000: episode: 1544, duration: 0.234s, episode steps:  98, steps per second: 419, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 0.906453, mae: 25.095434, mean_q: -36.346549, mean_eps: 0.050000\n",
      " 185661/200000: episode: 1545, duration: 0.232s, episode steps: 101, steps per second: 435, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 1.018555, mae: 24.496609, mean_q: -35.332135, mean_eps: 0.050000\n",
      " 185752/200000: episode: 1546, duration: 0.210s, episode steps:  91, steps per second: 434, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.778409, mae: 24.832224, mean_q: -36.057042, mean_eps: 0.050000\n",
      " 185827/200000: episode: 1547, duration: 0.178s, episode steps:  75, steps per second: 422, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.607434, mae: 24.516766, mean_q: -35.560784, mean_eps: 0.050000\n",
      " 185914/200000: episode: 1548, duration: 0.206s, episode steps:  87, steps per second: 423, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.875666, mae: 24.632485, mean_q: -35.756506, mean_eps: 0.050000\n",
      " 186022/200000: episode: 1549, duration: 0.247s, episode steps: 108, steps per second: 437, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 0.722994, mae: 24.837600, mean_q: -36.063869, mean_eps: 0.050000\n",
      " 186099/200000: episode: 1550, duration: 0.177s, episode steps:  77, steps per second: 434, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.034326, mae: 24.929423, mean_q: -36.114833, mean_eps: 0.050000\n",
      " 186174/200000: episode: 1551, duration: 0.166s, episode steps:  75, steps per second: 453, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.806695, mae: 24.623654, mean_q: -35.763837, mean_eps: 0.050000\n",
      " 186255/200000: episode: 1552, duration: 0.170s, episode steps:  81, steps per second: 475, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 0.922021, mae: 24.671860, mean_q: -35.913057, mean_eps: 0.050000\n",
      " 186324/200000: episode: 1553, duration: 0.157s, episode steps:  69, steps per second: 439, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.174 [0.000, 2.000],  loss: 0.891927, mae: 24.844581, mean_q: -36.174772, mean_eps: 0.050000\n",
      " 186411/200000: episode: 1554, duration: 0.184s, episode steps:  87, steps per second: 473, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.629605, mae: 25.017900, mean_q: -36.418819, mean_eps: 0.050000\n",
      " 186492/200000: episode: 1555, duration: 0.170s, episode steps:  81, steps per second: 476, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.538647, mae: 24.999120, mean_q: -36.476239, mean_eps: 0.050000\n",
      " 186577/200000: episode: 1556, duration: 0.196s, episode steps:  85, steps per second: 433, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 1.129217, mae: 24.778917, mean_q: -36.022420, mean_eps: 0.050000\n",
      " 186657/200000: episode: 1557, duration: 0.176s, episode steps:  80, steps per second: 453, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 1.937622, mae: 24.692951, mean_q: -35.891982, mean_eps: 0.050000\n",
      " 186742/200000: episode: 1558, duration: 0.176s, episode steps:  85, steps per second: 483, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 1.041016, mae: 24.211952, mean_q: -35.206978, mean_eps: 0.050000\n",
      " 186837/200000: episode: 1559, duration: 0.212s, episode steps:  95, steps per second: 448, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.976847, mae: 24.096968, mean_q: -34.969668, mean_eps: 0.050000\n",
      " 186914/200000: episode: 1560, duration: 0.167s, episode steps:  77, steps per second: 462, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.880615, mae: 24.796730, mean_q: -36.117723, mean_eps: 0.050000\n",
      " 186982/200000: episode: 1561, duration: 0.144s, episode steps:  68, steps per second: 472, episode reward: -67.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.824097, mae: 24.805152, mean_q: -36.159241, mean_eps: 0.050000\n",
      " 187075/200000: episode: 1562, duration: 0.213s, episode steps:  93, steps per second: 437, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.901733, mae: 24.092034, mean_q: -34.931703, mean_eps: 0.050000\n",
      " 187157/200000: episode: 1563, duration: 0.172s, episode steps:  82, steps per second: 478, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 1.074951, mae: 24.325446, mean_q: -35.319731, mean_eps: 0.050000\n",
      " 187232/200000: episode: 1564, duration: 0.171s, episode steps:  75, steps per second: 437, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.160 [0.000, 2.000],  loss: 0.721924, mae: 24.985351, mean_q: -36.380042, mean_eps: 0.050000\n",
      " 187326/200000: episode: 1565, duration: 0.207s, episode steps:  94, steps per second: 453, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.738200, mae: 24.318407, mean_q: -35.320163, mean_eps: 0.050000\n",
      " 187401/200000: episode: 1566, duration: 0.175s, episode steps:  75, steps per second: 429, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.896655, mae: 24.772586, mean_q: -36.020781, mean_eps: 0.050000\n",
      " 187482/200000: episode: 1567, duration: 0.191s, episode steps:  81, steps per second: 425, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 1.379150, mae: 25.119306, mean_q: -36.453725, mean_eps: 0.050000\n",
      " 187596/200000: episode: 1568, duration: 0.261s, episode steps: 114, steps per second: 437, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 1.656390, mae: 24.502129, mean_q: -35.475209, mean_eps: 0.050000\n",
      " 187683/200000: episode: 1569, duration: 0.204s, episode steps:  87, steps per second: 427, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 1.118319, mae: 24.286820, mean_q: -35.174095, mean_eps: 0.050000\n",
      " 187753/200000: episode: 1570, duration: 0.166s, episode steps:  70, steps per second: 423, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.819689, mae: 24.632936, mean_q: -35.814608, mean_eps: 0.050000\n",
      " 187843/200000: episode: 1571, duration: 0.204s, episode steps:  90, steps per second: 440, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.915794, mae: 24.513407, mean_q: -35.641878, mean_eps: 0.050000\n",
      " 187930/200000: episode: 1572, duration: 0.202s, episode steps:  87, steps per second: 430, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.839 [0.000, 2.000],  loss: 0.792125, mae: 24.288611, mean_q: -35.284864, mean_eps: 0.050000\n",
      " 188011/200000: episode: 1573, duration: 0.192s, episode steps:  81, steps per second: 422, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 0.613184, mae: 25.174433, mean_q: -36.583607, mean_eps: 0.050000\n",
      " 188092/200000: episode: 1574, duration: 0.189s, episode steps:  81, steps per second: 429, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 1.273071, mae: 24.334835, mean_q: -35.251165, mean_eps: 0.050000\n",
      " 188182/200000: episode: 1575, duration: 0.200s, episode steps:  90, steps per second: 450, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.043879, mae: 24.604820, mean_q: -35.672238, mean_eps: 0.050000\n",
      " 188280/200000: episode: 1576, duration: 0.239s, episode steps:  98, steps per second: 410, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.793091, mae: 24.476048, mean_q: -35.548482, mean_eps: 0.050000\n",
      " 188362/200000: episode: 1577, duration: 0.193s, episode steps:  82, steps per second: 424, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.930376, mae: 24.360576, mean_q: -35.301880, mean_eps: 0.050000\n",
      " 188443/200000: episode: 1578, duration: 0.185s, episode steps:  81, steps per second: 438, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 1.183716, mae: 24.646718, mean_q: -35.713951, mean_eps: 0.050000\n",
      " 188506/200000: episode: 1579, duration: 0.149s, episode steps:  63, steps per second: 424, episode reward: -62.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.079895, mae: 24.455968, mean_q: -35.398943, mean_eps: 0.050000\n",
      " 188575/200000: episode: 1580, duration: 0.150s, episode steps:  69, steps per second: 461, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 1.037415, mae: 24.303837, mean_q: -35.101916, mean_eps: 0.050000\n",
      " 188660/200000: episode: 1581, duration: 0.198s, episode steps:  85, steps per second: 430, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 1.016113, mae: 24.444291, mean_q: -35.382826, mean_eps: 0.050000\n",
      " 188752/200000: episode: 1582, duration: 0.223s, episode steps:  92, steps per second: 413, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 1.665683, mae: 24.227698, mean_q: -35.074317, mean_eps: 0.050000\n",
      " 188859/200000: episode: 1583, duration: 0.253s, episode steps: 107, steps per second: 423, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.252 [0.000, 2.000],  loss: 0.824864, mae: 24.826300, mean_q: -36.075221, mean_eps: 0.050000\n",
      " 188947/200000: episode: 1584, duration: 0.198s, episode steps:  88, steps per second: 444, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.949485, mae: 24.570289, mean_q: -35.774704, mean_eps: 0.050000\n",
      " 189110/200000: episode: 1585, duration: 0.372s, episode steps: 163, steps per second: 438, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.767 [0.000, 2.000],  loss: 1.061609, mae: 24.889042, mean_q: -36.083151, mean_eps: 0.050000\n",
      " 189229/200000: episode: 1586, duration: 0.274s, episode steps: 119, steps per second: 435, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.126 [0.000, 2.000],  loss: 0.797266, mae: 24.103572, mean_q: -34.905785, mean_eps: 0.050000\n",
      " 189400/200000: episode: 1587, duration: 0.395s, episode steps: 171, steps per second: 433, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.759301, mae: 24.397494, mean_q: -35.423132, mean_eps: 0.050000\n",
      " 189475/200000: episode: 1588, duration: 0.181s, episode steps:  75, steps per second: 414, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.769214, mae: 24.346253, mean_q: -35.357956, mean_eps: 0.050000\n",
      " 189575/200000: episode: 1589, duration: 0.224s, episode steps: 100, steps per second: 447, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.820 [0.000, 2.000],  loss: 0.804118, mae: 24.173089, mean_q: -35.068199, mean_eps: 0.050000\n",
      " 189710/200000: episode: 1590, duration: 0.322s, episode steps: 135, steps per second: 420, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.844 [0.000, 2.000],  loss: 1.263342, mae: 24.145605, mean_q: -35.052240, mean_eps: 0.050000\n",
      " 189784/200000: episode: 1591, duration: 0.181s, episode steps:  74, steps per second: 409, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 0.721002, mae: 24.830969, mean_q: -36.157152, mean_eps: 0.050000\n",
      " 189874/200000: episode: 1592, duration: 0.217s, episode steps:  90, steps per second: 414, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.856 [0.000, 2.000],  loss: 0.718404, mae: 25.069476, mean_q: -36.525790, mean_eps: 0.050000\n",
      " 189938/200000: episode: 1593, duration: 0.146s, episode steps:  64, steps per second: 439, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.891 [0.000, 2.000],  loss: 1.143555, mae: 24.749114, mean_q: -35.933660, mean_eps: 0.050000\n",
      " 190018/200000: episode: 1594, duration: 0.188s, episode steps:  80, steps per second: 426, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.709229, mae: 24.711530, mean_q: -35.947154, mean_eps: 0.050000\n",
      " 190111/200000: episode: 1595, duration: 0.196s, episode steps:  93, steps per second: 475, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 1.773704, mae: 24.155820, mean_q: -35.060067, mean_eps: 0.050000\n",
      " 190202/200000: episode: 1596, duration: 0.206s, episode steps:  91, steps per second: 443, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 1.709778, mae: 24.668931, mean_q: -35.803994, mean_eps: 0.050000\n",
      " 190323/200000: episode: 1597, duration: 0.264s, episode steps: 121, steps per second: 458, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.882454, mae: 23.994635, mean_q: -34.775051, mean_eps: 0.050000\n",
      " 190418/200000: episode: 1598, duration: 0.215s, episode steps:  95, steps per second: 442, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.253 [0.000, 2.000],  loss: 0.650208, mae: 25.156076, mean_q: -36.615130, mean_eps: 0.050000\n",
      " 190512/200000: episode: 1599, duration: 0.235s, episode steps:  94, steps per second: 399, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.170 [0.000, 2.000],  loss: 0.573420, mae: 24.019530, mean_q: -34.896906, mean_eps: 0.050000\n",
      " 190608/200000: episode: 1600, duration: 0.227s, episode steps:  96, steps per second: 423, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.953369, mae: 24.171416, mean_q: -34.934264, mean_eps: 0.050000\n",
      " 190683/200000: episode: 1601, duration: 0.174s, episode steps:  75, steps per second: 431, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.831323, mae: 24.171448, mean_q: -34.993950, mean_eps: 0.050000\n",
      " 190765/200000: episode: 1602, duration: 0.171s, episode steps:  82, steps per second: 480, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.705396, mae: 24.126055, mean_q: -34.995285, mean_eps: 0.050000\n",
      " 190863/200000: episode: 1603, duration: 0.209s, episode steps:  98, steps per second: 468, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.880717, mae: 23.599802, mean_q: -34.133776, mean_eps: 0.050000\n",
      " 190946/200000: episode: 1604, duration: 0.193s, episode steps:  83, steps per second: 430, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.855 [0.000, 2.000],  loss: 1.026478, mae: 23.829387, mean_q: -34.454778, mean_eps: 0.050000\n",
      " 191023/200000: episode: 1605, duration: 0.172s, episode steps:  77, steps per second: 447, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.779 [0.000, 2.000],  loss: 0.887451, mae: 24.015303, mean_q: -34.757216, mean_eps: 0.050000\n",
      " 191143/200000: episode: 1606, duration: 0.266s, episode steps: 120, steps per second: 450, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.746468, mae: 24.529961, mean_q: -35.546676, mean_eps: 0.050000\n",
      " 191244/200000: episode: 1607, duration: 0.216s, episode steps: 101, steps per second: 468, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 0.810434, mae: 24.560691, mean_q: -35.593875, mean_eps: 0.050000\n",
      " 191324/200000: episode: 1608, duration: 0.186s, episode steps:  80, steps per second: 431, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 0.831982, mae: 24.070924, mean_q: -34.840886, mean_eps: 0.050000\n",
      " 191485/200000: episode: 1609, duration: 0.340s, episode steps: 161, steps per second: 473, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.776 [0.000, 2.000],  loss: 1.453625, mae: 24.398499, mean_q: -35.373584, mean_eps: 0.050000\n",
      " 191566/200000: episode: 1610, duration: 0.171s, episode steps:  81, steps per second: 474, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.933350, mae: 23.272523, mean_q: -33.647557, mean_eps: 0.050000\n",
      " 191644/200000: episode: 1611, duration: 0.166s, episode steps:  78, steps per second: 471, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.812305, mae: 24.544152, mean_q: -35.626918, mean_eps: 0.050000\n",
      " 191715/200000: episode: 1612, duration: 0.146s, episode steps:  71, steps per second: 485, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 1.082194, mae: 23.998171, mean_q: -34.729932, mean_eps: 0.050000\n",
      " 191794/200000: episode: 1613, duration: 0.182s, episode steps:  79, steps per second: 434, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.920435, mae: 24.341741, mean_q: -35.299944, mean_eps: 0.050000\n",
      " 191896/200000: episode: 1614, duration: 0.217s, episode steps: 102, steps per second: 471, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.626526, mae: 24.787635, mean_q: -35.989738, mean_eps: 0.050000\n",
      " 191983/200000: episode: 1615, duration: 0.186s, episode steps:  87, steps per second: 467, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.756392, mae: 23.637353, mean_q: -34.166888, mean_eps: 0.050000\n",
      " 192058/200000: episode: 1616, duration: 0.172s, episode steps:  75, steps per second: 435, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.187 [0.000, 2.000],  loss: 0.877051, mae: 24.561299, mean_q: -35.528477, mean_eps: 0.050000\n",
      " 192147/200000: episode: 1617, duration: 0.190s, episode steps:  89, steps per second: 470, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.169 [0.000, 2.000],  loss: 0.829945, mae: 23.996516, mean_q: -34.693522, mean_eps: 0.050000\n",
      " 192249/200000: episode: 1618, duration: 0.224s, episode steps: 102, steps per second: 456, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.157 [0.000, 2.000],  loss: 0.848483, mae: 24.483117, mean_q: -35.424137, mean_eps: 0.050000\n",
      " 192321/200000: episode: 1619, duration: 0.150s, episode steps:  72, steps per second: 481, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.696343, mae: 24.964621, mean_q: -36.238874, mean_eps: 0.050000\n",
      " 192414/200000: episode: 1620, duration: 0.195s, episode steps:  93, steps per second: 476, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.682750, mae: 25.023170, mean_q: -36.302122, mean_eps: 0.050000\n",
      " 192508/200000: episode: 1621, duration: 0.213s, episode steps:  94, steps per second: 441, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 1.761719, mae: 24.583581, mean_q: -35.571867, mean_eps: 0.050000\n",
      " 192605/200000: episode: 1622, duration: 0.199s, episode steps:  97, steps per second: 486, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.134 [0.000, 2.000],  loss: 0.797384, mae: 24.626129, mean_q: -35.727474, mean_eps: 0.050000\n",
      " 192694/200000: episode: 1623, duration: 0.188s, episode steps:  89, steps per second: 473, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.191 [0.000, 2.000],  loss: 0.623801, mae: 23.853311, mean_q: -34.594266, mean_eps: 0.050000\n",
      " 192775/200000: episode: 1624, duration: 0.171s, episode steps:  81, steps per second: 474, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.554053, mae: 24.443500, mean_q: -35.488426, mean_eps: 0.050000\n",
      " 192852/200000: episode: 1625, duration: 0.168s, episode steps:  77, steps per second: 458, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.691748, mae: 23.007165, mean_q: -33.248498, mean_eps: 0.050000\n",
      " 192941/200000: episode: 1626, duration: 0.182s, episode steps:  89, steps per second: 490, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 0.814520, mae: 24.028044, mean_q: -34.841574, mean_eps: 0.050000\n",
      " 193022/200000: episode: 1627, duration: 0.177s, episode steps:  81, steps per second: 459, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.809082, mae: 24.548994, mean_q: -35.619244, mean_eps: 0.050000\n",
      " 193100/200000: episode: 1628, duration: 0.191s, episode steps:  78, steps per second: 408, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 0.863184, mae: 25.109153, mean_q: -36.417608, mean_eps: 0.050000\n",
      " 193204/200000: episode: 1629, duration: 0.229s, episode steps: 104, steps per second: 455, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.231 [0.000, 2.000],  loss: 0.834961, mae: 24.099168, mean_q: -34.884925, mean_eps: 0.050000\n",
      " 193292/200000: episode: 1630, duration: 0.190s, episode steps:  88, steps per second: 462, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 2.179554, mae: 24.153161, mean_q: -34.952827, mean_eps: 0.050000\n",
      " 193392/200000: episode: 1631, duration: 0.216s, episode steps: 100, steps per second: 464, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.739115, mae: 24.319366, mean_q: -35.227828, mean_eps: 0.050000\n",
      " 193471/200000: episode: 1632, duration: 0.169s, episode steps:  79, steps per second: 468, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.740039, mae: 24.511327, mean_q: -35.582441, mean_eps: 0.050000\n",
      " 193556/200000: episode: 1633, duration: 0.186s, episode steps:  85, steps per second: 458, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.141 [0.000, 2.000],  loss: 0.674483, mae: 24.088164, mean_q: -34.881220, mean_eps: 0.050000\n",
      " 193634/200000: episode: 1634, duration: 0.172s, episode steps:  78, steps per second: 454, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.141 [0.000, 2.000],  loss: 0.756616, mae: 24.255793, mean_q: -35.063614, mean_eps: 0.050000\n",
      " 193722/200000: episode: 1635, duration: 0.191s, episode steps:  88, steps per second: 462, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.510454, mae: 23.992144, mean_q: -34.783752, mean_eps: 0.050000\n",
      " 193863/200000: episode: 1636, duration: 0.293s, episode steps: 141, steps per second: 482, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 0.748363, mae: 24.731155, mean_q: -35.849483, mean_eps: 0.050000\n",
      " 193961/200000: episode: 1637, duration: 0.212s, episode steps:  98, steps per second: 462, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.616868, mae: 24.207000, mean_q: -35.089069, mean_eps: 0.050000\n",
      " 194046/200000: episode: 1638, duration: 0.179s, episode steps:  85, steps per second: 474, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.861816, mae: 24.091244, mean_q: -34.839400, mean_eps: 0.050000\n",
      " 194118/200000: episode: 1639, duration: 0.157s, episode steps:  72, steps per second: 459, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.791097, mae: 24.758618, mean_q: -35.795883, mean_eps: 0.050000\n",
      " 194198/200000: episode: 1640, duration: 0.168s, episode steps:  80, steps per second: 475, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.661401, mae: 24.386089, mean_q: -35.350836, mean_eps: 0.050000\n",
      " 194276/200000: episode: 1641, duration: 0.173s, episode steps:  78, steps per second: 452, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.761621, mae: 24.349094, mean_q: -35.163158, mean_eps: 0.050000\n",
      " 194380/200000: episode: 1642, duration: 0.225s, episode steps: 104, steps per second: 463, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.808 [0.000, 2.000],  loss: 0.715980, mae: 24.617279, mean_q: -35.648646, mean_eps: 0.050000\n",
      " 194462/200000: episode: 1643, duration: 0.170s, episode steps:  82, steps per second: 482, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 1.045093, mae: 24.162984, mean_q: -34.911045, mean_eps: 0.050000\n",
      " 194544/200000: episode: 1644, duration: 0.196s, episode steps:  82, steps per second: 418, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.159 [0.000, 2.000],  loss: 0.818677, mae: 23.966453, mean_q: -34.700366, mean_eps: 0.050000\n",
      " 194630/200000: episode: 1645, duration: 0.187s, episode steps:  86, steps per second: 460, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 1.347634, mae: 24.561043, mean_q: -35.621190, mean_eps: 0.050000\n",
      " 194708/200000: episode: 1646, duration: 0.170s, episode steps:  78, steps per second: 460, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 0.774854, mae: 23.496782, mean_q: -34.042091, mean_eps: 0.050000\n",
      " 194837/200000: episode: 1647, duration: 0.280s, episode steps: 129, steps per second: 461, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.202 [0.000, 2.000],  loss: 0.611908, mae: 24.515927, mean_q: -35.642121, mean_eps: 0.050000\n",
      " 194928/200000: episode: 1648, duration: 0.207s, episode steps:  91, steps per second: 440, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.446300, mae: 23.896999, mean_q: -34.748955, mean_eps: 0.050000\n",
      " 195017/200000: episode: 1649, duration: 0.210s, episode steps:  89, steps per second: 424, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.135 [0.000, 2.000],  loss: 0.592265, mae: 24.181386, mean_q: -35.073177, mean_eps: 0.050000\n",
      " 195094/200000: episode: 1650, duration: 0.150s, episode steps:  77, steps per second: 513, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.970595, mae: 24.537117, mean_q: -35.519842, mean_eps: 0.050000\n",
      " 195188/200000: episode: 1651, duration: 0.203s, episode steps:  94, steps per second: 464, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 0.779928, mae: 24.475733, mean_q: -35.598719, mean_eps: 0.050000\n",
      " 195265/200000: episode: 1652, duration: 0.172s, episode steps:  77, steps per second: 448, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.844 [0.000, 2.000],  loss: 0.967920, mae: 23.616158, mean_q: -34.200062, mean_eps: 0.050000\n",
      " 195359/200000: episode: 1653, duration: 0.195s, episode steps:  94, steps per second: 483, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 0.680753, mae: 24.286682, mean_q: -35.291954, mean_eps: 0.050000\n",
      " 195444/200000: episode: 1654, duration: 0.189s, episode steps:  85, steps per second: 451, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.996959, mae: 24.076946, mean_q: -35.032344, mean_eps: 0.050000\n",
      " 195543/200000: episode: 1655, duration: 0.217s, episode steps:  99, steps per second: 456, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.768 [0.000, 2.000],  loss: 0.815226, mae: 23.822402, mean_q: -34.523990, mean_eps: 0.050000\n",
      " 195621/200000: episode: 1656, duration: 0.165s, episode steps:  78, steps per second: 472, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 1.112598, mae: 24.009055, mean_q: -34.826131, mean_eps: 0.050000\n",
      " 195711/200000: episode: 1657, duration: 0.191s, episode steps:  90, steps per second: 472, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.789 [0.000, 2.000],  loss: 0.761919, mae: 24.234022, mean_q: -35.177393, mean_eps: 0.050000\n",
      " 195825/200000: episode: 1658, duration: 0.255s, episode steps: 114, steps per second: 448, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.737 [0.000, 2.000],  loss: 0.913818, mae: 24.072399, mean_q: -34.914767, mean_eps: 0.050000\n",
      " 195907/200000: episode: 1659, duration: 0.169s, episode steps:  82, steps per second: 484, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.590881, mae: 23.372637, mean_q: -33.969779, mean_eps: 0.050000\n",
      " 195971/200000: episode: 1660, duration: 0.139s, episode steps:  64, steps per second: 460, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 0.580292, mae: 23.288666, mean_q: -33.798164, mean_eps: 0.050000\n",
      " 196062/200000: episode: 1661, duration: 0.193s, episode steps:  91, steps per second: 472, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 0.776722, mae: 24.169947, mean_q: -35.051862, mean_eps: 0.050000\n",
      " 196161/200000: episode: 1662, duration: 0.216s, episode steps:  99, steps per second: 458, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 1.184373, mae: 23.966505, mean_q: -34.798950, mean_eps: 0.050000\n",
      " 196240/200000: episode: 1663, duration: 0.171s, episode steps:  79, steps per second: 463, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 0.681342, mae: 24.387454, mean_q: -35.483806, mean_eps: 0.050000\n",
      " 196348/200000: episode: 1664, duration: 0.247s, episode steps: 108, steps per second: 438, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 1.166382, mae: 23.798408, mean_q: -34.542474, mean_eps: 0.050000\n",
      " 196431/200000: episode: 1665, duration: 0.172s, episode steps:  83, steps per second: 482, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.819 [0.000, 2.000],  loss: 0.693604, mae: 24.200196, mean_q: -35.068116, mean_eps: 0.050000\n",
      " 196527/200000: episode: 1666, duration: 0.205s, episode steps:  96, steps per second: 468, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.830444, mae: 24.784525, mean_q: -35.941570, mean_eps: 0.050000\n",
      " 196611/200000: episode: 1667, duration: 0.186s, episode steps:  84, steps per second: 452, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 0.798118, mae: 23.959364, mean_q: -34.675967, mean_eps: 0.050000\n",
      " 196721/200000: episode: 1668, duration: 0.240s, episode steps: 110, steps per second: 458, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.809 [0.000, 2.000],  loss: 0.851057, mae: 24.323750, mean_q: -35.205029, mean_eps: 0.050000\n",
      " 196833/200000: episode: 1669, duration: 0.249s, episode steps: 112, steps per second: 449, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.741 [0.000, 2.000],  loss: 0.555498, mae: 24.679508, mean_q: -35.886554, mean_eps: 0.050000\n",
      " 196912/200000: episode: 1670, duration: 0.171s, episode steps:  79, steps per second: 461, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.823 [0.000, 2.000],  loss: 2.079237, mae: 23.980726, mean_q: -34.698281, mean_eps: 0.050000\n",
      " 197029/200000: episode: 1671, duration: 0.259s, episode steps: 117, steps per second: 451, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.684 [0.000, 2.000],  loss: 1.102344, mae: 24.148789, mean_q: -35.027252, mean_eps: 0.050000\n",
      " 197109/200000: episode: 1672, duration: 0.168s, episode steps:  80, steps per second: 477, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.944824, mae: 24.721780, mean_q: -35.806766, mean_eps: 0.050000\n",
      " 197208/200000: episode: 1673, duration: 0.220s, episode steps:  99, steps per second: 449, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 0.672689, mae: 23.783576, mean_q: -34.471957, mean_eps: 0.050000\n",
      " 197277/200000: episode: 1674, duration: 0.155s, episode steps:  69, steps per second: 446, episode reward: -68.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.598796, mae: 24.442233, mean_q: -35.485956, mean_eps: 0.050000\n",
      " 197365/200000: episode: 1675, duration: 0.194s, episode steps:  88, steps per second: 454, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.699086, mae: 24.230705, mean_q: -35.139832, mean_eps: 0.050000\n",
      " 197445/200000: episode: 1676, duration: 0.172s, episode steps:  80, steps per second: 464, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.900 [0.000, 2.000],  loss: 0.637305, mae: 23.949875, mean_q: -34.699319, mean_eps: 0.050000\n",
      " 197521/200000: episode: 1677, duration: 0.172s, episode steps:  76, steps per second: 442, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.842 [0.000, 2.000],  loss: 0.683813, mae: 24.377124, mean_q: -35.386628, mean_eps: 0.050000\n",
      " 197608/200000: episode: 1678, duration: 0.186s, episode steps:  87, steps per second: 467, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 1.371973, mae: 24.366085, mean_q: -35.182084, mean_eps: 0.050000\n",
      " 197712/200000: episode: 1679, duration: 0.239s, episode steps: 104, steps per second: 435, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.135 [0.000, 2.000],  loss: 2.055533, mae: 24.280996, mean_q: -35.111805, mean_eps: 0.050000\n",
      " 197808/200000: episode: 1680, duration: 0.223s, episode steps:  96, steps per second: 430, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.692668, mae: 23.849947, mean_q: -34.553224, mean_eps: 0.050000\n",
      " 197884/200000: episode: 1681, duration: 0.169s, episode steps:  76, steps per second: 450, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 2.291919, mae: 24.202189, mean_q: -35.102742, mean_eps: 0.050000\n",
      " 197966/200000: episode: 1682, duration: 0.175s, episode steps:  82, steps per second: 470, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.572803, mae: 23.852355, mean_q: -34.591460, mean_eps: 0.050000\n",
      " 198054/200000: episode: 1683, duration: 0.192s, episode steps:  88, steps per second: 459, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.725297, mae: 24.589166, mean_q: -35.619330, mean_eps: 0.050000\n",
      " 198164/200000: episode: 1684, duration: 0.246s, episode steps: 110, steps per second: 447, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.640852, mae: 24.164618, mean_q: -35.015968, mean_eps: 0.050000\n",
      " 198277/200000: episode: 1685, duration: 0.244s, episode steps: 113, steps per second: 464, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.257 [0.000, 2.000],  loss: 0.606541, mae: 23.725867, mean_q: -34.384151, mean_eps: 0.050000\n",
      " 198361/200000: episode: 1686, duration: 0.188s, episode steps:  84, steps per second: 448, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 1.057195, mae: 23.479316, mean_q: -34.028399, mean_eps: 0.050000\n",
      " 198459/200000: episode: 1687, duration: 0.208s, episode steps:  98, steps per second: 470, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.600891, mae: 23.222942, mean_q: -33.640042, mean_eps: 0.050000\n",
      " 198552/200000: episode: 1688, duration: 0.208s, episode steps:  93, steps per second: 447, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.301 [0.000, 2.000],  loss: 0.885431, mae: 23.714261, mean_q: -34.334275, mean_eps: 0.050000\n",
      " 198629/200000: episode: 1689, duration: 0.176s, episode steps:  77, steps per second: 437, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.624341, mae: 22.944784, mean_q: -33.212272, mean_eps: 0.050000\n",
      " 198721/200000: episode: 1690, duration: 0.213s, episode steps:  92, steps per second: 431, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.574504, mae: 23.037118, mean_q: -33.382631, mean_eps: 0.050000\n",
      " 198803/200000: episode: 1691, duration: 0.173s, episode steps:  82, steps per second: 473, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.693762, mae: 22.951108, mean_q: -33.238709, mean_eps: 0.050000\n",
      " 198888/200000: episode: 1692, duration: 0.191s, episode steps:  85, steps per second: 444, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.749463, mae: 23.303383, mean_q: -33.768635, mean_eps: 0.050000\n",
      " 198968/200000: episode: 1693, duration: 0.181s, episode steps:  80, steps per second: 442, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.603809, mae: 23.288270, mean_q: -33.744932, mean_eps: 0.050000\n",
      " 199073/200000: episode: 1694, duration: 0.243s, episode steps: 105, steps per second: 432, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.846610, mae: 24.076227, mean_q: -34.862662, mean_eps: 0.050000\n",
      " 199143/200000: episode: 1695, duration: 0.143s, episode steps:  70, steps per second: 490, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.743195, mae: 23.573557, mean_q: -34.067338, mean_eps: 0.050000\n",
      " 199232/200000: episode: 1696, duration: 0.217s, episode steps:  89, steps per second: 409, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.135 [0.000, 2.000],  loss: 0.857777, mae: 24.035774, mean_q: -34.818462, mean_eps: 0.050000\n",
      " 199313/200000: episode: 1697, duration: 0.200s, episode steps:  81, steps per second: 404, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 1.218373, mae: 23.742369, mean_q: -34.449826, mean_eps: 0.050000\n",
      " 199431/200000: episode: 1698, duration: 0.256s, episode steps: 118, steps per second: 460, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.237 [0.000, 2.000],  loss: 0.686366, mae: 23.280259, mean_q: -33.756930, mean_eps: 0.050000\n",
      " 199531/200000: episode: 1699, duration: 0.233s, episode steps: 100, steps per second: 430, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.590088, mae: 23.121053, mean_q: -33.552840, mean_eps: 0.050000\n",
      " 199605/200000: episode: 1700, duration: 0.154s, episode steps:  74, steps per second: 482, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.693197, mae: 23.241380, mean_q: -33.702366, mean_eps: 0.050000\n",
      " 199695/200000: episode: 1701, duration: 0.191s, episode steps:  90, steps per second: 470, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.925004, mae: 22.561842, mean_q: -32.622488, mean_eps: 0.050000\n",
      " 199767/200000: episode: 1702, duration: 0.154s, episode steps:  72, steps per second: 468, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.657010, mae: 23.318240, mean_q: -33.769830, mean_eps: 0.050000\n",
      " 199853/200000: episode: 1703, duration: 0.189s, episode steps:  86, steps per second: 455, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.846879, mae: 23.352378, mean_q: -33.879789, mean_eps: 0.050000\n",
      " 199933/200000: episode: 1704, duration: 0.170s, episode steps:  80, steps per second: 472, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.627417, mae: 23.239186, mean_q: -33.657782, mean_eps: 0.050000\n",
      " 199997/200000: episode: 1705, duration: 0.149s, episode steps:  64, steps per second: 431, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.436401, mae: 23.318531, mean_q: -33.793811, mean_eps: 0.050000\n",
      "done, took 373.730 seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Crear directorio para guardar resultados\u001b[39;00m\n\u001b[1;32m     67\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_improved\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 68\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Guardar modelo completo y pesos\u001b[39;00m\n\u001b[1;32m     71\u001b[0m dueling_dqn_improved\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdueling_dqn_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_improved_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Implementaci√≥n de la red mejorada con arquitectura Dueling DQN\n",
    "def build_dueling_model_improved(window_length, input_shape, nb_actions):\n",
    "    input_layer = tf.keras.layers.Input(shape=(window_length,) + input_shape)\n",
    "    flatten = tf.keras.layers.Flatten()(input_layer)\n",
    "\n",
    "    # Red compartida\n",
    "    shared_dense = tf.keras.layers.Dense(256, activation='relu')(flatten)\n",
    "    shared_dense = tf.keras.layers.Dense(256, activation='relu')(shared_dense)\n",
    "\n",
    "    # Stream de ventaja\n",
    "    advantage_fc = tf.keras.layers.Dense(128, activation='relu')(shared_dense)\n",
    "    advantage_fc = tf.keras.layers.Dense(64, activation='relu')(advantage_fc)\n",
    "    advantage = tf.keras.layers.Dense(nb_actions, activation='linear')(advantage_fc)\n",
    "\n",
    "    # Stream de valor\n",
    "    value_fc = tf.keras.layers.Dense(128, activation='relu')(shared_dense)\n",
    "    value_fc = tf.keras.layers.Dense(64, activation='relu')(value_fc)\n",
    "    value = tf.keras.layers.Dense(1, activation='linear')(value_fc)\n",
    "\n",
    "    # C√°lculo Q(s, a) = V(s) + A(s, a) - promedio(A(s, a))\n",
    "    output = tf.keras.layers.Add()([\n",
    "        value,\n",
    "        tf.keras.layers.Subtract()([\n",
    "            advantage,\n",
    "            tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1, keepdims=True))(advantage)\n",
    "        ])\n",
    "    ])\n",
    "\n",
    "    return tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "\n",
    "# Crear modelo mejorado\n",
    "window_length = 4\n",
    "input_shape = env.observation_space.shape\n",
    "improved_dueling_model = build_dueling_model_improved(window_length, input_shape, nb_actions)\n",
    "improved_dueling_model.summary()\n",
    "\n",
    "# Configurar optimizaci√≥n de precisi√≥n mixta\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Crear agente mejorado\n",
    "dueling_dqn_improved = DQNAgent(\n",
    "    model=improved_dueling_model,\n",
    "    nb_actions=nb_actions,\n",
    "    memory=SequentialMemory(limit=200000, window_length=window_length),\n",
    "    policy=LinearAnnealedPolicy(\n",
    "        EpsGreedyQPolicy(),\n",
    "        attr='eps',\n",
    "        value_max=1.0,\n",
    "        value_min=0.05,\n",
    "        value_test=0.01,\n",
    "        nb_steps=100000\n",
    "    ),\n",
    "    nb_steps_warmup=1000,\n",
    "    gamma=0.99,\n",
    "    target_model_update=500,\n",
    "    train_interval=8,\n",
    "    batch_size=64\n",
    ")\n",
    "dueling_dqn_improved.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=['mae'])\n",
    "\n",
    "# Entrenar el modelo mejorado\n",
    "nb_steps = 200000\n",
    "dueling_history_improved = dueling_dqn_improved.fit(env, nb_steps=nb_steps, visualize=False, verbose=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.pyenv/versions/3.8.16/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -172.000, steps: 173\n",
      "Episode 2: reward: -79.000, steps: 80\n",
      "Episode 3: reward: -79.000, steps: 80\n",
      "Episode 4: reward: -75.000, steps: 76\n",
      "Episode 5: reward: -99.000, steps: 100\n",
      "Episode 6: reward: -86.000, steps: 87\n",
      "Episode 7: reward: -82.000, steps: 83\n",
      "Episode 8: reward: -77.000, steps: 78\n",
      "Episode 9: reward: -70.000, steps: 71\n",
      "Episode 10: reward: -79.000, steps: 80\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear directorio para guardar resultados\n",
    "output_dir = f\"results_{env_name}_juan\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Guardar modelo completo y pesos\n",
    "dueling_dqn_improved.model.save(os.path.join(output_dir, f'dueling_dqn_{env_name}_improved_model.h5'))\n",
    "dueling_dqn_improved.save_weights(os.path.join(output_dir, f'dueling_dqn_{env_name}_improved_weights.h5f'), overwrite=True)\n",
    "\n",
    "# Exportar logs de entrenamiento a CSV\n",
    "train_history_df = pd.DataFrame(dueling_history_improved.history)\n",
    "train_history_df.to_csv(os.path.join(output_dir, f'dueling_dqn_{env_name}_improved_train_log.csv'), index=False)\n",
    "\n",
    "# Evaluar el modelo mejorado\n",
    "dueling_test_improved = dueling_dqn_improved.test(env, nb_episodes=10, visualize=False)\n",
    "test_rewards = dueling_test_improved.history['episode_reward']\n",
    "\n",
    "# Guardar recompensas de prueba\n",
    "\n",
    "np.savetxt(os.path.join(output_dir, f'dueling_dqn_{env_name}_improved_test_rewards.csv'), test_rewards, delimiter=',')\n",
    "\n",
    "# Graficar resultados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(dueling_history_improved.history['episode_reward'])), dueling_history_improved.history['episode_reward'], label='Recompensas Mejoradas')\n",
    "plt.xlabel('Episodios')\n",
    "plt.ylabel('Recompensa')\n",
    "plt.title('Recompensas por Episodio (Mejorado)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(output_dir, 'reward_per_episode_improved.png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
